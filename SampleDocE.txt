The idea of a mixture model. As a generative model. Formally defines the following generative model. Mixture of two unigram language models. Data document a. Would the ml estimate demote background words in theta sub d. Gerneal behavior of mixture model. Every component model attempts to assign high probabilities to highly frequent words in the data to collaboratively maximize likelihood. Different component models tend to bet high probabilities on different to avoid competition or a waste of porbabilites. The probability of choosing each component regulates the collaboration competition between the component models. Fixing one component to a background word distribution i e background language model. Helps get rid of background words in other component. Is an example of imposing a prior on the model parameters. Prior equals one model must be exactly the same as the background l m. Given all the parameters infer the distribution a word is from. Initialize the parameter values somewhat randomly, then take a guess of z values. The expectation maximization algorithm, Initialise p of w given theta sub d with random variables. Then iterativly improve it using e step and m step. Stop when likelihood doesn't change.   