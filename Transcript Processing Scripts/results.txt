11 augmented 2.4426557483831942e-06 so now the e step as you can recall is your augmented data and by predicting the values of the hidden variable 
{'3 - 14 - 2.14 Probabilistic Latent Semantic Analysis (PLSA)- Part 2 (00-10-15).srt : 00:00:36,750 --> 00:00:40,740': 'so now the e step as you can recall is your augmented data and', '3 - 12 - 2.12 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3 (00-06-25).srt : 00:05:10,056 --> 00:05:15,750': 'in the mstep then we would exploit such augmented data which would make'}
--------------------------------------------------
7 baye 1.2213278741915971e-06 in both cases we are using the baye rule as i explained basically assessing the likelihood of generating word from each of this division and there normalize 
{'3 - 14 - 2.14 Probabilistic Latent Semantic Analysis (PLSA)- Part 2 (00-10-15).srt : 00:01:46,320 --> 00:01:50,490': 'in both cases we are using the baye rule as i explained basically'}
--------------------------------------------------
23 division 1.2213278741915971e-06 in both cases we are using the baye rule as i explained basically assessing the likelihood of generating word from each of this division and there normalize 
{'3 - 14 - 2.14 Probabilistic Latent Semantic Analysis (PLSA)- Part 2 (00-10-15).srt : 00:01:50,490 --> 00:01:55,300': 'assessing the likelihood of generating word from each of this division and'}
--------------------------------------------------
14 presenting 1.2213278741915971e-06 and when we thinking this way we also have a more concise way of presenting the em algorithm 
{'3 - 14 - 2.14 Probabilistic Latent Semantic Analysis (PLSA)- Part 2 (00-10-15).srt : 00:04:34,163 --> 00:04:37,993': 'we also have a more concise way of presenting the em algorithm'}
--------------------------------------------------
15 awarded 2.4426557483831942e-06 so in our case we are interested in all of those coverage perimeters pi and awarded distributions inaudible and we just randomly normalize them 
{'3 - 14 - 2.14 Probabilistic Latent Semantic Analysis (PLSA)- Part 2 (00-10-15).srt : 00:04:55,000 --> 00:04:59,830': 'awarded distributions inaudible and we just randomly normalize them', '4 - 2 - 3.2 Text Clustering- Generative Probabilistic Models Part 1 (00-16-18).srt : 00:01:31,478 --> 00:01:35,750': 'each is awarded distribution and the other is pi i j'}
--------------------------------------------------
3 renormalize 2.2677482445081393e-06 or we can renormalize based on all the words 
{'3 - 14 - 2.14 Probabilistic Latent Semantic Analysis (PLSA)- Part 2 (00-10-15).srt : 00:07:02,040 --> 00:07:08,230': 'or we can renormalize based on all the words'}
--------------------------------------------------
2 intentionally 2.4426557483831942e-06 and i intentionally leave this as an exercise for you 
{'3 - 14 - 2.14 Probabilistic Latent Semantic Analysis (PLSA)- Part 2 (00-10-15).srt : 00:07:35,340 --> 00:07:38,550': 'and i intentionally leave this as an exercise for you', '4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:08:20,986 --> 00:08:23,320': 'we do not intentionally do that '}
--------------------------------------------------
5 envisioning 1.2213278741915971e-06 so in general in the envisioning of em algorithms you will see you accumulate the counts various counts and then you normalize them 
{'3 - 14 - 2.14 Probabilistic Latent Semantic Analysis (PLSA)- Part 2 (00-10-15).srt : 00:07:50,940 --> 00:07:54,710': 'so in general in the envisioning of em algorithms you will see you accumulate'}
--------------------------------------------------
5 predetermined 1.2213278741915971e-06 and we also added a predetermined background language model to help discover discriminative topics because this background language model can help attract the common terms 
{'3 - 14 - 2.14 Probabilistic Latent Semantic Analysis (PLSA)- Part 2 (00-10-15).srt : 00:08:11,830 --> 00:08:16,850': 'and we also added a predetermined background language model to'}
--------------------------------------------------
25 proportion 2.2677482445081393e-06 in this case plsa allows us to discover two things one is k worded distributions each one representing a topic and the other is the proportion of each topic in each document 
{'3 - 14 - 2.14 Probabilistic Latent Semantic Analysis (PLSA)- Part 2 (00-10-15).srt : 00:08:37,265 --> 00:08:40,779': 'the other is the proportion of each topic in each document'}
--------------------------------------------------
15 photo 1.2213278741915971e-06 and such detailed characterization of coverage of topics in documents can enable a lot of photo analysis 
{'3 - 14 - 2.14 Probabilistic Latent Semantic Analysis (PLSA)- Part 2 (00-10-15).srt : 00:08:46,510 --> 00:08:48,890': 'can enable a lot of photo analysis '}
--------------------------------------------------
10 pan 1.4829329667707326e-06 for example we can aggregate the documents in the particular pan period to assess the coverage of a particular topic in a time period 
{'3 - 14 - 2.14 Probabilistic Latent Semantic Analysis (PLSA)- Part 2 (00-10-15).srt : 00:08:53,970 --> 00:08:58,800': 'pan period to assess the coverage of a particular topic in a time period'}
--------------------------------------------------
31 completing 1.7445380593498679e-06 and were going to have text mining algorithms to help us to turn text data into actionable knowledge that we can use in real world especially for decision making or for completing whatever tasks that require text data to support 
{'2 - 2 - 1.2 Overview Text Mining and Analytics- Part 2 (00-11-44).srt : 00:00:34,130 --> 00:00:39,350': 'for completing whatever tasks that require text data to support'}
--------------------------------------------------
12 theyll 1.7445380593498679e-06 different people would be looking at the world from different angles and theyll pay attention to different things 
{'2 - 2 - 1.2 Overview Text Mining and Analytics- Part 2 (00-11-44).srt : 00:01:38,820 --> 00:01:41,210': 'theyll pay attention to different things '}
--------------------------------------------------
9 revert 1.2213278741915971e-06 the main goal of text mining is actually to revert this process of generating text data 
{'2 - 2 - 1.2 Overview Text Mining and Analytics- Part 2 (00-11-44).srt : 00:03:10,590 --> 00:03:15,790': 'the main goal of text mining is actually to revert this'}
--------------------------------------------------
18 herself 2.4426557483831942e-06 if you look further then you can also imagine we can mine knowledge about this observer himself or herself 
{'2 - 2 - 1.2 Overview Text Mining and Analytics- Part 2 (00-11-44).srt : 00:04:50,060 --> 00:04:54,710': 'we can mine knowledge about this observer himself or herself', '5 - 7 - 4.7 Contextual Text Mining- Motivation (00-06-47).srt : 00:05:47,810 --> 00:05:51,700': 'so this goes beyond just the author himself or herself'}
--------------------------------------------------
6 mood 1.7445380593498679e-06 and these properties could include the mood of the person or sentiment of the person 
{'2 - 2 - 1.2 Overview Text Mining and Analytics- Part 2 (00-11-44).srt : 00:05:03,380 --> 00:05:07,410': 'and these properties could include the mood of the person or'}
--------------------------------------------------
6 subjected 1.2213278741915971e-06 but the description can be also subjected with sentiment and so in general you can imagine the text data would contain some factual descriptions of the world plus some subjective comments 
{'2 - 2 - 1.2 Overview Text Mining and Analytics- Part 2 (00-11-44).srt : 00:05:21,040 --> 00:05:25,960': 'but the description can be also subjected with sentiment and so'}
--------------------------------------------------
11 supplying 1.7445380593498679e-06 now nontext data can be also used for analyzing text by supplying context 
{'2 - 2 - 1.2 Overview Text Mining and Analytics- Part 2 (00-11-44).srt : 00:08:17,161 --> 00:08:24,510': 'now nontext data can be also used for analyzing text by supplying context'}
--------------------------------------------------
15 andor 1.2213278741915971e-06 when we look at the text data alone well be mostly looking at the content andor opinions expressed in the text 
{'2 - 2 - 1.2 Overview Text Mining and Analytics- Part 2 (00-11-44).srt : 00:08:27,050 --> 00:08:31,770': 'well be mostly looking at the content andor opinions expressed in the text'}
--------------------------------------------------
6 contextsensitive 2.2677482445081393e-06 and it can help us make contextsensitive analysis of content or the language usage or the opinions about the observer or the authors of text data 
{'2 - 2 - 1.2 Overview Text Mining and Analytics- Part 2 (00-11-44).srt : 00:09:24,580 --> 00:09:29,340': 'and it can help us make contextsensitive '}
--------------------------------------------------
6 selectively 2.2677482445081393e-06 in this course were going to selectively cover some of those topics 
{'2 - 2 - 1.2 Overview Text Mining and Analytics- Part 2 (00-11-44).srt : 00:09:54,500 --> 00:09:59,850': 'in this course were going to selectively cover some of those topics'}
--------------------------------------------------
22 generational 1.2213278741915971e-06 now it also possible to develop a unified generative model for solving this problem and that is we not only model the generational overrating based on text 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:00:35,478 --> 00:00:41,360': 'that is we not only model the generational overrating based on text'}
--------------------------------------------------
23 overrating 2.4426557483831942e-06 now it also possible to develop a unified generative model for solving this problem and that is we not only model the generational overrating based on text 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:00:35,478 --> 00:00:41,360': 'that is we not only model the generational overrating based on text', '5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:01:18,470 --> 00:01:23,710': 'use the text to further predict the overrating'}
--------------------------------------------------
19 overrating 2.4426557483831942e-06 and then we can then plug in the latent regression model to use the text to further predict the overrating 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:00:35,478 --> 00:00:41,360': 'that is we not only model the generational overrating based on text', '5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:01:18,470 --> 00:01:23,710': 'use the text to further predict the overrating'}
--------------------------------------------------
26 cutting 2.4426557483831942e-06 so we dont have time to discuss this model in detail as in many other cases in this part of the cause where we discuss the cutting edge topics but there a reference site here where you can find more details 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:01:46,150 --> 00:01:51,990': 'many other cases in this part of the cause where we discuss the cutting edge topics', '1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:06:26,048 --> 00:06:31,474': 'in these chapters you can find some in depth discussion of cutting'}
--------------------------------------------------
4 decomposition 2.4426557483831942e-06 first it about rating decomposition 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:02:02,760 --> 00:02:05,450': 'first it about rating decomposition ', '5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:07:45,098 --> 00:07:48,920': 'and because of the decomposition we have now generated the summaries for'}
--------------------------------------------------
2 decomposing 2.2677482445081393e-06 but by decomposing these ratings into aspect ratings we can see some hotels have higher ratings for some dimensions like value but others might score better in other dimensions like location 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:02:18,980 --> 00:02:24,270': 'but by decomposing these ratings into aspect ratings'}
--------------------------------------------------
3 groundtruth 1.2213278741915971e-06 now here the groundtruth is shown in the parenthesis so it also allows you to see whether the prediction is accurate 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:02:38,750 --> 00:02:42,940': 'now here the groundtruth is shown in the parenthesis so'}
--------------------------------------------------
8 parenthesis 1.2213278741915971e-06 now here the groundtruth is shown in the parenthesis so it also allows you to see whether the prediction is accurate 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:02:38,750 --> 00:02:42,940': 'now here the groundtruth is shown in the parenthesis so'}
--------------------------------------------------
3 sentimental 2.4426557483831942e-06 and these are sentimental weights for words in different aspects 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:03:47,770 --> 00:03:52,930': 'and these are sentimental weights for words in different aspects', '5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:06:58,697 --> 00:07:05,640': 'of course these sentimental weights might be different for different aspects'}
--------------------------------------------------
21 polarities 1.7445380593498679e-06 now this kind of lexicon is very useful because in general a word like long let say may have different sentiment polarities for different context 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:04:21,410 --> 00:04:26,240': 'let say may have different sentiment polarities for different context'}
--------------------------------------------------
5 rebooting 1.7445380593498679e-06 but if i say the rebooting time for the laptop is long that bad right 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:04:31,270 --> 00:04:36,440': 'but if i say the rebooting time for the laptop is long that bad right'}
--------------------------------------------------
5 wether 1.2213278741915971e-06 remember the model can infer wether a reviewer cares more about service or the price 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:05:31,530 --> 00:05:36,165': 'remember the model can infer wether a reviewer cares more about service or'}
--------------------------------------------------
2 poses 1.7445380593498679e-06 and this poses a very difficult challenge for evaluation 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:05:41,380 --> 00:05:45,500': 'and this poses a very difficult challenge for evaluation'}
--------------------------------------------------
7 validating 1.7445380593498679e-06 and this provides some indirect way of validating the inferred weights 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:07:07,360 --> 00:07:14,720': 'and this provides some indirect way of validating the inferred weights'}
--------------------------------------------------
18 decomposition 2.4426557483831942e-06 for example a direct application would be to generate the rated aspect the summary and because of the decomposition we have now generated the summaries for each aspect 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:02:02,760 --> 00:02:05,450': 'first it about rating decomposition ', '5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:07:45,098 --> 00:07:48,920': 'and because of the decomposition we have now generated the summaries for'}
--------------------------------------------------
2 mp 1.2213278741915971e-06 these are mp reviews and these results show that the model can discover some interesting aspects 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:08:06,990 --> 00:08:08,810': 'these are mp reviews '}
--------------------------------------------------
11 appreciating 1.2213278741915971e-06 so that can help us discover for example consumers trend in appreciating different features of products 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:08:29,796 --> 00:08:34,460': 'trend in appreciating different features of products'}
--------------------------------------------------
14 screens 1.2213278741915971e-06 for example one might have discovered the trend that people tend to like larger screens of cell phones or light weight of laptop etcetera 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:08:39,980 --> 00:08:45,550': 'like larger screens of cell phones or light weight of laptop etcetera'}
--------------------------------------------------
17 phones 1.2213278741915971e-06 for example one might have discovered the trend that people tend to like larger screens of cell phones or light weight of laptop etcetera 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:08:39,980 --> 00:08:45,550': 'like larger screens of cell phones or light weight of laptop etcetera'}
--------------------------------------------------
31 cleanness 1.2213278741915971e-06 but if you look at the when they didnt like expensive hotels or cheaper hotels then youll see that they tended to have more weights on the condition of the room cleanness 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:10:00,600 --> 00:10:03,070': 'the room cleanness '}
--------------------------------------------------
17 yours 1.7445380593498679e-06 we can find the reviewers whose weights are more precise of course inferred rates are similar to yours 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:11:40,325 --> 00:11:43,795': 'of course inferred rates are similar to yours'}
--------------------------------------------------
2 nonpersonalized 2.2677482445081393e-06 now the nonpersonalized recommendations now shown on the top and you can see the top results generally have much higher price than the lower group and that because when the reviewer cared more about the value as dictated by this query they tended to really favor low price hotels 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:11:51,212 --> 00:11:56,030': 'now the nonpersonalized recommendations now shown on the top'}
--------------------------------------------------
23 holders 2.006143151929004e-06 but opinion mining from news and social media is also important but that more difficult than analyzing review data mainly because the opinion holders and opinion targets are all interested 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:13:35,800 --> 00:13:40,980': 'more difficult than analyzing review data mainly because the opinion holders and'}
--------------------------------------------------
17 undefined 1.7445380593498679e-06 and sometimes when we have  log of  we would generally define that as  because log of  is undefined 
{'2 - 10 - 1.10 Syntagmatic Relation Discovery- Entropy (00-11-00).srt : 00:05:18,370 --> 00:05:25,960': 'we would generally define that as because log of is undefined'}
--------------------------------------------------
15 tossing 2.4426557483831942e-06 it easier to look at the problem by thinking of a simple example using coin tossing 
{'2 - 10 - 1.10 Syntagmatic Relation Discovery- Entropy (00-11-00).srt : 00:07:40,800 --> 00:07:42,380': 'using coin tossing ', '2 - 10 - 1.10 Syntagmatic Relation Discovery- Entropy (00-11-00).srt : 00:07:43,420 --> 00:07:47,660': 'so when we think about random experiments like tossing a coin'}
--------------------------------------------------
8 tossing 2.4426557483831942e-06 so when we think about random experiments like tossing a coin it gives us a random variable that can represent the result 
{'2 - 10 - 1.10 Syntagmatic Relation Discovery- Entropy (00-11-00).srt : 00:07:40,800 --> 00:07:42,380': 'using coin tossing ', '2 - 10 - 1.10 Syntagmatic Relation Discovery- Entropy (00-11-00).srt : 00:07:43,420 --> 00:07:47,660': 'so when we think about random experiments like tossing a coin'}
--------------------------------------------------
15 toss 1.2213278741915971e-06 and this entropy indicates how difficult it is to predict the outcome of a coin toss 
{'2 - 10 - 1.10 Syntagmatic Relation Discovery- Entropy (00-11-00).srt : 00:08:22,050 --> 00:08:22,890': 'of a coin toss '}
--------------------------------------------------
18 documentive 1.2213278741915971e-06 one is the total number of documents in the collection and that is m the other is the documentive frequency 
{'2 - 8 - 1.8 TF Transformation (00-09-31).srt : 00:01:41,450 --> 00:01:45,880': 'the other is the documentive frequency '}
--------------------------------------------------
1 intriguingly 1.7445380593498679e-06 so intriguingly in order to lower the score for this document we need to somehow restrict the contribution of the matching of this term in the document 
{'2 - 8 - 1.8 TF Transformation (00-09-31).srt : 00:02:31,572 --> 00:02:37,240': 'so intriguingly in order to lower the score for this document we need'}
--------------------------------------------------
19 shouldnt 2.2677482445081393e-06 and if you think about the matching of terms in the document carefully you actually would realize we probably shouldnt reward multiple occurrences so generously 
{'2 - 8 - 1.8 TF Transformation (00-09-31).srt : 00:02:48,290 --> 00:02:53,059': 'actually would realize we probably shouldnt reward'}
--------------------------------------------------
24 generously 1.7445380593498679e-06 and if you think about the matching of terms in the document carefully you actually would realize we probably shouldnt reward multiple occurrences so generously 
{'2 - 8 - 1.8 TF Transformation (00-09-31).srt : 00:02:53,059 --> 00:02:58,360': 'multiple occurrences so generously '}
--------------------------------------------------
32 confirmed 1.7445380593498679e-06 if we see an extra occurrence on top of the first occurrence that is to go from one to two then we also can say that well the second occurrence kind of confirmed that it not a accidental mention of the word 
{'2 - 8 - 1.8 TF Transformation (00-09-31).srt : 00:03:34,940 --> 00:03:38,950': 'occurrence kind of confirmed that it not a accidental mention of the word'}
--------------------------------------------------
37 accidental 1.7445380593498679e-06 if we see an extra occurrence on top of the first occurrence that is to go from one to two then we also can say that well the second occurrence kind of confirmed that it not a accidental mention of the word 
{'2 - 8 - 1.8 TF Transformation (00-09-31).srt : 00:03:34,940 --> 00:03:38,950': 'occurrence kind of confirmed that it not a accidental mention of the word'}
--------------------------------------------------
9 increasingly 2.2677482445081393e-06 so in the previous ranking functions we actually have increasingly used some kind of transformation 
{'2 - 8 - 1.8 TF Transformation (00-09-31).srt : 00:04:33,360 --> 00:04:37,981': 'so in the previous ranking functions we actually have increasingly'}
--------------------------------------------------
5 zeroone 2.2677482445081393e-06 so for example in the zeroone bit vector retentation we actually use the suchier transformation function as shown here 
{'2 - 8 - 1.8 TF Transformation (00-09-31).srt : 00:04:40,410 --> 00:04:44,764': 'so for example in the zeroone bit vector retentation we actually use'}
--------------------------------------------------
8 retentation 1.2213278741915971e-06 so for example in the zeroone bit vector retentation we actually use the suchier transformation function as shown here 
{'2 - 8 - 1.8 TF Transformation (00-09-31).srt : 00:04:40,410 --> 00:04:44,764': 'so for example in the zeroone bit vector retentation we actually use'}
--------------------------------------------------
13 suchier 1.2213278741915971e-06 so for example in the zeroone bit vector retentation we actually use the suchier transformation function as shown here 
{'2 - 8 - 1.8 TF Transformation (00-09-31).srt : 00:04:44,764 --> 00:04:49,070': 'the suchier transformation function as shown here'}
--------------------------------------------------
6 bend 1.7445380593498679e-06 or we might want to even bend the curve more by applying logarithm twice 
{'2 - 8 - 1.8 TF Transformation (00-09-31).srt : 00:05:36,110 --> 00:05:41,550': 'or we might want to even bend the curve more by applying logarithm twice'}
--------------------------------------------------
3 tried 1.7445380593498679e-06 now people have tried all these methods and they are indeed working better than the linear form of the transformation but so far what works the best seems to be this special transformation called a bm transformation 
{'2 - 8 - 1.8 TF Transformation (00-09-31).srt : 00:05:42,730 --> 00:05:46,990': 'now people have tried all these methods and they are indeed working better than'}
--------------------------------------------------
3 bounded 1.7445380593498679e-06 so it upper bounded by k plus  
{'2 - 8 - 1.8 TF Transformation (00-09-31).srt : 00:06:28,060 --> 00:06:29,830': 'so it upper bounded by k plus '}
--------------------------------------------------
4 upperbound 1.2213278741915971e-06 which it doesnt have upperbound 
{'2 - 8 - 1.8 TF Transformation (00-09-31).srt : 00:06:37,010 --> 00:06:39,442': 'which it doesnt have upperbound '}
--------------------------------------------------
2 summarise 1.2213278741915971e-06 so to summarise this lecture the main point is that we need to do some sub linearity of tf transformation 
{'2 - 8 - 1.8 TF Transformation (00-09-31).srt : 00:08:12,300 --> 00:08:14,308': 'so to summarise this lecture '}
--------------------------------------------------
16 linearity 1.2213278741915971e-06 so to summarise this lecture the main point is that we need to do some sub linearity of tf transformation 
{'2 - 8 - 1.8 TF Transformation (00-09-31).srt : 00:08:14,308 --> 00:08:19,910': 'the main point is that we need to do some sub linearity of tf transformation'}
--------------------------------------------------
9 diminishing 1.7445380593498679e-06 and this is needed to capture the intuition of diminishing return from high term counts 
{'2 - 8 - 1.8 TF Transformation (00-09-31).srt : 00:08:19,910 --> 00:08:24,340': 'and this is needed to capture the intuition of diminishing return from'}
--------------------------------------------------
3 hiding 2.4426557483831942e-06 so in generally hiding marginal space such as  corresponds to a hyper plain 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:01:28,830 --> 00:01:33,990': 'so in generally hiding marginal space such as', '5 - 2 - 4.2 Web Indexing (00-17-19).srt : 00:04:05,668 --> 00:04:10,852': 'and so this framework is hiding a lot of '}
--------------------------------------------------
11 hyper 1.2213278741915971e-06 so in generally hiding marginal space such as  corresponds to a hyper plain 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:01:33,990 --> 00:01:37,070': 'corresponds to a hyper plain '}
--------------------------------------------------
12 plain 1.7445380593498679e-06 so in generally hiding marginal space such as  corresponds to a hyper plain 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:01:33,990 --> 00:01:37,070': 'corresponds to a hyper plain '}
--------------------------------------------------
3 separates 1.7445380593498679e-06 intuitively this line separates these two categories so we expect the points on one side would be positive and the points on the other side would be negative 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:03:09,610 --> 00:03:15,343': 'intuitively this line separates these two categories so we expect the points on'}
--------------------------------------------------
13 clauses 1.2213278741915971e-06 but the question is when we have multiple lines that can separate both clauses which align the best 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:05:12,310 --> 00:05:15,950': 'separate both clauses which align the best'}
--------------------------------------------------
4 lowercase 2.2677482445081393e-06 so im also using lowercase b to denote the beta  a biased constant 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:07:12,734 --> 00:07:19,030': 'so im also using lowercase b to denote the beta a biased constant'}
--------------------------------------------------
12 notations 2.4426557483831942e-06 now i use this way so that it more consistent with what notations people usually use when they talk about svm 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:08:16,713 --> 00:08:21,267': 'now i use this way so that it more consistent with what notations', '5 - 13 - 4.7 Recommender Systems- Collaborative Filtering - Part 2 (00-12-09).srt : 00:01:05,110 --> 00:01:11,700': 'so here the general idea and we use some notations here so'}
--------------------------------------------------
6 margins 2.4426557483831942e-06 okay so when we maximize the margins of a separator it just means the boundary of the separator is only determined by a few data points and these are the data points that we call support vectors 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:08:31,190 --> 00:08:39,780': 'okay so when we maximize the margins of a separator', '4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:10:32,406 --> 00:10:37,510': 'values to optimize the margins and then the training error'}
--------------------------------------------------
2 quotas 1.2213278741915971e-06 and these quotas define the margin basically and you can imagine once we know which are supportive vectors then this center separator line will be determined by them 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:08:56,220 --> 00:09:00,900': 'and these quotas define the margin basically and'}
--------------------------------------------------
16 supportive 1.2213278741915971e-06 and these quotas define the margin basically and you can imagine once we know which are supportive vectors then this center separator line will be determined by them 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:09:00,900 --> 00:09:05,350': 'you can imagine once we know which are supportive vectors then this'}
--------------------------------------------------
17 margins 2.4426557483831942e-06 so in the linear svm we are going to then seek these parameter values to optimize the margins and then the training error 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:08:31,190 --> 00:08:39,780': 'okay so when we maximize the margins of a separator', '4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:10:32,406 --> 00:10:37,510': 'values to optimize the margins and then the training error'}
--------------------------------------------------
15 inequality 2.2677482445081393e-06 but when y i is  you also see that this is equivalent to the other inequality 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:12:39,968 --> 00:12:48,020': 'but when y i is you also see that this is equivalent to the other inequality'}
--------------------------------------------------
30 phi 2.4426557483831942e-06 now we also have the objective that tied into a maximization of margin and this is simply to minimize w transpose multiplied by w and we often denote this by phi of w so now you can see this is basically a optimization problem 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:14:03,013 --> 00:14:06,251': 'and we often denote this by phi of w ', '4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:06:55,260 --> 00:07:00,290': 'so the class phi will take any value in x as input and'}
--------------------------------------------------
17 separable 2.006143151929004e-06 but in this case we have a soft margin because the data points may not be completely separable 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:15:12,270 --> 00:15:16,000': 'because the data points may not be completely separable'}
--------------------------------------------------
7 minimized 1.2213278741915971e-06 so because xi i needs to be minimized in order to control the error 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:16:37,570 --> 00:16:41,940': 'so because xi i needs to be minimized in order to control the error'}
--------------------------------------------------
13 achievable 1.2213278741915971e-06 here you need to optimize the c and this is in general also achievable by doing crossvalidation 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:18:35,510 --> 00:18:40,510': 'and this is in general also achievable by doing crossvalidation'}
--------------------------------------------------
16 crossvalidation 1.2213278741915971e-06 here you need to optimize the c and this is in general also achievable by doing crossvalidation 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:18:35,510 --> 00:18:40,510': 'and this is in general also achievable by doing crossvalidation'}
--------------------------------------------------
11 pros 1.2213278741915971e-06 so there still no clear winner although each one has its pros and cons 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:19:32,230 --> 00:19:37,920': 'so there still no clear winner although each one has its pros and cons'}
--------------------------------------------------
13 cons 1.2213278741915971e-06 so there still no clear winner although each one has its pros and cons 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:19:32,230 --> 00:19:37,920': 'so there still no clear winner although each one has its pros and cons'}
--------------------------------------------------
17 supervising 1.2213278741915971e-06 as long as we have humans to help annotate some training data sets and design features then supervising machine learning and all these classifiers can be easily applied to those problems to solve the categorization problem to allow us to characterize content of text concisely with categories 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:21:17,554 --> 00:21:23,493': 'design features then supervising machine learning and all these classifiers'}
--------------------------------------------------
44 concisely 1.2213278741915971e-06 as long as we have humans to help annotate some training data sets and design features then supervising machine learning and all these classifiers can be easily applied to those problems to solve the categorization problem to allow us to characterize content of text concisely with categories 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:21:29,255 --> 00:21:34,431': 'allow us to characterize content of text concisely with categories'}
--------------------------------------------------
14 plenty 1.2213278741915971e-06 but in order to achieve good performance they all require effective features and also plenty of training data 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:22:02,240 --> 00:22:03,750': 'also plenty of training data '}
--------------------------------------------------
7 widths 1.2213278741915971e-06 of the model meaning to recognize the widths 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:24:04,658 --> 00:24:07,538': 'of the model meaning to recognize the widths'}
--------------------------------------------------
2 factorization 1.2213278741915971e-06 so metrics factorization has been used to do such a job and this is some of the techniques are actually very similar to the talking models that well discuss 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:24:33,150 --> 00:24:38,150': 'so metrics factorization has been used to do'}
--------------------------------------------------
2 morals 2.4426557483831942e-06 so talking morals like psa or lda can actually help us reduce the dimension of features 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:24:44,820 --> 00:24:48,220': 'so talking morals like psa or ', '4 - 9 - 3.9 Text Categorization- Generative Probabilistic Models (00-31-18).srt : 00:16:35,940 --> 00:16:41,720': 'in fact the smoothing is a general problem in older estimate of language morals'}
--------------------------------------------------
4 psa 1.2213278741915971e-06 so talking morals like psa or lda can actually help us reduce the dimension of features 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:24:44,820 --> 00:24:48,220': 'so talking morals like psa or '}
--------------------------------------------------
8 supervise 1.2213278741915971e-06 especially we could also use the categories to supervise the learning of such low dimensional structures 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:25:21,720 --> 00:25:26,200': 'especially we could also use the categories to supervise the learning of'}
--------------------------------------------------
4 transpire 1.2213278741915971e-06 that it highly nonlinear transpire and some recent events that allowed us to train such a complex network effectively 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:26:07,110 --> 00:26:11,570': 'that it highly nonlinear transpire and '}
--------------------------------------------------
19 replantations 1.2213278741915971e-06 and one important advantage of this approach in relationship with the featured design is that they can learn intermediate replantations or compound the features automatically 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:26:39,010 --> 00:26:43,920': 'learn intermediate replantations or compound the features automatically'}
--------------------------------------------------
21 compound 1.2213278741915971e-06 and one important advantage of this approach in relationship with the featured design is that they can learn intermediate replantations or compound the features automatically 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:26:39,010 --> 00:26:43,920': 'learn intermediate replantations or compound the features automatically'}
--------------------------------------------------
11 recalibration 1.2213278741915971e-06 and this is very valuable for learning effective replantation for text recalibration 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:26:49,193 --> 00:26:51,660': 'for text recalibration '}
--------------------------------------------------
7 exemplary 1.2213278741915971e-06 although in text domain because words are exemplary representation of text content because these are human imaging for communication 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:26:51,660 --> 00:26:57,390': 'although in text domain because words are exemplary representation of text content'}
--------------------------------------------------
16 imaging 2.4426557483831942e-06 although in text domain because words are exemplary representation of text content because these are human imaging for communication 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:26:57,390 --> 00:27:01,620': 'because these are human imaging for communication', '5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:02:43,050 --> 00:02:48,070': 'scalability in particular google imaging of mapreduce'}
--------------------------------------------------
3 revenue 1.2213278741915971e-06 and the speech revenue where they are anchored corresponding where the design that worked as features 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:27:22,610 --> 00:27:26,490': 'and the speech revenue where they are anchored corresponding'}
--------------------------------------------------
7 anchored 1.2213278741915971e-06 and the speech revenue where they are anchored corresponding where the design that worked as features 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:27:22,610 --> 00:27:26,490': 'and the speech revenue where they are anchored corresponding'}
--------------------------------------------------
5 categorizer 1.7445380593498679e-06 so to train a of categorizer meaning we want to positive or negative 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:28:13,220 --> 00:28:21,250': 'so to train a of categorizer meaning we want to positive or negative'}
--------------------------------------------------
14 semisupervised 1.2213278741915971e-06 another idea is to exploit the unlabeled data and there are techniques called the semisupervised machine learning techniques that can allow you to combine labeled data with unlabeled data 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:28:47,970 --> 00:28:50,830': 'there are techniques called the semisupervised machine'}
--------------------------------------------------
28 liable 1.2213278741915971e-06 basically we can use let say a to classify all of the unlabeled text documents and then were going to assume the high confidence classification results are actually liable 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:29:48,480 --> 00:29:54,040': 'assume the high confidence classification results are actually liable'}
--------------------------------------------------
10 enabler 1.2213278741915971e-06 then you suddenly have more training data because from the enabler that we now know some are labeled as category one some are labeled as category two 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:29:54,040 --> 00:29:58,600': 'then you suddenly have more training data because from the enabler that we'}
--------------------------------------------------
24 adaptation 1.2213278741915971e-06 when the enabled data and the training data are very different and we might need to use other advanced machine learning techniques called domain adaptation or transfer learning 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:30:32,410 --> 00:30:35,150': 'called domain adaptation or transfer learning'}
--------------------------------------------------
5 borrow 1.2213278741915971e-06 this is when we can borrow some training examples from a related problem that may be different 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:30:37,580 --> 00:30:42,450': 'borrow some training examples from a related problem that may be different'}
--------------------------------------------------
4 password 1.2213278741915971e-06 or from a categorization password that follow very different distribution from what we are working on 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:30:42,450 --> 00:30:44,470': 'or from a categorization password '}
--------------------------------------------------
16 vine 1.2213278741915971e-06 so for example training categorization on news might not give you effective plus y for class vine topics and tweets 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:31:07,270 --> 00:31:12,410': 'give you effective plus y for class vine topics and tweets'}
--------------------------------------------------
3 mission 1.2213278741915971e-06 so there are mission learning techniques that can help you do that effectively 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:31:19,490 --> 00:31:25,470': 'so there are mission learning techniques that can help you do that effectively'}
--------------------------------------------------
17 deterministically 1.2213278741915971e-06 that were going to say it about sports things like that and this would allow us to deterministically decide which category a document that should be put into 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:00:55,650 --> 00:00:59,909': 'to deterministically decide which category a document that should be put into'}
--------------------------------------------------
4 official 1.2213278741915971e-06 so that means some official features like keywords or punctuations or whatever you can easily identify in text to data 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:01:31,310 --> 00:01:36,430': 'so that means some official features like keywords or'}
--------------------------------------------------
9 punctuations 1.2213278741915971e-06 so that means some official features like keywords or punctuations or whatever you can easily identify in text to data 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:01:36,430 --> 00:01:40,400': 'punctuations or whatever you can easily identify in text to data'}
--------------------------------------------------
15 padding 1.2213278741915971e-06 and that would be most effective because we can easily use such a vocabulary or padding of such a vocabulary to recognize this category 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:01:53,100 --> 00:01:56,410': 'padding of such a vocabulary to recognize this category'}
--------------------------------------------------
5 intensive 2.4426557483831942e-06 first off because it label intensive it requires a lot of manual work 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:02:22,790 --> 00:02:27,010': 'first off because it label intensive it requires a lot of manual work', '3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).srt : 00:01:44,980 --> 00:01:47,690': 'it very labor intensive '}
--------------------------------------------------
6 scratch 1.2213278741915971e-06 we have to do it from scratch for a different problem 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:02:30,730 --> 00:02:35,190': 'we have to do it from scratch for a different problem'}
--------------------------------------------------
12 cures 1.2213278741915971e-06 but one can also imagine some types of articles that mention these cures but may not be exactly about sports or only marginally touching sports 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:03:09,710 --> 00:03:15,650': 'but one can also imagine some types of articles that mention these cures but'}
--------------------------------------------------
23 touching 2.4426557483831942e-06 but one can also imagine some types of articles that mention these cures but may not be exactly about sports or only marginally touching sports 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:03:15,650 --> 00:03:21,470': 'may not be exactly about sports or only marginally touching sports', '3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:15:59,960 --> 00:16:03,870': 'and we exploit zipf law avoid touching many documents'}
--------------------------------------------------
6 inconsistent 1.2213278741915971e-06 and then finally the rules maybe inconsistent and this would lead to robustness 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:03:30,470 --> 00:03:34,630': 'and then finally the rules maybe inconsistent and'}
--------------------------------------------------
19 contradictory 1.2213278741915971e-06 and you will also have to decide an order of applying the rules or combination of results that are contradictory 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:03:52,570 --> 00:03:57,070': 'or combination of results that are contradictory'}
--------------------------------------------------
11 alleviated 2.2677482445081393e-06 and it turns out that both problems can be solved or alleviated by using machine learning 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:04:04,580 --> 00:04:06,179': 'alleviated by using machine learning '}
--------------------------------------------------
7 marks 2.4426557483831942e-06 but i still put automatic in quotation marks because they are not really completely automatic cause it still require many work 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:04:13,190 --> 00:04:16,750': 'but i still put automatic in quotation marks because', '5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:03:43,090 --> 00:03:47,960': 'that why we put causal in quotation marks'}
--------------------------------------------------
28 ancients 1.2213278741915971e-06 so using each has a feature is a very common choice to start with but of course there are other sophisticated features like phrases or even parts of ancients tags or even syntax to the structures 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:05:08,860 --> 00:05:13,020': 'even parts of ancients tags or even syntax to the structures'}
--------------------------------------------------
31 deterministic 1.2213278741915971e-06 so soft rules just means were going to get decided which category we should be assigned for a document but it not going to be use using a rule that is deterministic 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:05:27,900 --> 00:05:32,990': 'but it not going to be use using a rule that is deterministic'}
--------------------------------------------------
12 games 1.4829329667707326e-06 so we might use something similar to saying that if it matches games sports many times it likely to be sports 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:05:32,990 --> 00:05:38,690': 'so we might use something similar to saying that if it matches games'}
--------------------------------------------------
7 evidences 1.2213278741915971e-06 so that we can combine much more evidences 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:05:47,180 --> 00:05:50,380': 'so that we can combine much more evidences'}
--------------------------------------------------
21 supervisement 1.2213278741915971e-06 so when we use machine learning for text categorization we can also talk about the problem in the general setting of supervisement 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:06:33,195 --> 00:06:39,295': 'talk about the problem in the general setting of supervisement'}
--------------------------------------------------
3 phi 2.4426557483831942e-06 so the class phi will take any value in x as input and would generate a value in y as output 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:14:03,013 --> 00:14:06,251': 'and we often denote this by phi of w ', '4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:06:55,260 --> 00:07:00,290': 'so the class phi will take any value in x as input and'}
--------------------------------------------------
12 behaves 1.2213278741915971e-06 and then the computer going to figure out the how the function behaves like based on this examples 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:07:27,120 --> 00:07:30,680': 'how the function behaves like based on this examples'}
--------------------------------------------------
12 tradeoffs 1.7445380593498679e-06 but nonlinear models might be more complex for training so there are tradeoffs as well 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:08:43,250 --> 00:08:45,880': 'training so there are tradeoffs as well '}
--------------------------------------------------
3 nave 2.4426557483831942e-06 one example is nave bayes classifier in this case 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:10:38,080 --> 00:10:43,470': 'one example is nave bayes classifier in this case', '4 - 9 - 3.9 Text Categorization- Generative Probabilistic Models (00-31-18).srt : 00:28:24,815 --> 00:28:30,465': 'and this is estimated as the log of probability ratio here in nave bayes'}
--------------------------------------------------
15 systemgenerated 2.4426557483831942e-06 so in direct evaluation we want to answer the following questions how close are the systemgenerated clusters to the ideal clusters that are generated by humans 
{'4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:01:29,310 --> 00:01:34,060': 'we want to answer the following questions how close are the systemgenerated', '4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:03:08,650 --> 00:03:12,590': 'so we would like to then quantify the similarity between the systemgenerated'}
--------------------------------------------------
2 closeness 2.4426557483831942e-06 so the closeness here can be assessed from multiple perspectives and that will help us characterize the quality of cluster result in multiple angles and this is sometimes desirable 
{'4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:01:38,580 --> 00:01:43,040': 'so the closeness here can be assessed ', '4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:01:56,790 --> 00:02:04,010': 'now we also want to quantify the closeness because this would allow'}
--------------------------------------------------
6 assessed 1.2213278741915971e-06 so the closeness here can be assessed from multiple perspectives and that will help us characterize the quality of cluster result in multiple angles and this is sometimes desirable 
{'4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:01:38,580 --> 00:01:43,040': 'so the closeness here can be assessed '}
--------------------------------------------------
7 closeness 2.4426557483831942e-06 now we also want to quantify the closeness because this would allow us to easily compare different measures based on their performance figures 
{'4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:01:38,580 --> 00:01:43,040': 'so the closeness here can be assessed ', '4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:01:56,790 --> 00:02:04,010': 'now we also want to quantify the closeness because this would allow'}
--------------------------------------------------
22 figures 1.2213278741915971e-06 now we also want to quantify the closeness because this would allow us to easily compare different measures based on their performance figures 
{'4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:02:04,010 --> 00:02:08,500': 'us to easily compare different measures based on their performance figures'}
--------------------------------------------------
26 desire 1.7445380593498679e-06 and finally you can see in this case we essentially inject the clustering bias by using humans basically humans would bring in the the need or desire to clustering bias 
{'4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:02:21,660 --> 00:02:24,260': 'desire to clustering bias '}
--------------------------------------------------
11 systemgenerated 2.4426557483831942e-06 so we would like to then quantify the similarity between the systemgenerated clusters and the gold standard clusters 
{'4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:01:29,310 --> 00:01:34,060': 'we want to answer the following questions how close are the systemgenerated', '4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:03:08,650 --> 00:03:12,590': 'so we would like to then quantify the similarity between the systemgenerated'}
--------------------------------------------------
16 meshes 2.4426557483831942e-06 and this similarity can also be measure from multiple perspectives and this will give us various meshes to quantitatively evaluate a cluster a clustering result 
{'4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:03:20,110 --> 00:03:26,750': 'give us various meshes to quantitatively evaluate a cluster a clustering result', '2 - 6 - 1.6 Vector Space Model- Simplest Instantiation (00-17-30).srt : 00:12:58,740 --> 00:13:03,820': 'meshes how many unique query terms are matched in a document'}
--------------------------------------------------
9 purity 1.7445380593498679e-06 and some of the commonly used measures include the purity which measures whether a cluster has a similar object from the same cluster in the gold standard 
{'4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:03:26,750 --> 00:03:34,140': 'and some of the commonly used measures include the purity which measures whether'}
--------------------------------------------------
15 identity 2.4426557483831942e-06 and normalized mutual information is a commonly used measure which basically measures based on the identity of cluster of object in the system generally 
{'4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:03:45,545 --> 00:03:50,485': 'which basically measures based on the identity of', '4 - 2 - 3.2 Text Clustering- Generative Probabilistic Models Part 1 (00-16-18).srt : 00:10:26,810 --> 00:10:31,165': 'that would allow us to recover the cluster identity of a document'}
--------------------------------------------------
18 quantifying 1.2213278741915971e-06 and mutual information captures the correlation between these cluster labels and normalized mutual information is often used for quantifying the similarity for this evaluation purpose f measure is another possible measure 
{'4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:04:11,100 --> 00:04:15,300': 'used for quantifying the similarity for '}
--------------------------------------------------
2 wise 2.4426557483831942e-06 now procedure wise we also would create a test set with text objects for the intended application to quantify the performance of the system 
{'4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:05:19,098 --> 00:05:25,120': 'now procedure wise we also would create a test set with text objects for', '5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:14:21,230 --> 00:14:26,580': 'whether we can actually get topics that are wise for the time series'}
--------------------------------------------------
21 implied 1.2213278741915971e-06 now sometimes you may see some methods that can automatically determine the number of clusters but in general that has some implied application of clustering bias there and that just not specified 
{'4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:08:08,515 --> 00:08:13,575': 'the number of clusters but in general that has some implied'}
--------------------------------------------------
10 fitness 2.4426557483831942e-06 in other situations we might be able to use the fitness to data to assess whether weve got a good number of clusters to explain our data well 
{'4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:08:46,880 --> 00:08:53,470': 'in other situations we might be able to use the fitness to data', '4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:09:23,930 --> 00:09:28,350': 'is as you add more components would you be able to significantly improve the fitness'}
--------------------------------------------------
28 fitness 2.4426557483831942e-06 so you cant in general fit the data worse than before but the question is as you add more components would you be able to significantly improve the fitness of the data and that can be used to determine the right number of clusters 
{'4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:08:46,880 --> 00:08:53,470': 'in other situations we might be able to use the fitness to data', '4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:09:23,930 --> 00:09:28,350': 'is as you add more components would you be able to significantly improve the fitness'}
--------------------------------------------------
13 blind 2.006143151929004e-06 there is another form of feedback called a pseudo relevance feedback or a blind feedback also called an automatic feedback 
{'4 - 8 - 3.6 Feedback in Text Retrieval (00-06-49).srt : 00:02:00,663 --> 00:02:03,860': 'or a blind feedback also called an automatic feedback'}
--------------------------------------------------
8 doubt 2.4426557483831942e-06 so this is factored for improving the search doubt 
{'4 - 8 - 3.6 Feedback in Text Retrieval (00-06-49).srt : 00:04:11,420 --> 00:04:18,378': 'so this is factored for improving the search doubt', '2 - 11 - 1.11 Syntagmatic Relation Discovery- Conditional Entropy (00-11-57).srt : 00:07:22,870 --> 00:07:27,763': 'no doubt smaller entropy means easier for prediction'}
--------------------------------------------------
4 relevancy 1.2213278741915971e-06 but of course pseudo relevancy feedback is completely unreliable 
{'4 - 8 - 3.6 Feedback in Text Retrieval (00-06-49).srt : 00:04:18,378 --> 00:04:21,754': 'but of course pseudo relevancy feedback is completely unreliable'}
--------------------------------------------------
8 unreliable 1.7445380593498679e-06 but of course pseudo relevancy feedback is completely unreliable 
{'4 - 8 - 3.6 Feedback in Text Retrieval (00-06-49).srt : 00:04:18,378 --> 00:04:21,754': 'but of course pseudo relevancy feedback is completely unreliable'}
--------------------------------------------------
3 arbitrarily 1.7445380593498679e-06 we have to arbitrarily set a cutoff 
{'4 - 8 - 3.6 Feedback in Text Retrieval (00-06-49).srt : 00:04:21,754 --> 00:04:23,961': 'we have to arbitrarily set a cutoff '}
--------------------------------------------------
9 interacts 1.2213278741915971e-06 instead we are going to observe how the user interacts with the search results 
{'4 - 8 - 3.6 Feedback in Text Retrieval (00-06-49).srt : 00:04:33,503 --> 00:04:38,503': 'instead we are going to observe how the user interacts with the search results'}
--------------------------------------------------
15 activities 2.4426557483831942e-06 you know think about google and bing and they can collect a lot of user activities 
{'4 - 8 - 3.6 Feedback in Text Retrieval (00-06-49).srt : 00:05:39,545 --> 00:05:45,086': 'you know think about google and bing and they can collect a lot of user activities', '2 - 1 - 1.1 Overview Text Mining and Analytics- Part 1 (00-11-43).srt : 00:08:02,580 --> 00:08:04,873': 'or activities in the network and are reported'}
--------------------------------------------------
3 serverless 1.2213278741915971e-06 why they are serverless 
{'4 - 8 - 3.6 Feedback in Text Retrieval (00-06-49).srt : 00:05:45,086 --> 00:05:46,770': 'why they are serverless '}
--------------------------------------------------
3 exquisite 1.2213278741915971e-06 where the use exquisite judgement it takes some used effort but the judgement that information is reliable 
{'4 - 8 - 3.6 Feedback in Text Retrieval (00-06-49).srt : 00:06:04,378 --> 00:06:08,206': 'where the use exquisite judgement it takes some used effort but'}
--------------------------------------------------
2 tfi 2.4426557483831942e-06 we have tfi tf weight here 
{'4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt : 00:01:11,218 --> 00:01:13,920': 'we have tfi tf weight here ', '3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:16:23,940 --> 00:16:28,842': 'and they also tend to have high tfi diff weights in these intermediate'}
--------------------------------------------------
8 mercer 1.4829329667707326e-06 and this is also called a jelinek and mercer smoothing 
{'4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt : 00:01:57,420 --> 00:01:59,910': 'and this is also called a jelinek and mercer smoothing'}
--------------------------------------------------
4 duration 1.2213278741915971e-06 it often called a duration of the ply or bayesian smoothing 
{'4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt : 00:04:49,880 --> 00:04:54,540': 'it often called a duration of the ply or bayesian smoothing'}
--------------------------------------------------
7 ply 1.2213278741915971e-06 it often called a duration of the ply or bayesian smoothing 
{'4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt : 00:04:49,880 --> 00:04:54,540': 'it often called a duration of the ply or bayesian smoothing'}
--------------------------------------------------
8 counters 1.7445380593498679e-06 so this is the total number of pseudo counters that we added 
{'4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt : 00:07:57,380 --> 00:08:00,200': 'so this is the total number of pseudo counters that we added'}
--------------------------------------------------
1 james 1.7445380593498679e-06 the james move method this is the constant 
{'4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt : 00:09:39,211 --> 00:09:42,545': 'the james move method this is the constant'}
--------------------------------------------------
5 devices 2.2677482445081393e-06 in contrast we have other devices such as video recorder that can report what happening in the real world objective to generate the viewer data for example 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:00:43,450 --> 00:00:50,880': 'in contrast we have other devices such as video recorder that can report what'}
--------------------------------------------------
9 recorder 1.2213278741915971e-06 in contrast we have other devices such as video recorder that can report what happening in the real world objective to generate the viewer data for example 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:00:43,450 --> 00:00:50,880': 'in contrast we have other devices such as video recorder that can report what'}
--------------------------------------------------
23 viewer 2.4426557483831942e-06 in contrast we have other devices such as video recorder that can report what happening in the real world objective to generate the viewer data for example 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:00:50,880 --> 00:00:56,670': 'happening in the real world objective to generate the viewer data for example', '5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:08:30,638 --> 00:08:35,740': 'which aspects are more important to a viewer can be revealed as well'}
--------------------------------------------------
6 advantaged 1.2213278741915971e-06 now this is actually a unique advantaged of text data as compared with other data because the office is a great opportunity to understand the observers 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:01:16,730 --> 00:01:22,170': 'now this is actually a unique advantaged of text data as compared with other data'}
--------------------------------------------------
25 observers 2.006143151929004e-06 now this is actually a unique advantaged of text data as compared with other data because the office is a great opportunity to understand the observers 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:01:22,170 --> 00:01:28,190': 'because the office is a great opportunity to understand the observers'}
--------------------------------------------------
2 highlighted 1.2213278741915971e-06 now i highlighted quite a few words here 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:02:08,770 --> 00:02:11,140': 'now i highlighted quite a few words here '}
--------------------------------------------------
5 differentiating 1.7445380593498679e-06 and this is a key differentiating factor from opinions which tends to be not easy to prove wrong or right because it reflects what the person thinks about something 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:02:45,180 --> 00:02:49,470': 'and this is a key differentiating factor from opinions'}
--------------------------------------------------
4 posted 1.7445380593498679e-06 when the review is posted usually you cant such information easier 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:06:21,340 --> 00:06:26,210': 'when the review is posted usually you cant such information easier'}
--------------------------------------------------
2 tasker 1.2213278741915971e-06 and the tasker is in general harder 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:07:21,410 --> 00:07:24,990': 'and the tasker is in general harder '}
--------------------------------------------------
12 connecticut 2.2677482445081393e-06 so we can identify opinion holder here and that the governor of connecticut 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:07:24,990 --> 00:07:31,700': 'so we can identify opinion holder here and that the governor of connecticut'}
--------------------------------------------------
13 worst 2.2677482445081393e-06 well there a negative sentiment here that indicated by words like bad and worst 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:07:49,660 --> 00:07:52,029': 'that indicated by words like bad and worst'}
--------------------------------------------------
8 england 1.7445380593498679e-06 and we can also then identify context new england in this case 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:07:53,360 --> 00:07:59,270': 'and we can also then identify context new england in this case'}
--------------------------------------------------
4 playoff 1.7445380593498679e-06 now unlike in the playoff review all these elements must be extracted by using natural ram processing techniques 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:08:00,420 --> 00:08:03,300': 'now unlike in the playoff review '}
--------------------------------------------------
15 ram 1.2213278741915971e-06 now unlike in the playoff review all these elements must be extracted by using natural ram processing techniques 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:08:03,300 --> 00:08:08,370': 'all these elements must be extracted by using natural ram processing techniques'}
--------------------------------------------------
6 committee 1.7445380593498679e-06 sometimes the opinion was from a committee 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:08:50,870 --> 00:08:53,190': 'sometimes the opinion was from a committee'}
--------------------------------------------------
4 country 1.7445380593498679e-06 or from a whole country of people 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:08:53,190 --> 00:08:55,060': 'or from a whole country of people '}
--------------------------------------------------
15 ect 1.2213278741915971e-06 it can be about one entity a particular person a particular product a particular policy ect 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:09:02,870 --> 00:09:04,830': 'a particular policy ect '}
--------------------------------------------------
16 onesentence 1.7445380593498679e-06 now opinion content of course can also vary a lot on the surface you can identify onesentence opinion or onephrase opinion 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:09:38,680 --> 00:09:42,050': 'you can identify onesentence opinion or onephrase opinion'}
--------------------------------------------------
19 onephrase 1.7445380593498679e-06 now opinion content of course can also vary a lot on the surface you can identify onesentence opinion or onephrase opinion 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:09:38,680 --> 00:09:42,050': 'you can identify onesentence opinion or onephrase opinion'}
--------------------------------------------------
11 damage 1.2213278741915971e-06 and furthermore we identify the variation in the sentiment or emotion damage that above the feeding of the opinion holder 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:09:51,550 --> 00:09:56,520': 'emotion damage that above the feeding of the opinion holder'}
--------------------------------------------------
15 feeding 1.7445380593498679e-06 and furthermore we identify the variation in the sentiment or emotion damage that above the feeding of the opinion holder 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:09:51,550 --> 00:09:56,520': 'emotion damage that above the feeding of the opinion holder'}
--------------------------------------------------
9 targeting 1.7445380593498679e-06 first the observer might make a comment about opinion targeting observe the word so in case we have the author opinion 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:10:46,810 --> 00:10:50,440': 'first the observer might make a comment about opinion targeting'}
--------------------------------------------------
1 complication 1.7445380593498679e-06 another complication is that there may be indirect opinions or inferred opinions that can be obtained 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:11:51,400 --> 00:11:56,240': 'another complication is that there may be indirect opinions or'}
--------------------------------------------------
16 hour 1.7445380593498679e-06 for example one statement that might be this phone ran out of battery in just one hour 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:12:10,190 --> 00:12:14,260': 'this phone ran out of battery in just one hour'}
--------------------------------------------------
4 wished 1.2213278741915971e-06 the opinion holder clearly wished that the battery do last longer 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:12:37,160 --> 00:12:40,520': 'the opinion holder clearly wished that the battery do last longer'}
--------------------------------------------------
22 commend 1.2213278741915971e-06 instead again all the sentences that are about the opinions are useful for understanding the person or understanding the product that we commend 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:13:15,240 --> 00:13:18,420': 'understanding the person or understanding the product that we commend'}
--------------------------------------------------
11 textualized 1.2213278741915971e-06 so the task of opinion mining can be defined as taking textualized input to generate a set of opinion representations 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:13:19,520 --> 00:13:24,530': 'so the task of opinion mining can be defined as taking textualized input'}
--------------------------------------------------
19 buying 1.2213278741915971e-06 we often look at other people opinions look at read the reviews in order to make a decisions like buying a product or using a service 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:14:47,180 --> 00:14:51,100': 'in order to make a decisions like buying a product or using a service'}
--------------------------------------------------
11 whom 2.2677482445081393e-06 we also would be interested in others opinions when we decide whom to vote for example 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:14:56,770 --> 00:14:59,330': 'when we decide whom to vote for example '}
--------------------------------------------------
2 makers 1.2213278741915971e-06 and policy makers may also want to know people opinions when designing a new policy 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:15:00,978 --> 00:15:02,030': 'and policy makers '}
--------------------------------------------------
9 voluntary 2.4426557483831942e-06 now the third kind of application can be called voluntary survey 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:15:48,560 --> 00:15:53,800': 'now the third kind of application can be called voluntary survey', '5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:17:12,310 --> 00:17:17,890': 'and these can be regarded as voluntary survey done by those people'}
--------------------------------------------------
13 surveys 2.4426557483831942e-06 now this is most important research that used to be done by doing surveys doing manual surveys 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:15:53,800 --> 00:15:59,360': 'now this is most important research that used to be done by doing surveys', '5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:15:59,360 --> 00:16:00,248': 'doing manual surveys '}
--------------------------------------------------
16 surveys 2.4426557483831942e-06 now this is most important research that used to be done by doing surveys doing manual surveys 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:15:53,800 --> 00:15:59,360': 'now this is most important research that used to be done by doing surveys', '5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:15:59,360 --> 00:16:00,248': 'doing manual surveys '}
--------------------------------------------------
4 informs 1.2213278741915971e-06 people need to feel informs to answer their questions 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:16:03,670 --> 00:16:07,680': 'people need to feel informs to answer their questions'}
--------------------------------------------------
8 oppinions 1.2213278741915971e-06 market research has to do with understanding consumers oppinions 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:16:37,090 --> 00:16:40,590': 'market research has to do with understanding consumers oppinions'}
--------------------------------------------------
5 directive 1.2213278741915971e-06 and this create very useful directive for that 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:16:40,590 --> 00:16:43,535': 'and this create very useful directive for that'}
--------------------------------------------------
0 datadriven 2.006143151929004e-06 datadriven social science research can benefit from this because they can do text mining to understand the people opinions 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:16:43,535 --> 00:16:48,840': 'datadriven social science research can benefit from this because they can'}
--------------------------------------------------
6 voluntary 2.4426557483831942e-06 and these can be regarded as voluntary survey done by those people 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:15:48,560 --> 00:15:53,800': 'now the third kind of application can be called voluntary survey', '5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:17:12,310 --> 00:17:17,890': 'and these can be regarded as voluntary survey done by those people'}
--------------------------------------------------
10 relates 1.2213278741915971e-06 so that a logistical function and you can see it relates this probability to probability that y to the feature values 
{'5 - 3 - 4.3 Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression (00-13-43).srt : 00:02:47,306 --> 00:02:52,421': 'you can see it relates this probability to'}
--------------------------------------------------
1 altogether 1.7445380593498679e-06 so altogether well have k classifiers 
{'5 - 3 - 4.3 Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression (00-13-43).srt : 00:04:11,989 --> 00:04:16,281': 'so altogether well have k classifiers '}
--------------------------------------------------
23 straight 1.2213278741915971e-06 now if we do that of course then we can also solve this problem and the logistical regression program will be also very straight forward as you have just seen on the previous slide 
{'5 - 3 - 4.3 Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression (00-13-43).srt : 00:04:23,580 --> 00:04:27,750': 'and the logistical regression program will be also very straight forward'}
--------------------------------------------------
12 twentyfive 2.4426557483831942e-06 the rating is k now what if it not as large as twentyfive 
{'5 - 3 - 4.3 Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression (00-13-43).srt : 00:06:02,540 --> 00:06:06,750': 'now what if it not as large as twentyfive', '5 - 3 - 4.3 Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression (00-13-43).srt : 00:06:20,660 --> 00:06:23,140': 'and if the probability is larger than twentyfive'}
--------------------------------------------------
7 twentyfive 2.4426557483831942e-06 and if the probability is larger than twentyfive then well say well then it k 
{'5 - 3 - 4.3 Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression (00-13-43).srt : 00:06:02,540 --> 00:06:06,750': 'now what if it not as large as twentyfive', '5 - 3 - 4.3 Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression (00-13-43).srt : 00:06:20,660 --> 00:06:23,140': 'and if the probability is larger than twentyfive'}
--------------------------------------------------
7 invoking 1.2213278741915971e-06 and so were going to just keep invoking these classifiers 
{'5 - 3 - 4.3 Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression (00-13-43).srt : 00:06:30,280 --> 00:06:34,990': 'and so were going to just keep invoking these classifiers'}
--------------------------------------------------
12 fives 1.2213278741915971e-06 now the second problems is that these problems these k minus  plus fives are not really independent 
{'5 - 3 - 4.3 Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression (00-13-43).srt : 00:08:10,751 --> 00:08:15,595': 'these k minus plus fives are not really independent'}
--------------------------------------------------
9 families 1.7445380593498679e-06 one is it going to reduce the number of families significantly 
{'5 - 3 - 4.3 Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression (00-13-43).srt : 00:09:34,370 --> 00:09:37,450': 'one is it going to reduce the number of families significantly'}
--------------------------------------------------
15 param 1.2213278741915971e-06 so if you think about it for a moment and you will see now the param we have far fewer parameters 
{'5 - 3 - 4.3 Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression (00-13-43).srt : 00:11:00,910 --> 00:11:05,415': 'you will see now the param we have far fewer parameters'}
--------------------------------------------------
9 tying 1.7445380593498679e-06 it turns out that with this this idea of tying all the parameters the beta values 
{'5 - 3 - 4.3 Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression (00-13-43).srt : 00:11:31,290 --> 00:11:39,730': 'it turns out that with this this idea of tying all the parameters the beta values'}
--------------------------------------------------
24 bracket 1.7445380593498679e-06 so this means now we can simply make a decision of rating by looking at the value of this scoring function and see which bracket it falls into 
{'5 - 3 - 4.3 Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression (00-13-43).srt : 00:12:21,820 --> 00:12:27,900': 'the value of this scoring function and see which bracket it falls into'}
--------------------------------------------------
2 ranges 1.7445380593498679e-06 because these ranges of alpha values correspond to the different levels of ratings and that from the way we train these alpha values 
{'5 - 3 - 4.3 Opinion Mining and Sentiment Analysis- Ordinal Logistic Regression (00-13-43).srt : 00:13:14,220 --> 00:13:19,750': 'because these ranges of alpha values correspond to the different'}
--------------------------------------------------
6 rethink 1.2213278741915971e-06 the user the author might have rethink other words 
{'4 - 4 - 3.4 Smoothing of Language Model - Part 1 (00-12-15).srt : 00:03:19,020 --> 00:03:22,910': 'the user the author might have rethink other words'}
--------------------------------------------------
25 abstractor 1.2213278741915971e-06 but imagine the user who is interested in the topic of this abstract the user might actually choose a word that is not in the abstractor to to use as query 
{'4 - 4 - 3.4 Smoothing of Language Model - Part 1 (00-12-15).srt : 00:04:03,030 --> 00:04:08,480': 'actually choose a word that is not in the abstractor to to use as query'}
--------------------------------------------------
7 attempted 1.2213278741915971e-06 so smoothing of the language model is attempted to to try to recover the model for the whole whole article 
{'4 - 4 - 3.4 Smoothing of Language Model - Part 1 (00-12-15).srt : 00:04:18,970 --> 00:04:23,450': 'so smoothing of the language model is attempted to'}
--------------------------------------------------
22 governed 1.7445380593498679e-06 that means if you dont observe the word in the data set were going to assume that its probability is kind of governed by another reference language model that we were constructing 
{'4 - 4 - 3.4 Smoothing of Language Model - Part 1 (00-12-15).srt : 00:05:12,400 --> 00:05:16,310': 'governed by another reference language model that we were constructing'}
--------------------------------------------------
31 constructing 2.2677482445081393e-06 that means if you dont observe the word in the data set were going to assume that its probability is kind of governed by another reference language model that we were constructing 
{'4 - 4 - 3.4 Smoothing of Language Model - Part 1 (00-12-15).srt : 00:05:12,400 --> 00:05:16,310': 'governed by another reference language model that we were constructing'}
--------------------------------------------------
5 reverse 2.2677482445081393e-06 and this is almost a reverse process of the first step here 
{'4 - 4 - 3.4 Smoothing of Language Model - Part 1 (00-12-15).srt : 00:10:05,880 --> 00:10:11,110': 'and this is almost a reverse process of the first step here'}
--------------------------------------------------
6 severity 1.2213278741915971e-06 in fact there is even more severity here 
{'4 - 4 - 3.4 Smoothing of Language Model - Part 1 (00-12-15).srt : 00:11:55,336 --> 00:11:59,378': 'in fact there is even more severity here '}
--------------------------------------------------
14 multiway 1.2213278741915971e-06 so the consequence is that our switch for choosing a topic is now a multiway switch 
{'3 - 13 - 2.13 Probabilistic Latent Semantic Analysis (PLSA)- Part 1 (00-10-38).srt : 00:03:43,930 --> 00:03:49,450': 'so the consequence is that our switch for choosing a topic is now a multiway switch'}
--------------------------------------------------
8 nonbackground 2.4426557483831942e-06 so it the background lambda sub b versus nonbackground 
{'3 - 13 - 2.13 Probabilistic Latent Semantic Analysis (PLSA)- Part 1 (00-10-38).srt : 00:03:59,660 --> 00:04:06,913': 'so it the background lambda sub b versus nonbackground', '3 - 13 - 2.13 Probabilistic Latent Semantic Analysis (PLSA)- Part 1 (00-10-38).srt : 00:04:11,490 --> 00:04:16,300': 'actually choosing a nonbackground topic '}
--------------------------------------------------
7 nonbackground 2.4426557483831942e-06 us the probability of actually choosing a nonbackground topic 
{'3 - 13 - 2.13 Probabilistic Latent Semantic Analysis (PLSA)- Part 1 (00-10-38).srt : 00:03:59,660 --> 00:04:06,913': 'so it the background lambda sub b versus nonbackground', '3 - 13 - 2.13 Probabilistic Latent Semantic Analysis (PLSA)- Part 1 (00-10-38).srt : 00:04:11,490 --> 00:04:16,300': 'actually choosing a nonbackground topic '}
--------------------------------------------------
6 designs 1.7445380593498679e-06 this is just the difference of designs 
{'3 - 13 - 2.13 Probabilistic Latent Semantic Analysis (PLSA)- Part 1 (00-10-38).srt : 00:04:31,450 --> 00:04:33,775': 'this is just the difference of designs '}
--------------------------------------------------
0 cough 1.2213278741915971e-06 cough excuse me 
{'3 - 13 - 2.13 Probabilistic Latent Semantic Analysis (PLSA)- Part 1 (00-10-38).srt : 00:07:55,008 --> 00:07:57,960': 'cough excuse me '}
--------------------------------------------------
1 excuse 1.2213278741915971e-06 cough excuse me 
{'3 - 13 - 2.13 Probabilistic Latent Semantic Analysis (PLSA)- Part 1 (00-10-38).srt : 00:07:55,008 --> 00:07:57,960': 'cough excuse me '}
--------------------------------------------------
2 xiang 1.2213278741915971e-06 im cheng xiang zhai 
{'1 - 1 - Course Welcome (00-03-11).srt : 00:00:12,312 --> 00:00:13,984': 'im cheng xiang zhai '}
--------------------------------------------------
3 nickname 2.4426557483831942e-06 i have a nickname cheng 
{'1 - 1 - Course Welcome (00-03-11).srt : 00:00:13,984 --> 00:00:16,430': 'i have a nickname cheng ', '1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:00:13,940 --> 00:00:15,110': 'i have a nickname cheng '}
--------------------------------------------------
3 lecturebased 1.2213278741915971e-06 there are five lecturebased courses as you see on the slide 
{'1 - 1 - Course Welcome (00-03-11).srt : 00:01:10,320 --> 00:01:15,270': 'there are five lecturebased courses as you see on the slide'}
--------------------------------------------------
2 capstone 2.4426557483831942e-06 plus one capstone project course in the end 
{'1 - 1 - Course Welcome (00-03-11).srt : 00:01:15,270 --> 00:01:19,250': 'plus one capstone project course in the end', '1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:00:44,770 --> 00:00:49,370': 'a capstone project course that all of us will teach together'}
--------------------------------------------------
1 patent 1.2213278741915971e-06 so patent discovery taught by the professor jowi han and cluster analysis again taught by him about the general data mining techniques to handle structure 
{'1 - 1 - Course Welcome (00-03-11).srt : 00:01:38,470 --> 00:01:43,635': 'so patent discovery taught by the professor jowi han and cluster analysis'}
--------------------------------------------------
7 jowi 1.2213278741915971e-06 so patent discovery taught by the professor jowi han and cluster analysis again taught by him about the general data mining techniques to handle structure 
{'1 - 1 - Course Welcome (00-03-11).srt : 00:01:38,470 --> 00:01:43,635': 'so patent discovery taught by the professor jowi han and cluster analysis'}
--------------------------------------------------
15 him 1.4829329667707326e-06 so patent discovery taught by the professor jowi han and cluster analysis again taught by him about the general data mining techniques to handle structure 
{'1 - 1 - Course Welcome (00-03-11).srt : 00:01:43,635 --> 00:01:49,480': 'again taught by him about the general data mining techniques to handle structure'}
--------------------------------------------------
8 jung 1.2213278741915971e-06 and data mine data visualization covered by professor jung hart is about the general visualization techniques 
{'1 - 1 - Course Welcome (00-03-11).srt : 00:01:52,330 --> 00:01:57,229': 'and data mine data visualization covered by professor jung hart is about'}
--------------------------------------------------
9 hart 2.4426557483831942e-06 and data mine data visualization covered by professor jung hart is about the general visualization techniques 
{'1 - 1 - Course Welcome (00-03-11).srt : 00:01:52,330 --> 00:01:57,229': 'and data mine data visualization covered by professor jung hart is about', '1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:00:39,380 --> 00:00:44,770': 'professor jiawei han professor john hart and me followed by'}
--------------------------------------------------
6 growing 2.4426557483831942e-06 and the data is everywhere is growing rapidly so you must have been experiencing this growth 
{'1 - 1 - Course Welcome (00-03-11).srt : 00:02:11,210 --> 00:02:15,130': 'and the data is everywhere is growing rapidly so', '1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:01:35,708 --> 00:01:39,651': 'text data has been growing dramatically recently'}
--------------------------------------------------
7 rapidly 1.2213278741915971e-06 and the data is everywhere is growing rapidly so you must have been experiencing this growth 
{'1 - 1 - Course Welcome (00-03-11).srt : 00:02:11,210 --> 00:02:15,130': 'and the data is everywhere is growing rapidly so'}
--------------------------------------------------
13 experiencing 1.2213278741915971e-06 and the data is everywhere is growing rapidly so you must have been experiencing this growth 
{'1 - 1 - Course Welcome (00-03-11).srt : 00:02:15,130 --> 00:02:19,240': 'you must have been experiencing this growth'}
--------------------------------------------------
9 capacity 1.7445380593498679e-06 so the amount of text data is beyond our capacity to understand them 
{'1 - 1 - Course Welcome (00-03-11).srt : 00:02:43,070 --> 00:02:48,483': 'so the amount of text data is beyond our capacity to understand them'}
--------------------------------------------------
10 discarding 1.2213278741915971e-06 and it because recommending useful items to people is like discarding or filtering out the useless articles 
{'5 - 10 - 4.6 Recommender Systems- Content-based Filtering - Part 1  (00-12-55).srt : 00:02:05,670 --> 00:02:10,764': 'discarding or filtering out the useless articles'}
--------------------------------------------------
42 cu 1.2213278741915971e-06 now note that in this case typically users information need is stable so the system would have a lot of opportunities to observe the users you know if the user has taken a recommended item as viewed that and this is a cu a signal to indicate that the recommended item may be relevant 
{'5 - 10 - 4.6 Recommender Systems- Content-based Filtering - Part 1  (00-12-55).srt : 00:04:58,530 --> 00:05:04,020': 'this is a cu a signal to indicate that the recommended item may be relevant'}
--------------------------------------------------
20 clock 1.2213278741915971e-06 and so such feedback can be a longterm feedback and can last for a long time and the system can clock collect a lot of information about this user interests 
{'5 - 10 - 4.6 Recommender Systems- Content-based Filtering - Part 1  (00-12-55).srt : 00:05:11,828 --> 00:05:16,550': 'the system can clock collect a lot of information about this user interests'}
--------------------------------------------------
21 incur 1.7445380593498679e-06 if you think about it you will see that when we get a big award for delivering a good document you incur only a small penalty for delivering a bad one 
{'5 - 10 - 4.6 Recommender Systems- Content-based Filtering - Part 1  (00-12-55).srt : 00:08:02,810 --> 00:08:08,410': 'delivering a good document you incur only a small penalty for delivering a bad one'}
--------------------------------------------------
4 encouraging 1.2213278741915971e-06 intuitively you would be encouraging to deliver more right 
{'5 - 10 - 4.6 Recommender Systems- Content-based Filtering - Part 1  (00-12-55).srt : 00:08:08,410 --> 00:08:11,740': 'intuitively you would be encouraging to deliver more right'}
--------------------------------------------------
17 prize 1.7445380593498679e-06 right so on the other hand if you choose  minus  you dont really get such a big prize if you deliver a good document 
{'5 - 10 - 4.6 Recommender Systems- Content-based Filtering - Part 1  (00-12-55).srt : 00:08:22,575 --> 00:08:28,253': 'you dont really get such a big prize if you deliver a good document'}
--------------------------------------------------
9 reluctant 1.7445380593498679e-06 you can imagine that the system would be very reluctant to deliver lot of documents 
{'5 - 10 - 4.6 Recommender Systems- Content-based Filtering - Part 1  (00-12-55).srt : 00:08:31,250 --> 00:08:35,380': 'you can imagine that the system would be very reluctant to'}
--------------------------------------------------
8 maker 1.2213278741915971e-06 so it has to be a binary decision maker a binary classifier 
{'5 - 10 - 4.6 Recommender Systems- Content-based Filtering - Part 1  (00-12-55).srt : 00:08:53,620 --> 00:08:58,200': 'so it has to be a binary decision maker a binary classifier'}
--------------------------------------------------
29 passing 2.4426557483831942e-06 we we do retrieval and then we kind of find the scores of documents and then we apply a threshold to to say to see whether a document is passing this threshold or not 
{'5 - 10 - 4.6 Recommender Systems- Content-based Filtering - Part 1  (00-12-55).srt : 00:10:52,210 --> 00:10:56,890': 'to see whether a document is passing this threshold or not', '5 - 10 - 4.6 Recommender Systems- Content-based Filtering - Part 1  (00-12-55).srt : 00:10:56,890 --> 00:10:59,790': 'and if it passing the threshold we are going to say it relevant and'}
--------------------------------------------------
3 passing 2.4426557483831942e-06 and if it passing the threshold we are going to say it relevant and we are going to deliver it to the user 
{'5 - 10 - 4.6 Recommender Systems- Content-based Filtering - Part 1  (00-12-55).srt : 00:10:52,210 --> 00:10:56,890': 'to see whether a document is passing this threshold or not', '5 - 10 - 4.6 Recommender Systems- Content-based Filtering - Part 1  (00-12-55).srt : 00:10:56,890 --> 00:10:59,790': 'and if it passing the threshold we are going to say it relevant and'}
--------------------------------------------------
15 ramping 1.2213278741915971e-06 in this lecture were going to talk about the the specific way of design the ramping function called a vector space mutual model 
{'2 - 5 - 1.5 Vector Space Model- Basic Idea (00-09-44).srt : 00:00:33,550 --> 00:00:36,550': 'design the ramping function called a vector space mutual model'}
--------------------------------------------------
4 pri 1.2213278741915971e-06 so to be more pri precise be more precise 
{'2 - 5 - 1.5 Vector Space Model- Basic Idea (00-09-44).srt : 00:04:58,330 --> 00:05:04,050': 'so to be more pri precise be more precise'}
--------------------------------------------------
9 enneagram 1.2213278741915971e-06 for example a word or a phrase or even enneagram of characters 
{'2 - 5 - 1.5 Vector Space Model- Basic Idea (00-09-44).srt : 00:05:21,670 --> 00:05:28,920': 'for example a word or a phrase or even enneagram of characters'}
--------------------------------------------------
5 ndimensional 1.2213278741915971e-06 in our vocabulary we define ndimensional space 
{'2 - 5 - 1.5 Vector Space Model- Basic Idea (00-09-44).srt : 00:05:38,690 --> 00:05:42,380': 'in our vocabulary we define ndimensional space'}
--------------------------------------------------
6 orthogonal 2.2677482445081393e-06 we clearly assume the concepts are orthogonal otherwise there will be redundancy 
{'2 - 5 - 1.5 Vector Space Model- Basic Idea (00-09-44).srt : 00:07:32,550 --> 00:07:36,180': 'we clearly assume the concepts are orthogonal'}
--------------------------------------------------
1 overemphasizing 2.4426557483831942e-06 or overemphasizing of matching this concept 
{'2 - 5 - 1.5 Vector Space Model- Basic Idea (00-09-44).srt : 00:07:54,870 --> 00:08:01,610': 'or overemphasizing of matching this concept', '2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:06:14,735 --> 00:06:19,365': 'so we just talked about how to solve the problem of overemphasizing a frequently'}
--------------------------------------------------
11 operational 1.7445380593498679e-06 so these questions must be addressed before we can have an operational function that we can actually implement using a program language 
{'2 - 5 - 1.5 Vector Space Model- Basic Idea (00-09-44).srt : 00:09:15,600 --> 00:09:20,400': 'so these questions must be addressed before we can have an operational'}
--------------------------------------------------
17 characterised 1.2213278741915971e-06 this way our target topic theta here will be only generating the common handle words that are characterised the content of the document 
{'3 - 7 - 2.7 Probabilistic Topic Models- Mixture of Unigram Language Models (00-12-39).srt : 00:01:47,040 --> 00:01:51,439': 'the common handle words that are characterised the content of the document'}
--------------------------------------------------
6 enacting 2.4426557483831942e-06 so this is a probability of enacting the topic word of distribution 
{'3 - 7 - 2.7 Probabilistic Topic Models- Mixture of Unigram Language Models (00-12-39).srt : 00:02:41,850 --> 00:02:47,170': 'so this is a probability of enacting the topic word of distribution', '3 - 7 - 2.7 Probabilistic Topic Models- Mixture of Unigram Language Models (00-12-39).srt : 00:02:47,170 --> 00:02:51,150': 'this is the probability of enacting the background word'}
--------------------------------------------------
5 enacting 2.4426557483831942e-06 this is the probability of enacting the background word of distribution denoted by theta sub b 
{'3 - 7 - 2.7 Probabilistic Topic Models- Mixture of Unigram Language Models (00-12-39).srt : 00:02:41,850 --> 00:02:47,170': 'so this is a probability of enacting the topic word of distribution', '3 - 7 - 2.7 Probabilistic Topic Models- Mixture of Unigram Language Models (00-12-39).srt : 00:02:47,170 --> 00:02:51,150': 'this is the probability of enacting the background word'}
--------------------------------------------------
3 convince 2.2677482445081393e-06 and you should convince yourself that this is indeed the probability of obsolete text 
{'3 - 7 - 2.7 Probabilistic Topic Models- Mixture of Unigram Language Models (00-12-39).srt : 00:05:43,940 --> 00:05:48,130': 'and you should convince yourself that this is indeed the probability of'}
--------------------------------------------------
12 obsolete 1.2213278741915971e-06 and you should convince yourself that this is indeed the probability of obsolete text 
{'3 - 7 - 2.7 Probabilistic Topic Models- Mixture of Unigram Language Models (00-12-39).srt : 00:05:48,130 --> 00:05:49,940': 'obsolete text '}
--------------------------------------------------
12 thesetwo 1.2213278741915971e-06 so the basic idea of a mixture model is just to retrieve thesetwo distributions together as one model 
{'3 - 7 - 2.7 Probabilistic Topic Models- Mixture of Unigram Language Models (00-12-39).srt : 00:06:23,825 --> 00:06:28,820': 'thesetwo distributions together as one model'}
--------------------------------------------------
9 dimmer 1.2213278741915971e-06 the illustration that you have seen before which is dimmer now is just the illustration of this generated model 
{'3 - 7 - 2.7 Probabilistic Topic Models- Mixture of Unigram Language Models (00-12-39).srt : 00:07:10,140 --> 00:07:14,210': 'which is dimmer now is just the illustration of this generated model'}
--------------------------------------------------
6 functioning 1.2213278741915971e-06 we can write down that like functioning as we see here 
{'3 - 7 - 2.7 Probabilistic Topic Models- Mixture of Unigram Language Models (00-12-39).srt : 00:08:31,350 --> 00:08:34,310': 'we can write down that like functioning as we see here'}
--------------------------------------------------
11 degenerate 2.2677482445081393e-06 and if you look at the likelihood function it will then degenerate to the special case of just one distribution 
{'3 - 7 - 2.7 Probabilistic Topic Models- Mixture of Unigram Language Models (00-12-39).srt : 00:09:36,320 --> 00:09:40,640': 'it will then degenerate to the special case of just one distribution'}
--------------------------------------------------
4 recalled 1.2213278741915971e-06 so you might have recalled before we just had this one there 
{'3 - 7 - 2.7 Probabilistic Topic Models- Mixture of Unigram Language Models (00-12-39).srt : 00:11:20,200 --> 00:11:24,420': 'so you might have recalled before we just had this one there'}
--------------------------------------------------
14 commutative 1.2213278741915971e-06 and this form where we look at the different and unique words is a commutative that formed for computing the maximum likelihood estimate later 
{'3 - 7 - 2.7 Probabilistic Topic Models- Mixture of Unigram Language Models (00-12-39).srt : 00:11:57,582 --> 00:12:04,720': 'a commutative that formed for computing the maximum likelihood estimate later'}
--------------------------------------------------
16 formed 2.2677482445081393e-06 and this form where we look at the different and unique words is a commutative that formed for computing the maximum likelihood estimate later 
{'3 - 7 - 2.7 Probabilistic Topic Models- Mixture of Unigram Language Models (00-12-39).srt : 00:11:57,582 --> 00:12:04,720': 'a commutative that formed for computing the maximum likelihood estimate later'}
--------------------------------------------------
12 awarded 2.4426557483831942e-06 one is a set of topics denoted by theta i each is awarded distribution and the other is pi i j 
{'3 - 14 - 2.14 Probabilistic Latent Semantic Analysis (PLSA)- Part 2 (00-10-15).srt : 00:04:55,000 --> 00:04:59,830': 'awarded distributions inaudible and we just randomly normalize them', '4 - 2 - 3.2 Text Clustering- Generative Probabilistic Models Part 1 (00-16-18).srt : 00:01:31,478 --> 00:01:35,750': 'each is awarded distribution and the other is pi i j'}
--------------------------------------------------
11 hear 1.7445380593498679e-06 then well have a definition of the clustering problem as youll hear 
{'4 - 2 - 3.2 Text Clustering- Generative Probabilistic Models Part 1 (00-16-18).srt : 00:02:37,200 --> 00:02:41,987': 'then well have a definition of the clustering problem as youll hear'}
--------------------------------------------------
6 requirement 2.2677482445081393e-06 and you will see a main requirement is how can we force every document to be generated from precisely one topic instead of k topics as in the topic model 
{'4 - 2 - 3.2 Text Clustering- Generative Probabilistic Models Part 1 (00-16-18).srt : 00:05:21,407 --> 00:05:26,391': 'and you will see a main requirement is how can we force every'}
--------------------------------------------------
5 violating 1.2213278741915971e-06 and it more importantly it violating our assumption about the partitioning of documents in the clusters 
{'4 - 2 - 3.2 Text Clustering- Generative Probabilistic Models Part 1 (00-16-18).srt : 00:07:34,380 --> 00:07:37,280': 'and it more importantly it violating our assumption'}
--------------------------------------------------
21 regime 2.4426557483831942e-06 but this time once we have made a decision to choose one of the topics were going to stay with this regime to generate all the words in the document 
{'4 - 2 - 3.2 Text Clustering- Generative Probabilistic Models Part 1 (00-16-18).srt : 00:08:42,581 --> 00:08:47,999': 'were going to stay with this regime to generate all the words in the document', '3 - 5 - 2.5 Probabilistic Topic Models- Overview of Statistical Language Models- Part 2 (00-13-11).srt : 00:10:14,310 --> 00:10:18,970': 'to infer this posterior this regime and '}
--------------------------------------------------
30 identity 2.4426557483831942e-06 and of course a main problem in document clustering is to infer which distribution has been used to generate a document and that would allow us to recover the cluster identity of a document 
{'4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:03:45,545 --> 00:03:50,485': 'which basically measures based on the identity of', '4 - 2 - 3.2 Text Clustering- Generative Probabilistic Models Part 1 (00-16-18).srt : 00:10:26,810 --> 00:10:31,165': 'that would allow us to recover the cluster identity of a document'}
--------------------------------------------------
13 regenerate 1.2213278741915971e-06 the second is that word distribution here is going to be used to regenerate all the words for a document 
{'4 - 2 - 3.2 Text Clustering- Generative Probabilistic Models Part 1 (00-16-18).srt : 00:11:12,600 --> 00:11:17,800': 'is going to be used to regenerate all the words for a document'}
--------------------------------------------------
2 stick 1.2213278741915971e-06 we just stick with one particular distribution 
{'4 - 2 - 3.2 Text Clustering- Generative Probabilistic Models Part 1 (00-16-18).srt : 00:11:46,750 --> 00:11:50,842': 'we just stick with one particular distribution'}
--------------------------------------------------
17 datapoint 1.2213278741915971e-06 one is the probability of choosing the distribution the other is the probability of observing a particular datapoint from that distribution 
{'4 - 2 - 3.2 Text Clustering- Generative Probabilistic Models Part 1 (00-16-18).srt : 00:13:23,450 --> 00:13:26,480': 'of observing a particular datapoint from that distribution'}
--------------------------------------------------
16 copying 1.2213278741915971e-06 but it also useful to think about the difference and for that purpose i am also copying the probability of topic model of these two components here 
{'4 - 2 - 3.2 Text Clustering- Generative Probabilistic Models Part 1 (00-16-18).srt : 00:14:48,790 --> 00:14:56,350': 'i am also copying the probability of topic model of these two components here'}
--------------------------------------------------
16 responding 1.2213278741915971e-06 finally were going to talk about the document selection versus document ranking as two strategies for responding to a user query 
{'2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:01:02,950 --> 00:01:06,350': 'as two strategies for responding to a user query'}
--------------------------------------------------
18 lockins 1.2213278741915971e-06 so text retrieval is basically a task where the system would respond to a user query with relevant lockins basically through supported querying as one way to implement the poor mold of information access 
{'2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:01:25,425 --> 00:01:30,673': 'to a user query with relevant lockins basically through'}
--------------------------------------------------
30 mold 1.2213278741915971e-06 so text retrieval is basically a task where the system would respond to a user query with relevant lockins basically through supported querying as one way to implement the poor mold of information access 
{'2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:01:30,673 --> 00:01:37,300': 'supported querying as one way to implement the poor mold of information access'}
--------------------------------------------------
2 audio 1.2213278741915971e-06 for example audio video et cetera 
{'2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:02:27,610 --> 00:02:31,020': 'for example audio video et cetera '}
--------------------------------------------------
18 medias 1.2213278741915971e-06 it worth noting that text retrieval is at the core of information retrieval in the sense that other medias such as video can be retrieved by exploiting the companion text data 
{'2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:02:37,115 --> 00:02:41,656': 'information retrieval in the sense that other medias such as'}
--------------------------------------------------
13 schema 2.2677482445081393e-06 but in databases they are structured data where there is a clear defined schema to tell you this column is the names of people and that column is ages et cetera 
{'2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:04:23,110 --> 00:04:28,110': 'schema to tell you this column is the names of people and'}
--------------------------------------------------
28 ages 1.2213278741915971e-06 but in databases they are structured data where there is a clear defined schema to tell you this column is the names of people and that column is ages et cetera 
{'2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:04:28,110 --> 00:04:31,660': 'that column is ages et cetera '}
--------------------------------------------------
7 emperical 1.2213278741915971e-06 that also means we must rely on emperical evaluation more than users to know which method works better 
{'2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:06:52,630 --> 00:06:56,710': 'that also means we must rely on emperical evaluation'}
--------------------------------------------------
2 pictures 1.2213278741915971e-06 so this pictures shows how it works 
{'2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:14:50,240 --> 00:14:53,960': 'so this pictures shows how it works '}
--------------------------------------------------
8 misclassified 1.2213278741915971e-06 and similarly there a relevant document that that misclassified as nonrelevant 
{'2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:15:43,090 --> 00:15:48,825': 'and similarly there a relevant document that that misclassified as nonrelevant'}
--------------------------------------------------
4 distraction 1.2213278741915971e-06 and there are many distraction documents using similar words 
{'2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:17:58,490 --> 00:18:04,280': 'and there are many distraction documents using similar words'}
--------------------------------------------------
8 prespecify 1.2213278741915971e-06 so it very hard for a user to prespecify the right level of of constraints 
{'2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:18:33,780 --> 00:18:38,837': 'so it very hard for a user to prespecify'}
--------------------------------------------------
3 prioritize 2.4426557483831942e-06 so we must prioritize these documents for user to exam 
{'2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:18:59,560 --> 00:19:05,190': 'so we must prioritize these documents for user to exam', '2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:24:41,680 --> 00:24:46,170': 'this is will help users prioritize examination of search results'}
--------------------------------------------------
9 exam 1.4829329667707326e-06 so we must prioritize these documents for user to exam 
{'2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:18:59,560 --> 00:19:05,190': 'so we must prioritize these documents for user to exam'}
--------------------------------------------------
5 prioritization 1.7445380593498679e-06 and this note that this prioritization is very important because a user cannot digest all the contents at once 
{'2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:19:06,320 --> 00:19:10,690': 'and this note that this prioritization is very important'}
--------------------------------------------------
7 justification 1.2213278741915971e-06 now this preference also has a theoretical justification and this is given by the probability ranking principle 
{'2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:19:36,330 --> 00:19:39,660': 'now this preference also has a theoretical justification and'}
--------------------------------------------------
22 ordering 1.2213278741915971e-06 if the user is not going to follow the ranked list is not going to examine the documents sequentially then obviously the ordering would not be optimal 
{'2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:20:55,040 --> 00:20:59,400': 'the documents sequentially then obviously the ordering would not be optimal'}
--------------------------------------------------
32 restrictive 1.2213278741915971e-06 and if you think about the more complicated interfaces that would possibly use like two dimensional interface where you can put additional information on the screen then sequential browsing is a very restrictive assumption 
{'2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:23:24,555 --> 00:23:29,261': 'on the screen then sequential browsing is a very restrictive assumption'}
--------------------------------------------------
19 establishes 1.2213278741915971e-06 so the point here is that none of these assumptions is really true but nevertheless the probability ranking principle establishes some solid foundation for ranking as a primary task for search engines 
{'2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:23:39,672 --> 00:23:46,745': 'the probability ranking principle establishes some solid foundation for'}
--------------------------------------------------
12 prioritize 2.4426557483831942e-06 second document ranking is generally prefer and this is will help users prioritize examination of search results 
{'2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:18:59,560 --> 00:19:05,190': 'so we must prioritize these documents for user to exam', '2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:24:41,680 --> 00:24:46,170': 'this is will help users prioritize examination of search results'}
--------------------------------------------------
13 examination 1.7445380593498679e-06 second document ranking is generally prefer and this is will help users prioritize examination of search results 
{'2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:24:41,680 --> 00:24:46,170': 'this is will help users prioritize examination of search results'}
--------------------------------------------------
19 encountered 2.2677482445081393e-06 so naturally there had to be some further extensions of the classical search algorithms to address some new challenges encountered in web search 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:00:45,780 --> 00:00:52,480': 'search algorithms to address some new challenges encountered in web search'}
--------------------------------------------------
4 dynamics 1.7445380593498679e-06 the third challenge is dynamics of the web 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:01:22,130 --> 00:01:24,350': 'the third challenge is dynamics of the web'}
--------------------------------------------------
12 eve 1.2213278741915971e-06 the new pages are constantly created and some pages may be updated eve very quickly 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:01:27,410 --> 00:01:32,390': 'some pages may be updated eve very quickly'}
--------------------------------------------------
18 imaging 2.4426557483831942e-06 one is parallel indexing and searching and this is to address the issue of scalability in particular google imaging of mapreduce is very inferential and has been very helpful in that aspect 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:26:57,390 --> 00:27:01,620': 'because these are human imaging for communication', '5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:02:43,050 --> 00:02:48,070': 'scalability in particular google imaging of mapreduce'}
--------------------------------------------------
23 inferential 1.2213278741915971e-06 one is parallel indexing and searching and this is to address the issue of scalability in particular google imaging of mapreduce is very inferential and has been very helpful in that aspect 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:02:48,070 --> 00:02:53,570': 'is very inferential and has been very helpful in that aspect'}
--------------------------------------------------
8 addressing 2.2677482445081393e-06 second there are techniques that are developed for addressing the problem of spams 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:02:56,720 --> 00:02:58,580': 'addressing the problem of spams '}
--------------------------------------------------
24 tricks 2.4426557483831942e-06 and were going to use a lot of signals to rank pages so that it not easy to spam the search engine with particular tricks 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:03:10,520 --> 00:03:15,410': 'that it not easy to spam the search engine with particular tricks', '3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:11:53,750 --> 00:11:58,850': 'so there are some tricks to further improve the efficiency some general mac'}
--------------------------------------------------
10 crawls 1.7445380593498679e-06 not just link analysis but also exploiting all kinds of crawls like the layout of web pages or anchor text that describes a link to another page 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:03:35,710 --> 00:03:41,700': 'not just link analysis but also exploiting all kinds of crawls like'}
--------------------------------------------------
11 spider 1.7445380593498679e-06 first were going to talk about the crawler also called a spider or a software robot that would do something like a crawling pages on the web 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:04:32,500 --> 00:04:37,730': 'first were going to talk about the crawler also called a spider or'}
--------------------------------------------------
3 toy 2.2677482445081393e-06 to build a toy crawler is relatively easy because you just need to start with a set of seed pages and then fetch pages from the web and parse these pages new links 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:04:43,400 --> 00:04:46,810': 'to build a toy crawler is relatively easy because you just need to start with a set'}
--------------------------------------------------
4 trap 1.7445380593498679e-06 what if there a trap that generates dynamically generated webpages that might attract your crawler to keep crawling the same site and to fetch dynamically generated pages 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:05:13,650 --> 00:05:19,480': 'what if there a trap that generates dynamically generated webpages that might'}
--------------------------------------------------
9 webpages 1.2213278741915971e-06 what if there a trap that generates dynamically generated webpages that might attract your crawler to keep crawling the same site and to fetch dynamically generated pages 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:05:13,650 --> 00:05:19,480': 'what if there a trap that generates dynamically generated webpages that might'}
--------------------------------------------------
8 exclusion 2.2677482445081393e-06 and you have to respect the the robot exclusion protocol 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:05:34,320 --> 00:05:39,020': 'and you have to respect the the robot exclusion protocol'}
--------------------------------------------------
9 protocol 1.7445380593498679e-06 and you have to respect the the robot exclusion protocol 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:05:34,320 --> 00:05:39,020': 'and you have to respect the the robot exclusion protocol'}
--------------------------------------------------
4 cgi 1.2213278741915971e-06 so sometimes those are cgi scripts and you know internal references etc and sometimes you have javascripts on the page that they also create challenges 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:05:51,490 --> 00:05:57,630': 'so sometimes those are cgi scripts and you know internal references etc and'}
--------------------------------------------------
5 scripts 1.2213278741915971e-06 so sometimes those are cgi scripts and you know internal references etc and sometimes you have javascripts on the page that they also create challenges 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:05:51,490 --> 00:05:57,630': 'so sometimes those are cgi scripts and you know internal references etc and'}
--------------------------------------------------
16 javascripts 1.2213278741915971e-06 so sometimes those are cgi scripts and you know internal references etc and sometimes you have javascripts on the page that they also create challenges 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:05:57,630 --> 00:06:03,995': 'sometimes you have javascripts on the page that they also create challenges'}
--------------------------------------------------
10 balances 1.7445380593498679e-06 in general breadthfirst is most common because it naturally balance balances server load 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:06:32,577 --> 00:06:36,560': 'because it naturally balance balances server load'}
--------------------------------------------------
4 probing 1.2213278741915971e-06 you would not keep probing a particular server inaudible 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:06:36,560 --> 00:06:42,625': 'you would not keep probing a particular server inaudible'}
--------------------------------------------------
13 parallelise 1.2213278741915971e-06 also parallel crawling is very natural because this task is very easy to parallelise and there are some variations of the crawling task 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:06:46,505 --> 00:06:51,135': 'to parallelise and there are some variations of the crawling task'}
--------------------------------------------------
5 automobiles 1.7445380593498679e-06 for example all pages about automobiles 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:06:59,860 --> 00:07:02,180': 'for example all pages about automobiles '}
--------------------------------------------------
7 refine 1.7445380593498679e-06 if they are then you can probably refine them by recrawling the older page 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:07:35,930 --> 00:07:40,539': 'if they are then you can probably refine them by recrawling the older page'}
--------------------------------------------------
10 recrawling 1.2213278741915971e-06 if they are then you can probably refine them by recrawling the older page 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:07:35,930 --> 00:07:40,539': 'if they are then you can probably refine them by recrawling the older page'}
--------------------------------------------------
5 uminteresting 1.2213278741915971e-06 so these are also some uminteresting challenges that have to be solved 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:07:41,890 --> 00:07:45,940': 'so these are also some uminteresting challenges that have to be solved'}
--------------------------------------------------
12 months 1.7445380593498679e-06 if the page is a static page that hasnt been changed for months you probably dont have to recrawl it everyday right 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:09:07,480 --> 00:09:11,510': 'months you probably dont have to recrawl it everyday right'}
--------------------------------------------------
9 fetched 1.7445380593498679e-06 compare it with another page that has never been fetched by any users for a year 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:09:40,960 --> 00:09:45,750': 'compare it with another page that has never been fetched by any users for'}
--------------------------------------------------
25 urgent 2.006143151929004e-06 than even though that page has been changed a lot then it probably not necessary to crawl that page or at least it not as urgent as to maintain the freshness of frequently accessed page by users 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:09:49,750 --> 00:09:55,810': 'it probably not necessary to crawl that page or at least it not as urgent as'}
--------------------------------------------------
30 freshness 1.7445380593498679e-06 than even though that page has been changed a lot then it probably not necessary to crawl that page or at least it not as urgent as to maintain the freshness of frequently accessed page by users 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:09:55,810 --> 00:10:01,780': 'to maintain the freshness of frequently accessed page by users'}
--------------------------------------------------
14 oppose 1.2213278741915971e-06 then we can assume the user would use this working as a basis to oppose a query to try and retrieve this doc 
{'4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).srt : 00:00:51,030 --> 00:00:54,670': 'working as a basis to oppose a query to try and retrieve this doc'}
--------------------------------------------------
18 imperative 2.4426557483831942e-06 and this allows to also not rely on the big table that i showed you earlier to use imperative data to estimate this probability 
{'4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).srt : 00:01:52,580 --> 00:01:55,750': 'to use imperative data to estimate this probability', '3 - 4 - 2.4 Evaluation of TR Systems (00-10-10).srt : 00:01:08,440 --> 00:01:14,390': 'there we mentioned that text retrieval is imperative to find the problem'}
--------------------------------------------------
6 president 1.7445380593498679e-06 and see here  and weve seen president jou tai so that two over the lands of document the four 
{'4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).srt : 00:03:54,388 --> 00:03:57,620': 'and see here and weve seen president jou tai'}
--------------------------------------------------
7 jou 1.2213278741915971e-06 and see here  and weve seen president jou tai so that two over the lands of document the four 
{'4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).srt : 00:03:54,388 --> 00:03:57,620': 'and see here and weve seen president jou tai'}
--------------------------------------------------
8 tai 1.2213278741915971e-06 and see here  and weve seen president jou tai so that two over the lands of document the four 
{'4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).srt : 00:03:54,388 --> 00:03:57,620': 'and see here and weve seen president jou tai'}
--------------------------------------------------
13 seeming 1.2213278741915971e-06 multiply by  over lands of document of  for the probability of campaign and seeming we can probabilities for the other two documents 
{'4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).srt : 00:04:06,200 --> 00:04:11,250': 'campaign and seeming we can probabilities for the other two documents'}
--------------------------------------------------
8 coarse 1.2213278741915971e-06 so if we take longer than transformation of coarse the product that would become a sum as you stake in the line here 
{'4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).srt : 00:09:52,860 --> 00:09:57,051': 'so if we take longer than transformation of coarse the product that would become'}
--------------------------------------------------
18 stake 1.2213278741915971e-06 so if we take longer than transformation of coarse the product that would become a sum as you stake in the line here 
{'4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).srt : 00:09:57,051 --> 00:09:59,920': 'a sum as you stake in the line here '}
--------------------------------------------------
1 wh 1.2213278741915971e-06 that wh why we have a count here 
{'4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).srt : 00:10:56,435 --> 00:10:58,375': 'that wh why we have a count here '}
--------------------------------------------------
16 mg 1.2213278741915971e-06 and then this part is log of the probability of the word given by the document mg model 
{'4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).srt : 00:11:03,555 --> 00:11:06,865': 'log of the probability of the word given by the document mg model'}
--------------------------------------------------
7 stuff 2.4426557483831942e-06 here are different ways to estimate this stuff in the language model will lead you to a different ranking function for query likelihood 
{'4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).srt : 00:11:45,670 --> 00:11:50,436': 'here are different ways to estimate this stuff in the language model', '2 - 11 - 1.11 Syntagmatic Relation Discovery- Conditional Entropy (00-11-57).srt : 00:08:45,300 --> 00:08:49,885': 'well that when this stuff is not really related to meat'}
--------------------------------------------------
19 premise 1.2213278741915971e-06 so this estimate that in due it also makes sense and it often very useful and it seeks the premise that best explains the data 
{'3 - 5 - 2.5 Probabilistic Topic Models- Overview of Statistical Language Models- Part 2 (00-13-11).srt : 00:01:22,122 --> 00:01:27,070': 'and it seeks the premise that best explains the data'}
--------------------------------------------------
15 detour 1.2213278741915971e-06 and the likelihood of observing this y if x is indeed true so much for detour about bayes rule 
{'3 - 5 - 2.5 Probabilistic Topic Models- Overview of Statistical Language Models- Part 2 (00-13-11).srt : 00:05:57,200 --> 00:06:02,250': 'so much for detour about bayes rule '}
--------------------------------------------------
8 noninformative 1.7445380593498679e-06 because if we define our prior as a noninformative prior meaning that it uniform over all the theta values 
{'3 - 5 - 2.5 Probabilistic Topic Models- Overview of Statistical Language Models- Part 2 (00-13-11).srt : 00:07:02,860 --> 00:07:08,700': 'because if we define our prior as a noninformative prior'}
--------------------------------------------------
4 lunch 1.2213278741915971e-06 there is no free lunch and if you want to solve the problem with more knowledge we have to have that knowledge 
{'3 - 5 - 2.5 Probabilistic Topic Models- Overview of Statistical Language Models- Part 2 (00-13-11).srt : 00:07:44,140 --> 00:07:49,460': 'there is no free lunch and if you want to solve the problem with more knowledge'}
--------------------------------------------------
15 simplification 2.2677482445081393e-06 so i show the theta values as just a one dimension value and that a simplification of course 
{'3 - 5 - 2.5 Probabilistic Topic Models- Overview of Statistical Language Models- Part 2 (00-13-11).srt : 00:08:12,720 --> 00:08:18,040': 'dimension value and that a simplification of course'}
--------------------------------------------------
5 syllables 1.2213278741915971e-06 and that just means loose syllables can best expand our theta 
{'3 - 5 - 2.5 Probabilistic Topic Models- Overview of Statistical Language Models- Part 2 (00-13-11).srt : 00:08:56,800 --> 00:08:59,710': 'and that just means loose syllables can best expand our theta'}
--------------------------------------------------
6 inbetween 1.2213278741915971e-06 it would say that it somewhere inbetween 
{'3 - 5 - 2.5 Probabilistic Topic Models- Overview of Statistical Language Models- Part 2 (00-13-11).srt : 00:09:07,810 --> 00:09:11,960': 'it would say that it somewhere inbetween'}
--------------------------------------------------
16 additives 1.2213278741915971e-06 now in general in bayesian inference we are interested in the distribution of all these parameter additives as you see here 
{'3 - 5 - 2.5 Probabilistic Topic Models- Overview of Statistical Language Models- Part 2 (00-13-11).srt : 00:09:55,930 --> 00:09:59,340': 'the distribution of all these parameter additives as you see here'}
--------------------------------------------------
12 regime 2.4426557483831942e-06 so the problem of bayesian inference is to infer this posterior this regime and also to infer other interesting quantities that might depend on theta 
{'4 - 2 - 3.2 Text Clustering- Generative Probabilistic Models Part 1 (00-16-18).srt : 00:08:42,581 --> 00:08:47,999': 'were going to stay with this regime to generate all the words in the document', '3 - 5 - 2.5 Probabilistic Topic Models- Overview of Statistical Language Models- Part 2 (00-13-11).srt : 00:10:14,310 --> 00:10:18,970': 'to infer this posterior this regime and '}
--------------------------------------------------
28 livelihood 1.2213278741915971e-06 given a data sample x we can use this function to determine which parameter values would maximize the probability of the observed data and this is the maximum livelihood estimate 
{'3 - 5 - 2.5 Probabilistic Topic Models- Overview of Statistical Language Models- Part 2 (00-13-11).srt : 00:12:27,840 --> 00:12:29,640': 'and this is the maximum livelihood estimate'}
--------------------------------------------------
8 subclass 1.2213278741915971e-06 in this lecture were going to discuss another subclass in this big class called a language modeling approaches to retrieval 
{'4 - 1 - 3.1 Probabilistic Retrieval Model- Basic Idea (00-12-44).srt : 00:01:34,920 --> 00:01:39,991': 'in this lecture were going to discuss another subclass in'}
--------------------------------------------------
11 latitude 2.4426557483831942e-06 there is also another line called a divergencefromrandomness model which has latitude the pl function 
{'4 - 1 - 3.1 Probabilistic Retrieval Model- Basic Idea (00-12-44).srt : 00:02:02,881 --> 00:02:06,048': 'which has latitude the pl function ', '2 - 1 - 1.1 Overview Text Mining and Analytics- Part 1 (00-11-43).srt : 00:07:53,740 --> 00:07:57,140': 'example in the form of longitude value and latitude value'}
--------------------------------------------------
9 readiness 2.4426557483831942e-06 in query likelihood our assumption is that this probability readiness can be approximated by the probability of query given a document and readiness 
{'4 - 1 - 3.1 Probabilistic Retrieval Model- Basic Idea (00-12-44).srt : 00:02:10,797 --> 00:02:16,851': 'in query likelihood our assumption is that this probability readiness', '4 - 1 - 3.1 Probabilistic Retrieval Model- Basic Idea (00-12-44).srt : 00:02:16,851 --> 00:02:23,503': 'can be approximated by the probability of query given a document and readiness'}
--------------------------------------------------
22 readiness 2.4426557483831942e-06 in query likelihood our assumption is that this probability readiness can be approximated by the probability of query given a document and readiness 
{'4 - 1 - 3.1 Probabilistic Retrieval Model- Basic Idea (00-12-44).srt : 00:02:10,797 --> 00:02:16,851': 'in query likelihood our assumption is that this probability readiness', '4 - 1 - 3.1 Probabilistic Retrieval Model- Basic Idea (00-12-44).srt : 00:02:16,851 --> 00:02:23,503': 'can be approximated by the probability of query given a document and readiness'}
--------------------------------------------------
12 relying 2.2677482445081393e-06 we have to somehow be able to estimate this conditional probability without relying on this big table 
{'4 - 1 - 3.1 Probabilistic Retrieval Model- Basic Idea (00-12-44).srt : 00:09:51,464 --> 00:09:54,924': 'probability without relying on this big table'}
--------------------------------------------------
18 mortar 1.2213278741915971e-06 and by making this assumption we have some way to bypass this big table and try to just mortar how to use a formula to the query 
{'4 - 1 - 3.1 Probabilistic Retrieval Model- Basic Idea (00-12-44).srt : 00:10:04,816 --> 00:10:08,712': 'try to just mortar how to use a formula to the query'}
--------------------------------------------------
18 formulates 2.2677482445081393e-06 which of these documents is most likely the imaginary relevant document in the user mind when the user formulates this query 
{'4 - 1 - 3.1 Probabilistic Retrieval Model- Basic Idea (00-12-44).srt : 00:10:31,073 --> 00:10:33,961': 'the user mind when the user formulates this query'}
--------------------------------------------------
5 proximate 1.2213278741915971e-06 we also talked about a proximate in this by using the query likelihood 
{'4 - 1 - 3.1 Probabilistic Retrieval Model- Basic Idea (00-12-44).srt : 00:11:16,128 --> 00:11:22,091': 'we also talked about a proximate in this sound by using the query likelihood'}
--------------------------------------------------
11 approve 1.2213278741915971e-06 if the user like this document how likely the user would approve this query 
{'4 - 1 - 3.1 Probabilistic Retrieval Model- Basic Idea (00-12-44).srt : 00:12:11,756 --> 00:12:19,045': 'if the user like this document how likely the user would approve this query'}
--------------------------------------------------
3 attaching 1.7445380593498679e-06 values of z attaching to other words 
{'3 - 11 - 2.11 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2 (00-10-39).srt : 00:00:58,915 --> 00:01:01,875': 'values of z attaching to other words '}
--------------------------------------------------
9 roll 1.2213278741915971e-06 these initialized values would allow us to use base roll to take a guess of these z values so wed guess these values 
{'3 - 11 - 2.11 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2 (00-10-39).srt : 00:01:39,680 --> 00:01:44,150': 'these initialized values would allow us to use base roll to take a guess'}
--------------------------------------------------
6 textt 1.2213278741915971e-06 we cant say for sure whether textt is from background or not 
{'3 - 11 - 2.11 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2 (00-10-39).srt : 00:01:48,510 --> 00:01:53,580': 'we cant say for sure whether textt is from background or not'}
--------------------------------------------------
12 revise 2.4426557483831942e-06 we can then normalize the count to estimate the probabilities or to revise our estimate of the parameters 
{'3 - 11 - 2.11 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2 (00-10-39).srt : 00:02:32,865 --> 00:02:35,479': 'to revise our estimate of the parameters ', '5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:12:24,000 --> 00:12:29,250': 'we just revise the scores which generate a new set of scores'}
--------------------------------------------------
7 softer 1.2213278741915971e-06 but rather were going to do a softer split 
{'3 - 11 - 2.11 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2 (00-10-39).srt : 00:03:24,850 --> 00:03:26,800': 'but rather were going to do a softer split'}
--------------------------------------------------
7 uur 1.2213278741915971e-06 so the em algorithm would iteratively improve uur initial estimate of parameters by using estep first and then mstep 
{'3 - 11 - 2.11 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2 (00-10-39).srt : 00:03:48,120 --> 00:03:52,472': 'so the em algorithm would iteratively improve uur initial'}
--------------------------------------------------
6 bridge 1.7445380593498679e-06 okay so as i said the bridge between the two is really the variable z hidden variable which indicates how likely this water is from the top water distribution theta sub p so this slide has a lot of content and you may need to 
{'3 - 11 - 2.11 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2 (00-10-39).srt : 00:04:39,610 --> 00:04:44,670': 'okay so as i said the bridge between the two'}
--------------------------------------------------
8 themself 1.2213278741915971e-06 start with initial values that are often random themself 
{'3 - 11 - 2.11 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2 (00-10-39).srt : 00:05:07,300 --> 00:05:12,500': 'start with initial values that are often random themself'}
--------------------------------------------------
12 superscripts 1.7445380593498679e-06 formulas that you see before and you can also see there are superscripts here like here n to indicate the generation of parameters 
{'3 - 11 - 2.11 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2 (00-10-39).srt : 00:05:41,840 --> 00:05:48,220': 'formulas that you see before and you can also see there are superscripts'}
--------------------------------------------------
9 numerals 1.2213278741915971e-06 so in this setting we have assumed the two numerals have equal probabilities and the background model is null 
{'3 - 11 - 2.11 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2 (00-10-39).srt : 00:06:04,047 --> 00:06:08,106': 'so in this setting we have assumed the two numerals have equal probabilities and'}
--------------------------------------------------
18 null 1.7445380593498679e-06 so in this setting we have assumed the two numerals have equal probabilities and the background model is null 
{'3 - 11 - 2.11 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2 (00-10-39).srt : 00:06:08,106 --> 00:06:09,689': 'the background model is null '}
--------------------------------------------------
4 audition 1.2213278741915971e-06 and then our initial audition say uniform distribution because of the difference in the background of the distribution we have different guess the probability 
{'3 - 11 - 2.11 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2 (00-10-39).srt : 00:07:00,020 --> 00:07:05,320': 'and then our initial audition say uniform distribution because of the difference'}
--------------------------------------------------
6 zs 1.2213278741915971e-06 and these new inferred values of zs will give us then another generation of the estimate of probabilities of the word 
{'3 - 11 - 2.11 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2 (00-10-39).srt : 00:08:51,115 --> 00:08:56,343': 'and these new inferred values of zs will give us then'}
--------------------------------------------------
6 biproduct 1.2213278741915971e-06 but the last column is also biproduct 
{'3 - 11 - 2.11 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2 (00-10-39).srt : 00:10:00,900 --> 00:10:04,400': 'but the last column is also biproduct '}
--------------------------------------------------
24 chose 1.7445380593498679e-06 note that unlike in plsa and this probability of theta i is not dependent on d now you may recall that the topic you chose at each document actually depends on d that means each document can have a potentially different choice of topics but here we have a generic choice probability for all the documents 
{'4 - 3 - 3.3 Text Clustering- Generative Probabilistic Models Part 2 (00-08-37).srt : 00:05:37,640 --> 00:05:41,520': 'now you may recall that the topic you chose at each document'}
--------------------------------------------------
6 probablitistic 1.2213278741915971e-06 and were going to introduce contextual probablitistic latent semantic analysis as exchanging of pos for doing contextual text mining 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:00:23,930 --> 00:00:28,990': 'and were going to introduce contextual probablitistic latent semantic analysis'}
--------------------------------------------------
11 exchanging 2.4426557483831942e-06 and were going to introduce contextual probablitistic latent semantic analysis as exchanging of pos for doing contextual text mining 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:00:28,990 --> 00:00:32,630': 'as exchanging of pos for doing contextual text mining', '3 - 16 - 2.16 Latent Dirichlet Allocation (LDA)- Part 2 (00-12-03).srt : 00:00:00,025 --> 00:00:05,631': 'sound so now let talk about the exchanging of'}
--------------------------------------------------
4 searcher 1.2213278741915971e-06 so to generate the searcher document with context with first also choose a view 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:06:00,980 --> 00:06:07,685': 'so to generate the searcher document with context with first also choose a view'}
--------------------------------------------------
6 yellow 2.2677482445081393e-06 let say we have picked the yellow topic 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:07:34,880 --> 00:07:38,230': 'let say we have picked the yellow topic '}
--------------------------------------------------
18 switches 1.2213278741915971e-06 and the word distribution we let the context influence our choice so in other words we have extra switches that are tied to these contacts that will control the choices of different views of topics and the choices of coverage 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:08:11,250 --> 00:08:16,050': 'in other words we have extra switches that are tied to these contacts that will'}
--------------------------------------------------
24 contacts 2.4426557483831942e-06 and the word distribution we let the context influence our choice so in other words we have extra switches that are tied to these contacts that will control the choices of different views of topics and the choices of coverage 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:08:11,250 --> 00:08:16,050': 'in other words we have extra switches that are tied to these contacts that will', '2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:13:06,210 --> 00:13:11,560': 'all the possible words that can be overlapped between the two contacts'}
--------------------------------------------------
6 indicting 1.2213278741915971e-06 and there is a common theme indicting that united nations is involved in both wars 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:09:36,040 --> 00:09:42,260': 'and there is a common theme indicting that united nations is involved in both wars'}
--------------------------------------------------
13 weapons 1.7445380593498679e-06 so it indicates that the iraq war united nations was more involved in weapons factions whereas in the afghanistan war it was more involved in maybe aid to northern alliance 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:10:16,660 --> 00:10:21,060': 'united nations was more involved in weapons factions whereas in'}
--------------------------------------------------
14 factions 1.2213278741915971e-06 so it indicates that the iraq war united nations was more involved in weapons factions whereas in the afghanistan war it was more involved in maybe aid to northern alliance 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:10:16,660 --> 00:10:21,060': 'united nations was more involved in weapons factions whereas in'}
--------------------------------------------------
28 northern 2.2677482445081393e-06 so it indicates that the iraq war united nations was more involved in weapons factions whereas in the afghanistan war it was more involved in maybe aid to northern alliance 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:10:21,060 --> 00:10:25,710': 'the afghanistan war it was more involved in maybe aid to northern alliance'}
--------------------------------------------------
29 alliance 2.2677482445081393e-06 so it indicates that the iraq war united nations was more involved in weapons factions whereas in the afghanistan war it was more involved in maybe aid to northern alliance 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:10:21,060 --> 00:10:25,710': 'the afghanistan war it was more involved in maybe aid to northern alliance'}
--------------------------------------------------
5 walls 1.2213278741915971e-06 in this case different the walls or different the collection of texts 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:10:33,140 --> 00:10:36,215': 'in this case different the walls or different the collection of texts'}
--------------------------------------------------
2 visualisation 1.2213278741915971e-06 but the visualisation shows that with this technique we can have conditional distribution of time 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:12:12,370 --> 00:12:15,000': 'but the visualisation shows that with this technique'}
--------------------------------------------------
7 tracked 1.7445380593498679e-06 we see that initially the two curves tracked each other very well 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:12:26,000 --> 00:12:31,560': 'we see that initially the two curves tracked each other very well'}
--------------------------------------------------
3 triggered 1.7445380593498679e-06 and that apparently triggered more discussion about the flooding of the city 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:12:49,010 --> 00:12:52,470': 'and that apparently triggered more discussion about the flooding of the city'}
--------------------------------------------------
14 migrating 1.2213278741915971e-06 and it also shows some shift of coverage that might be related to people migrating from the state of louisiana to texas for example 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:13:11,620 --> 00:13:19,150': 'people migrating from the state of louisiana to texas for example'}
--------------------------------------------------
6 spacial 1.2213278741915971e-06 these are some additional results on spacial patterns 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:13:27,780 --> 00:13:33,070': 'these are some additional results on spacial patterns'}
--------------------------------------------------
2 visualizations 1.7445380593498679e-06 and these visualizations show the coverage in different weeks of the event 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:13:48,280 --> 00:13:54,260': 'and these visualizations show the coverage in different weeks of the event'}
--------------------------------------------------
8 weeks 1.7445380593498679e-06 and these visualizations show the coverage in different weeks of the event 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:13:48,280 --> 00:13:54,260': 'and these visualizations show the coverage in different weeks of the event'}
--------------------------------------------------
7 victim 2.2677482445081393e-06 and initially it covered mostly in the victim states in the south but then gradually spread into other locations 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:13:54,260 --> 00:13:59,610': 'and initially it covered mostly in the victim states'}
--------------------------------------------------
11 south 1.7445380593498679e-06 and initially it covered mostly in the victim states in the south but then gradually spread into other locations 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:13:59,610 --> 00:14:05,530': 'in the south but then gradually spread into other locations'}
--------------------------------------------------
4 moral 1.2213278741915971e-06 and of course the moral is completely general so you can apply this to any other connections of text 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:14:24,960 --> 00:14:27,280': 'and of course the moral is completely general so'}
--------------------------------------------------
2 spatial 1.2213278741915971e-06 to review spatial temporal patterns 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:14:30,980 --> 00:14:32,850': 'to review spatial temporal patterns '}
--------------------------------------------------
5 sponsored 1.7445380593498679e-06 this is a major evaluation sponsored by us government and was launched in  or around that time 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:15:08,290 --> 00:15:11,459': 'this is a major evaluation sponsored by us'}
--------------------------------------------------
11 launched 2.4426557483831942e-06 this is a major evaluation sponsored by us government and was launched in  or around that time 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:15:11,459 --> 00:15:16,722': 'government and was launched in or around that time', '2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:19:55,320 --> 00:19:58,910': 'google has recently launched a knowledge graph'}
--------------------------------------------------
7 seminal 2.2677482445081393e-06 the other is the publication of a seminal paper by croft and porte 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:15:23,870 --> 00:15:28,680': 'the other is the publication of a seminal paper by croft and porte'}
--------------------------------------------------
12 porte 1.2213278741915971e-06 the other is the publication of a seminal paper by croft and porte 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:15:23,870 --> 00:15:28,680': 'the other is the publication of a seminal paper by croft and porte'}
--------------------------------------------------
20 subtopical 1.2213278741915971e-06 that seems to suggest some different retrieval tasks so for example email was used in the enterprise search tasks and subtopical retrieval was another task later introduced by trec 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:16:22,980 --> 00:16:26,550': 'subtopical retrieval was another task later introduced by trec'}
--------------------------------------------------
4 generals 1.2213278741915971e-06 again the technique is generals so you can use this to analyze the impact of any event 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:17:00,764 --> 00:17:03,403': 'again the technique is generals so '}
--------------------------------------------------
6 staging 1.2213278741915971e-06 the first is paper about simple staging of psi to label crosscollection comparison 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:17:11,940 --> 00:17:20,090': 'the first is paper about simple staging of psi to label crosscollection comparison'}
--------------------------------------------------
8 psi 1.2213278741915971e-06 the first is paper about simple staging of psi to label crosscollection comparison 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:17:11,940 --> 00:17:20,090': 'the first is paper about simple staging of psi to label crosscollection comparison'}
--------------------------------------------------
11 crosscollection 2.006143151929004e-06 the first is paper about simple staging of psi to label crosscollection comparison 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:17:11,940 --> 00:17:20,090': 'the first is paper about simple staging of psi to label crosscollection comparison'}
--------------------------------------------------
22 algorithmatical 1.2213278741915971e-06 this model gives the sequence today wednesday a probability of  it gives today wednesday is a very very small probability because it algorithmatical 
{'4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:01:03,310 --> 00:01:08,550': 'today wednesday is a very very small probability because it algorithmatical'}
--------------------------------------------------
8 private 1.7445380593498679e-06 but imagine in the context of discussing a private math maybe the higher values positive would have a higher probability 
{'4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:01:28,500 --> 00:01:32,080': 'but imagine in the context of discussing a private math'}
--------------------------------------------------
15 device 2.4426557483831942e-06 so we can ask for a sequence and it to sample a sequence from the device if you want 
{'4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:02:08,090 --> 00:02:12,480': 'it to sample a sequence from the device if you want', '4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:05:08,070 --> 00:05:12,680': 'so for example now we can ask the device or the model'}
--------------------------------------------------
15 acoustical 1.2213278741915971e-06 obviously this would be very useful speech recognition because happy and habit would have similar acoustical sound 
{'4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:03:19,950 --> 00:03:23,380': 'habit would have similar acoustical sound '}
--------------------------------------------------
16 sound 2.2677482445081393e-06 obviously this would be very useful speech recognition because happy and habit would have similar acoustical sound 
{'3 - 14 - 2.14 Probabilistic Latent Semantic Analysis (PLSA)- Part 2 (00-10-15).srt : 00:00:00,366 --> 00:00:03,025': 'sound ', '2 - 2 - 1.2 Overview Text Mining and Analytics- Part 2 (00-11-44).srt : 00:00:00,266 --> 00:00:09,956': 'sound so', '5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:00:00,025 --> 00:00:05,363': 'sound this lecture is a continued ', '2 - 10 - 1.10 Syntagmatic Relation Discovery- Entropy (00-11-00).srt : 00:00:00,250 --> 00:00:06,380': 'sound ', '2 - 8 - 1.8 TF Transformation (00-09-31).srt : 00:00:00,354 --> 00:00:01,185': 'sound ', '4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:00:07,290 --> 00:00:11,068': 'sound this lecture is a continued discussion of', '4 - 8 - 3.6 Feedback in Text Retrieval (00-06-49).srt : 00:00:00,000 --> 00:00:05,731': 'sound this lecture is about ', '4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt : 00:00:00,000 --> 00:00:05,545': 'sound ', '5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:00:00,025 --> 00:00:04,628': 'sound this lecture is about ', '3 - 13 - 2.13 Probabilistic Latent Semantic Analysis (PLSA)- Part 1 (00-10-38).srt : 00:00:00,012 --> 00:00:07,295': 'sound this', '1 - 1 - Course Welcome (00-03-11).srt : 00:00:00,120 --> 00:00:08,484': 'sound hello', '5 - 10 - 4.6 Recommender Systems- Content-based Filtering - Part 1  (00-12-55).srt : 00:00:00,012 --> 00:00:04,586': 'sound ', '2 - 5 - 1.5 Vector Space Model- Basic Idea (00-09-44).srt : 00:00:00,012 --> 00:00:03,386': 'sound ', '4 - 2 - 3.2 Text Clustering- Generative Probabilistic Models Part 1 (00-16-18).srt : 00:00:00,217 --> 00:00:06,963': 'sound this', '2 - 3 - 1.3 Text Retrieval Problem (00-26-18).srt : 00:00:00,025 --> 00:00:05,716': 'sound this lecture is about ', '4 - 1 - 3.1 Probabilistic Retrieval Model- Basic Idea (00-12-44).srt : 00:00:00,000 --> 00:00:05,140': 'sound this lecture is about ', '4 - 1 - 3.1 Probabilistic Retrieval Model- Basic Idea (00-12-44).srt : 00:11:16,128 --> 00:11:22,091': 'we also talked about a proximate in this sound by using the query likelihood', '3 - 11 - 2.11 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2 (00-10-39).srt : 00:00:00,012 --> 00:00:08,224': 'sound so', '4 - 3 - 3.3 Text Clustering- Generative Probabilistic Models Part 2 (00-08-37).srt : 00:00:00,069 --> 00:00:07,429': 'sound this lecture', '4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:00:00,012 --> 00:00:03,295': 'sound ', '4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:03:19,950 --> 00:03:23,380': 'habit would have similar acoustical sound ', '3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt : 00:00:00,012 --> 00:00:08,031': 'sound this', '2 - 6 - 1.6 Vector Space Model- Simplest Instantiation (00-17-30).srt : 00:00:00,008 --> 00:00:10,008': 'sound ', '3 - 4 - 2.4 Probabilistic Topic Models- Overview of Statistical Language Models- Part 1 (00-10-25).srt : 00:00:00,025 --> 00:00:07,147': 'sound this', '5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:00:00,012 --> 00:00:04,920': 'sound ', '3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:00:00,000 --> 00:00:02,885': 'sound ', '3 - 6 - 2.6 Probabilistic Topic Models- Mining One Topic (00-12-21).srt : 00:00:00,025 --> 00:00:05,683': 'sound this lecture is a continued ', '4 - 9 - 3.7 Feedback in Vector Space Model- Rocchio (00-12-05).srt : 00:00:00,025 --> 00:00:05,172': 'sound this lecture is about ', '5 - 11 - 4.6  Recommender Systems- Content-based Filtering - Part 2 (00-10-42).srt : 00:00:00,000 --> 00:00:03,093': 'sound ', '5 - 14 - 4.7 Recommender Systems- Collaborative Filtering - Part 3 (00-04-45).srt : 00:00:00,083 --> 00:00:07,026': 'sound so to summarize our discussion of recommender systems', '2 - 7 - 1.7 Word Association Mining and Analysis (00-15-39).srt : 00:00:00,025 --> 00:00:04,546': 'sound this lecture is ', '5 - 12 - 4.7 Recommender Systems- Collaborative Filtering - Part 1 (00-06-20).srt : 00:00:00,000 --> 00:00:05,275': 'sound this lecture is about ', '2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:00:00,025 --> 00:00:06,228': 'sound this lecture is about ', '5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:00:00,346 --> 00:00:04,854': 'sound this lecture is ', '2 - 4 - 1.4 Natural Language Content Analysis- Part 2 (00-04-25).srt : 00:00:00,266 --> 00:00:05,440': 'sound ', '2 - 1 - 1.1 Overview Text Mining and Analytics- Part 1 (00-11-43).srt : 00:00:00,012 --> 00:00:06,665': 'sound in', '4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt : 00:00:00,012 --> 00:00:07,093': 'sound this', '2 - 7 - 1.7 Vector Space Model- Improved Instantiation (00-16-52).srt : 00:00:00,012 --> 00:00:04,774': 'sound ', '5 - 7 - 4.4 Learning to Rank - Part 2 (00-05-54).srt : 00:00:00,025 --> 00:00:08,649': 'sound so now let take a look at the specific', '1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:00:00,025 --> 00:00:03,183': 'sound ', '3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).srt : 00:00:00,025 --> 00:00:04,912': 'sound this lecture is about how to ', '3 - 4 - 2.4 Evaluation of TR Systems (00-10-10).srt : 00:00:00,025 --> 00:00:05,569': 'sound this lecture is about ', '5 - 5 - 4.3 Link Analysis - Part 3 (00-05-59).srt : 00:00:00,025 --> 00:00:09,030': 'sound so we talked about a page rank as a way to', '5 - 2 - 4.2 Web Indexing (00-17-19).srt : 00:00:00,172 --> 00:00:07,487': 'sound ', '4 - 1 - 3.1 Text Clustering- Motivation (00-15-52).srt : 00:00:00,012 --> 00:00:07,427': 'sound this', '2 - 3 - 1.3 Natural Language Content Analysis- Part 1 (00-12-48).srt : 00:00:00,300 --> 00:00:03,380': 'sound ', '2 - 9 - 1.9 Doc Length Normalization (00-18-56).srt : 00:10:22,994 --> 00:10:29,150': 'like the or of we use the phrases to define that sound', '2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:00:00,025 --> 00:00:06,397': 'sound in this lecture we continue discussing', '5 - 6 - 4.6 Text-Based Prediction (00-12-08).srt : 00:00:00,025 --> 00:00:05,203': 'sound this lecture is about ', '2 - 5 - 1.5 Text Representation- Part 1 (00-10-46).srt : 00:00:07,540 --> 00:00:10,170': 'sound this lecture is about text representation', '3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:00:00,008 --> 00:00:04,562': 'sound this lecture is about ', '3 - 16 - 2.16 Latent Dirichlet Allocation (LDA)- Part 2 (00-12-03).srt : 00:00:00,025 --> 00:00:05,631': 'sound so now let talk about the exchanging of', '2 - 2 - 1.2 Text Access (00-09-24).srt : 00:00:08,450 --> 00:00:14,319': 'sound in this lecture were going to talk about text access', '4 - 9 - 3.9 Text Categorization- Generative Probabilistic Models (00-31-18).srt : 00:00:00,012 --> 00:00:08,521': 'sound this lecture is', '3 - 7 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01) .srt : 00:00:00,012 --> 00:00:03,467': 'sound ', '2 - 11 - 1.11 Syntagmatic Relation Discovery- Conditional Entropy (00-11-57).srt : 00:00:00,025 --> 00:00:05,819': 'sound this lecture is about the syntagmatic', '5 - 3 - 4.3 Link Analysis - Part 1 (00-09-16).srt : 00:00:00,025 --> 00:00:07,462': 'sound ', '4 - 7 - 3.7 Text Categorization- Motivation (00-14-37).srt : 00:00:00,192 --> 00:00:03,512': 'sound ', '3 - 1 - 2.1 Topic Mining and Analysis- Motivation and Task Definition (00-07-36).srt : 00:00:00,025 --> 00:00:06,885': 'sound this', '4 - 5 - 3.4 Smoothing of Language Model - Part 2 (00-09-36).srt : 00:00:00,006 --> 00:00:03,503': 'sound ', '2 - 13 - 1.13 Syntagmatic Relation Discovery- Mutual Information- Part 2 (00-09-42).srt : 00:00:00,000 --> 00:00:04,714': 'sound ', '2 - 13 - 1.13 Syntagmatic Relation Discovery- Mutual Information- Part 2 (00-09-42).srt : 00:09:29,441 --> 00:09:39,441': 'sound ', '5 - 9 - 4.9 Contextual Text Mining- Mining Topics with Social Network Context (00-14-43).srt : 00:00:00,025 --> 00:00:05,098': 'sound this lecture is about ', '3 - 9 - 2.9 Probabilistic Topic Models- Mixture Model Estimation- Part 2 (00-08-15).srt : 00:00:00,025 --> 00:00:07,001': 'sound now lets look at another behaviour of the mixed model and', '5 - 6 - 4.4 Learning to Rank Part 1 (00-13-09).srt : 00:00:00,000 --> 00:00:06,657': 'sound this lecture is about ', '2 - 8 - 1.8 Paradigmatic Relation Discovery Part 1 (00-14-31).srt : 00:00:00,025 --> 00:00:07,935': 'sound this', '5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:00:00,012 --> 00:00:04,047': 'sound ', '2 - 6 - 1.6 Text Representation- Part 2 (00-09-29).srt : 00:00:00,532 --> 00:00:08,683': 'sound ', '4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:00:00,025 --> 00:00:06,573': 'sound this lecture is a continued discussion', '4 - 12 - 3.12 Text Categorization- Evaluation Part 1 (00-14-12).srt : 00:00:00,025 --> 00:00:04,845': 'sound this lecture is about ', '4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt : 00:00:00,025 --> 00:00:06,458': 'sound this lecture is about the feedback ', '1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:00:00,124 --> 00:00:07,641': 'sound this', '4 - 4 - 3.4 Text Clustering- Generative Probabilistic Models Part 3 (00-14-55).srt : 00:00:00,012 --> 00:00:08,289': 'sound this lecture', '3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:00:00,012 --> 00:00:03,256': 'sound ', '5 - 7 - 4.7 Contextual Text Mining- Motivation (00-06-47).srt : 00:00:00,160 --> 00:00:07,187': 'sound this', '4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:00:00,006 --> 00:00:03,253': 'sound ', '3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).srt : 00:00:00,000 --> 00:00:06,869': 'sound this lecture is about the the basic measures for', '3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).srt : 00:00:17,202 --> 00:00:24,290': 'measures sound to quantitatively compare two original sound systems', '3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).srt : 00:00:52,197 --> 00:00:57,140': 'sound which settles results is better is system a better or system b better', '3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).srt : 00:00:57,140 --> 00:01:02,318': 'sound so let now talk about how to actually quantify their performance', '3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).srt : 00:07:07,210 --> 00:07:10,969': 'so naturally it would be interesting to sound combine them and', '5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:00:00,000 --> 00:00:05,086': 'sound ', '2 - 12 - 1.12 Syntagmatic Relation Discovery- Mutual Information- Part 1 (00-13-55).srt : 00:00:00,025 --> 00:00:07,457': 'sound '}
--------------------------------------------------
0 acoustic 1.7445380593498679e-06 acoustic signals 
{'4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:03:23,380 --> 00:03:25,180': 'acoustic signals '}
--------------------------------------------------
2 lan 2.4426557483831942e-06 called a lan unigram language model 
{'4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:04:05,720 --> 00:04:07,910': 'called a lan unigram language model ', '4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:02:58,720 --> 00:03:00,996': 'and we see docking the lan relationship here'}
--------------------------------------------------
8 device 2.4426557483831942e-06 so for example now we can ask the device or the model to stochastically generate the words for us instead of in sequences 
{'4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:02:08,090 --> 00:02:12,480': 'it to sample a sequence from the device if you want', '4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:05:08,070 --> 00:05:12,680': 'so for example now we can ask the device or the model'}
--------------------------------------------------
13 stochastically 1.2213278741915971e-06 so for example now we can ask the device or the model to stochastically generate the words for us instead of in sequences 
{'4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:05:12,680 --> 00:05:18,020': 'to stochastically generate the words for us instead of in sequences'}
--------------------------------------------------
0 supposedly 1.2213278741915971e-06 supposedly we now have available a particular document 
{'4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:08:23,900 --> 00:08:28,250': 'supposedly we now have available a particular document'}
--------------------------------------------------
5 justified 1.2213278741915971e-06 and that in fact inaudible justified 
{'4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:09:27,260 --> 00:09:29,440': 'and that in fact inaudible justified '}
--------------------------------------------------
22 incentive 2.4426557483831942e-06 well a consequence of this is of course were going to assign if we have an observed word there will be no incentive to assign a non probability using this approach 
{'4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:10:20,350 --> 00:10:25,280': 'there will be no incentive to assign a non probability using this approach', '3 - 9 - 2.9 Probabilistic Topic Models- Mixture Model Estimation- Part 2 (00-08-15).srt : 00:05:37,910 --> 00:05:44,100': 'if you have a various small probability of being chosen then the incentive is less'}
--------------------------------------------------
2 surprisingly 2.4426557483831942e-06 well not surprisingly we see these common words on top as we always do 
{'4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:14:13,220 --> 00:14:19,440': 'well not surprisingly we see these common words on top as we always do', '2 - 6 - 1.6 Text Representation- Part 2 (00-09-29).srt : 00:08:20,910 --> 00:08:25,373': 'third these techniques are actually surprisingly powerful and'}
--------------------------------------------------
18 ramped 1.2213278741915971e-06 so if we do that we take the ratio well see that then on the top computer is ramped and then followed by software program all these words related to computer 
{'4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:16:06,150 --> 00:16:09,800': 'computer is ramped and then followed by software'}
--------------------------------------------------
4 ball 2.4426557483831942e-06 by taking the same ball of text that contains the computer we dont really see more occurrences of that in general 
{'4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:16:30,780 --> 00:16:35,670': 'by taking the same ball of text that contains the computer we dont', '5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:09:54,700 --> 00:10:01,940': 'so is it because she here on the ball easily taken from the previous slide'}
--------------------------------------------------
4 simplistic 1.2213278741915971e-06 we talked about the simplistic language model called unigram language model 
{'4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:16:56,360 --> 00:17:00,130': 'we talked about the simplistic language model called unigram language model'}
--------------------------------------------------
12 classing 1.2213278741915971e-06 one is to represent the the topic in a document in a classing or in general 
{'4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:17:05,320 --> 00:17:10,360': 'one is to represent the the topic in a document in a classing or in general'}
--------------------------------------------------
15 pointers 1.7445380593498679e-06 the second is a article that has a survey of statistical language models with other pointers to research work 
{'4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:17:35,468 --> 00:17:39,378': 'models with other pointers to research work'}
--------------------------------------------------
2 topwords 1.2213278741915971e-06 from the topwords equation 
{'3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt : 00:00:30,026 --> 00:00:33,480': 'from the topwords equation '}
--------------------------------------------------
7 pop 2.4426557483831942e-06 and the other kind is from our pop board distribution that we are interested in 
{'3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt : 00:00:48,800 --> 00:00:53,820': 'and the other kind is from our pop board distribution that we are interested in', '2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:06:39,060 --> 00:06:42,290': 'pop that commonly used in retrieval '}
--------------------------------------------------
8 probabilist 1.7445380593498679e-06 so this is a case of customizing a probabilist model so that we embedded a known variable that we are interested in 
{'3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt : 00:01:20,130 --> 00:01:25,130': 'so this is a case of customizing a probabilist model so'}
--------------------------------------------------
6 holistically 1.2213278741915971e-06 now although we designed the model holistically 
{'3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt : 00:02:09,470 --> 00:02:12,530': 'now although we designed the model holistically'}
--------------------------------------------------
6 generalizable 1.2213278741915971e-06 the observed pattern here actually are generalizable to mixture model in general 
{'3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt : 00:03:08,860 --> 00:03:15,130': 'the observed pattern here actually are generalizable to mixture model in general'}
--------------------------------------------------
25 likeable 1.2213278741915971e-06 now lets also assume that our data is extremely simple the document has just two words text and the so now lets right down the likeable function in such a case 
{'3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt : 00:04:08,340 --> 00:04:13,820': 'two words text and the so now lets right down the likeable function in such a case'}
--------------------------------------------------
3 lateral 1.2213278741915971e-06 so naturally our lateral function is just a product of the two 
{'3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt : 00:04:58,900 --> 00:05:03,490': 'so naturally our lateral function is just a product of the two'}
--------------------------------------------------
2 inducing 1.2213278741915971e-06 and this inducing can be work supported by mathematical fact which is when the sum of two variables is a constant then the product of them which is maximum when they are equal and this is a fact we know from algebra 
{'3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt : 00:07:32,130 --> 00:07:38,160': 'and this inducing can be work supported by mathematical fact which is when'}
--------------------------------------------------
5 weak 2.4426557483831942e-06 because the background part is weak for text 
{'3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt : 00:08:56,690 --> 00:08:59,850': 'because the background part is weak for text', '4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:09:54,920 --> 00:10:00,220': 'to see subtle differences between methods or tow see where a method might be weak'}
--------------------------------------------------
4 compensate 1.2213278741915971e-06 so in order to compensate for that we must make the probability for text given by theta sub d somewhat larger so that the two sides can be balanced 
{'3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt : 00:09:00,710 --> 00:09:04,900': 'so in order to compensate for that we must make the probability for'}
--------------------------------------------------
28 balanced 1.2213278741915971e-06 so in order to compensate for that we must make the probability for text given by theta sub d somewhat larger so that the two sides can be balanced 
{'3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt : 00:09:04,900 --> 00:09:11,270': 'text given by theta sub d somewhat larger so that the two sides can be balanced'}
--------------------------------------------------
3 discourage 1.7445380593498679e-06 basically it would discourage other distributions to do the same and this is to balance them out so we can account for all kinds of words 
{'3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt : 00:09:25,540 --> 00:09:28,960': 'basically it would discourage other distributions to do the same and'}
--------------------------------------------------
11 motor 1.2213278741915971e-06 meaning that they have a very small probability from the background motor like text here 
{'3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt : 00:09:58,170 --> 00:10:02,754': 'meaning that they have a very small probability from the background motor like'}
--------------------------------------------------
25 opinionsabout 1.2213278741915971e-06 so if we can decompose the overall ratings the ratings on these different aspects then we can obtain a more detailed understanding of the reviewer opinionsabout the hotel 
{'5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:01:24,820 --> 00:01:29,442': 'can obtain a more detailed understanding of the reviewer opinionsabout the hotel'}
--------------------------------------------------
4 aspectlevel 1.7445380593498679e-06 we can generate an aspectlevel opinion summary 
{'5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:04:13,260 --> 00:04:17,590': 'we can generate an aspectlevel opinion summary'}
--------------------------------------------------
7 brisk 1.2213278741915971e-06 but im going to give you a brisk basic introduction to the technique development for this problem 
{'5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:04:41,050 --> 00:04:42,926': 'but im going to give you a brisk '}
--------------------------------------------------
21 segmented 2.2677482445081393e-06 and then from those segments we can further mine correlated words with these seed words and that would allow us to segmented the text into segments discussing different aspects 
{'5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:05:41,467 --> 00:05:46,419': 'words with these seed words and that would allow us to segmented'}
--------------------------------------------------
10 council 1.2213278741915971e-06 but anyway that the first stage where the obtain the council of words in each segment 
{'5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:05:59,970 --> 00:06:05,470': 'where the obtain the council of words in each segment'}
--------------------------------------------------
8 acted 1.2213278741915971e-06 but another word like far which is an acted weight if it mentioned many times and it will decrease the rating 
{'5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:06:40,827 --> 00:06:44,727': 'but another word like far which is an acted weight'}
--------------------------------------------------
3 sentimental 2.4426557483831942e-06 of course these sentimental weights might be different for different aspects 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:03:47,770 --> 00:03:52,930': 'and these are sentimental weights for words in different aspects', '5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:06:58,697 --> 00:07:05,640': 'of course these sentimental weights might be different for different aspects'}
--------------------------------------------------
14 observable 1.7445380593498679e-06 so this set up allows us to predict the overall rating based on the observable frequencies 
{'5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:07:53,230 --> 00:07:56,110': 'the observable frequencies '}
--------------------------------------------------
12 rs 1.2213278741915971e-06 and then of course we can adjust these parameter values including betas rs and alpha is in order to maximize the probability of the data 
{'5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:08:31,440 --> 00:08:38,894': 'and then of course we can adjust these parameter values including betas rs and'}
--------------------------------------------------
10 pisa 1.2213278741915971e-06 so we have seen such cases before in for example pisa where we predict a text data 
{'5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:08:54,248 --> 00:08:59,361': 'example pisa where we predict a text data'}
--------------------------------------------------
4 composer 1.2213278741915971e-06 and these are the composer ratings on different aspects 
{'5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:09:13,470 --> 00:09:16,175': 'and these are the composer ratings on different aspects'}
--------------------------------------------------
19 presegments 1.2213278741915971e-06 and each review document denote by a d and the overall ratings denote by r sub d and d presegments turn into k aspect segments 
{'5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:09:45,130 --> 00:09:48,578': 'and d presegments turn into k aspect segments'}
--------------------------------------------------
5 ciwd 1.2213278741915971e-06 and were going to use ciwd to denote the count of word w in aspect segment i 
{'5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:09:48,578 --> 00:09:55,915': 'and were going to use ciwd to denote the count of word w in aspect segment i'}
--------------------------------------------------
17 provisional 1.2213278741915971e-06 now the model is going to predict the rating based on d so were interested in the provisional problem of r subd given d and this model is set up as follows 
{'5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:10:04,070 --> 00:10:10,757': 'so were interested in the provisional problem of r subd given d'}
--------------------------------------------------
21 subd 2.4426557483831942e-06 now the model is going to predict the rating based on d so were interested in the provisional problem of r subd given d and this model is set up as follows 
{'5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:10:04,070 --> 00:10:10,757': 'so were interested in the provisional problem of r subd given d', '5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:10:13,276 --> 00:10:18,457': 'so r subd is assumed the two follow a normal distribution'}
--------------------------------------------------
2 subd 2.4426557483831942e-06 so r subd is assumed the two follow a normal distribution doesnt mean that denotes actually await the average of the aspect of ratings r sub i of d as shown here 
{'5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:10:04,070 --> 00:10:10,757': 'so were interested in the provisional problem of r subd given d', '5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:10:13,276 --> 00:10:18,457': 'so r subd is assumed the two follow a normal distribution'}
--------------------------------------------------
16 await 1.2213278741915971e-06 so r subd is assumed the two follow a normal distribution doesnt mean that denotes actually await the average of the aspect of ratings r sub i of d as shown here 
{'5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:10:18,457 --> 00:10:23,321': 'doesnt mean that denotes actually await the average'}
--------------------------------------------------
8 squared 2.2677482445081393e-06 this normal distribution is a variance of data squared 
{'5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:10:28,150 --> 00:10:30,350': 'this normal distribution is a variance of data squared'}
--------------------------------------------------
24 covariance 1.2213278741915971e-06 and were going to assume that this vector itself is drawn from another multivariate gaussian distribution with mean denoted by a mu factor and covariance metrics sigma here 
{'5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:11:36,080 --> 00:11:41,960': 'with mean denoted by a mu factor and covariance metrics sigma here'}
--------------------------------------------------
18 aspectspecific 1.2213278741915971e-06 it also used for see what parameters we have here beta sub i and w gives us the aspectspecific sentiment of w so obviously that one of the important parameters 
{'5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:12:54,536 --> 00:12:59,488': 'w gives us the aspectspecific sentiment of w'}
--------------------------------------------------
23 maximized 1.7445380593498679e-06 now we can as usual use the maximum likelihood estimate and this will give us the settings of these parameters that with a maximized observed ratings condition of their respective reviews 
{'5 - 4 - 4.4 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 1 (00-15-17).srt : 00:13:28,395 --> 00:13:34,300': 'that with a maximized observed ratings condition of their respective reviews'}
--------------------------------------------------
2 guidance 1.7445380593498679e-06 as a guidance to instantiate the framework to derive a specific ranking function 
{'2 - 6 - 1.6 Vector Space Model- Simplest Instantiation (00-17-30).srt : 00:00:40,370 --> 00:00:48,270': 'as a guidance to instantiate the framework to derive a specific ranking function'}
--------------------------------------------------
3 waits 2.2677482445081393e-06 and then the waits of terms for query and document 
{'2 - 6 - 1.6 Vector Space Model- Simplest Instantiation (00-17-30).srt : 00:02:18,976 --> 00:02:22,940': 'and then the waits of terms for query and document'}
--------------------------------------------------
5 simpliciter 1.2213278741915971e-06 and think about the the simpliciter instantiation 
{'2 - 6 - 1.6 Vector Space Model- Simplest Instantiation (00-17-30).srt : 00:02:43,790 --> 00:02:47,630': 'and think about the the simpliciter instantiation'}
--------------------------------------------------
4 figured 1.2213278741915971e-06 but you still havent figured out how to compute this vector exactly how to define this similarity function 
{'2 - 6 - 1.6 Vector Space Model- Simplest Instantiation (00-17-30).srt : 00:03:03,800 --> 00:03:07,687': 'but you still havent figured out how to compute this vector exactly'}
--------------------------------------------------
5 ramp 1.2213278741915971e-06 where we use it to ramp it up for use in query 
{'2 - 6 - 1.6 Vector Space Model- Simplest Instantiation (00-17-30).srt : 00:07:24,140 --> 00:07:27,280': 'where we use it to ramp it up for use in query'}
--------------------------------------------------
3 pausing 2.4426557483831942e-06 and perhaps by pausing the lecture 
{'2 - 6 - 1.6 Vector Space Model- Simplest Instantiation (00-17-30).srt : 00:09:07,241 --> 00:09:10,190': 'and perhaps by pausing the lecture ', '2 - 5 - 1.5 Text Representation- Part 1 (00-10-46).srt : 00:05:02,433 --> 00:05:07,360': 'then well be pausing the sentence to obtain a syntactic structure'}
--------------------------------------------------
10 nonrelavant 1.2213278741915971e-06 and the other three d d and d are really nonrelavant 
{'2 - 6 - 1.6 Vector Space Model- Simplest Instantiation (00-17-30).srt : 00:09:35,770 --> 00:09:41,800': 'and the other three d d and d are really nonrelavant'}
--------------------------------------------------
8 zeroes 2.2677482445081393e-06 we actually dont have to care about these zeroes because if whenever we have a zero the product will be zero 
{'2 - 6 - 1.6 Vector Space Model- Simplest Instantiation (00-17-30).srt : 00:11:44,470 --> 00:11:48,860': 'we actually dont have to care about these zeroes'}
--------------------------------------------------
8 meshes 2.4426557483831942e-06 so as a result this scoring function basically meshes how many unique query terms are matched in a document 
{'4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:03:20,110 --> 00:03:26,750': 'give us various meshes to quantitatively evaluate a cluster a clustering result', '2 - 6 - 1.6 Vector Space Model- Simplest Instantiation (00-17-30).srt : 00:12:58,740 --> 00:13:03,820': 'meshes how many unique query terms are matched in a document'}
--------------------------------------------------
5 distinctive 1.2213278741915971e-06 because d matched the three distinctive query words news presidential campaign 
{'2 - 6 - 1.6 Vector Space Model- Simplest Instantiation (00-17-30).srt : 00:13:15,770 --> 00:13:21,640': 'because d matched the three distinctive query words news presidential campaign'}
--------------------------------------------------
7 extended 1.7445380593498679e-06 in case of d presidential could be extended mentioned 
{'2 - 6 - 1.6 Vector Space Model- Simplest Instantiation (00-17-30).srt : 00:14:42,100 --> 00:14:47,365': 'in case of d presidential could be extended mentioned'}
--------------------------------------------------
11 soul 1.2213278741915971e-06 another problem is that d and d also have the same soul 
{'2 - 6 - 1.6 Vector Space Model- Simplest Instantiation (00-17-30).srt : 00:14:51,126 --> 00:14:58,200': 'another problem is that d and d also have the same soul'}
--------------------------------------------------
10 statical 1.2213278741915971e-06 in this lecture were going to give a overview of statical language models 
{'3 - 4 - 2.4 Probabilistic Topic Models- Overview of Statistical Language Models- Part 1 (00-10-25).srt : 00:00:15,906 --> 00:00:21,320': 'a overview of statical language models '}
--------------------------------------------------
9 nongrammatical 1.2213278741915971e-06 it might give today wednesday is which is a nongrammatical sentence a very very small probability as shown here 
{'3 - 4 - 2.4 Probabilistic Topic Models- Overview of Statistical Language Models- Part 1 (00-10-25).srt : 00:00:49,040 --> 00:00:53,560': 'is a nongrammatical sentence a very very small probability as shown here'}
--------------------------------------------------
5 eigenvalue 2.4426557483831942e-06 and similarly another sentence the eigenvalue is positive might get the probability of  
{'3 - 4 - 2.4 Probabilistic Topic Models- Overview of Statistical Language Models- Part 1 (00-10-25).srt : 00:00:56,170 --> 00:01:01,430': 'the eigenvalue is positive might get the probability of', '3 - 4 - 2.4 Probabilistic Topic Models- Overview of Statistical Language Models- Part 1 (00-10-25).srt : 00:04:06,610 --> 00:04:11,910': 'and some other words like eigenvalue might have a small probability etcetera'}
--------------------------------------------------
5 eigenvalue 2.4426557483831942e-06 and some other words like eigenvalue might have a small probability etcetera 
{'3 - 4 - 2.4 Probabilistic Topic Models- Overview of Statistical Language Models- Part 1 (00-10-25).srt : 00:00:56,170 --> 00:01:01,430': 'the eigenvalue is positive might get the probability of', '3 - 4 - 2.4 Probabilistic Topic Models- Overview of Statistical Language Models- Part 1 (00-10-25).srt : 00:04:06,610 --> 00:04:11,910': 'and some other words like eigenvalue might have a small probability etcetera'}
--------------------------------------------------
5 fake 1.7445380593498679e-06 for example i show some fake numbers here and when you multiply these numbers together you get the probability that today wednesday 
{'3 - 4 - 2.4 Probabilistic Topic Models- Overview of Statistical Language Models- Part 1 (00-10-25).srt : 00:04:42,000 --> 00:04:45,380': 'for example i show some fake numbers here and when you'}
--------------------------------------------------
5 attacks 1.2213278741915971e-06 although the probability of generating attacks mine inaudible publishing in the top conference is nonzero assuming that no word has a zero probability in the distribution 
{'3 - 4 - 2.4 Probabilistic Topic Models- Overview of Statistical Language Models- Part 1 (00-10-25).srt : 00:06:45,150 --> 00:06:49,079': 'although the probability of generating attacks mine'}
--------------------------------------------------
8 publishing 1.7445380593498679e-06 although the probability of generating attacks mine inaudible publishing in the top conference is nonzero assuming that no word has a zero probability in the distribution 
{'3 - 4 - 2.4 Probabilistic Topic Models- Overview of Statistical Language Models- Part 1 (00-10-25).srt : 00:06:49,079 --> 00:06:53,535': 'inaudible publishing in the top conference is'}
--------------------------------------------------
3 healthy 2.2677482445081393e-06 so food inaudible healthy inaudible etcetera 
{'3 - 4 - 2.4 Probabilistic Topic Models- Overview of Statistical Language Models- Part 1 (00-10-25).srt : 00:07:14,310 --> 00:07:17,940': 'so food inaudible healthy inaudible etcetera'}
--------------------------------------------------
26 guesses 1.2213278741915971e-06 of course in order to answer this question we have to define what do we mean by best in this case it turns out that our guesses are indeed the best 
{'3 - 4 - 2.4 Probabilistic Topic Models- Overview of Statistical Language Models- Part 1 (00-10-25).srt : 00:09:45,070 --> 00:09:50,540': 'it turns out that our guesses are indeed the best'}
--------------------------------------------------
6 dow 1.2213278741915971e-06 here what you are seeing is dow jones industrial average stock price curves 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:00:34,450 --> 00:00:39,430': 'here what you are seeing is dow jones industrial average stock price curves'}
--------------------------------------------------
8 industrial 1.2213278741915971e-06 here what you are seeing is dow jones industrial average stock price curves 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:00:34,450 --> 00:00:39,430': 'here what you are seeing is dow jones industrial average stock price curves'}
--------------------------------------------------
21 stamp 1.7445380593498679e-06 well if you know the background and you might be able to figure it out if you look at the time stamp or there are other data that can help us think about 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:00:51,760 --> 00:00:56,180': 'look at the time stamp or there are other data that can help us think about'}
--------------------------------------------------
9 rise 1.7445380593498679e-06 and that the time when there is a sudden rise of the topic about september  happened in news articles 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:01:16,480 --> 00:01:21,270': 'and that the time when there is a sudden rise of the topic'}
--------------------------------------------------
5 trunk 1.2213278741915971e-06 for example i write a trunk of market would have stocks for each candidate 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:01:36,792 --> 00:01:44,980': 'for example i write a trunk of market would have stocks for each candidate'}
--------------------------------------------------
10 stocks 1.7445380593498679e-06 for example i write a trunk of market would have stocks for each candidate 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:01:36,792 --> 00:01:44,980': 'for example i write a trunk of market would have stocks for each candidate'}
--------------------------------------------------
19 causing 1.7445380593498679e-06 and if you believe one candidate that will win then you tend to buy the stock for that candidate causing the price of that candidate to increase 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:01:49,660 --> 00:01:53,970': 'that candidate causing the price of that candidate to increase'}
--------------------------------------------------
7 marks 2.4426557483831942e-06 that why we put causal in quotation marks 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:04:13,190 --> 00:04:16,750': 'but i still put automatic in quotation marks because', '5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:03:43,090 --> 00:03:47,960': 'that why we put causal in quotation marks'}
--------------------------------------------------
5 correlating 1.2213278741915971e-06 but at least they are correlating topics that might potentially explain the cause and humans can certainly further analyze such topics to understand the issue better 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:03:47,960 --> 00:03:51,600': 'but at least they are correlating topics that might potentially'}
--------------------------------------------------
5 reprehend 1.7445380593498679e-06 meaning that they have to reprehend the meaningful topics in text 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:04:17,270 --> 00:04:21,477': 'meaning that they have to reprehend the meaningful topics in text'}
--------------------------------------------------
0 cement 1.2213278741915971e-06 cement but also more importantly they should be correlated with external hand series that given as a context 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:04:21,477 --> 00:04:23,870': 'cement but also more importantly '}
--------------------------------------------------
16 reactive 1.4829329667707326e-06 so to understand how we solve this problem let first adjust to solve the problem with reactive topic model for example prsa 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:04:33,580 --> 00:04:36,930': 'solve the problem with reactive topic model for example prsa'}
--------------------------------------------------
14 cprsa 1.2213278741915971e-06 and we can apply this to text stream and with some extension like a cprsa or contextual prsa 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:04:40,330 --> 00:04:44,260': 'with some extension like a cprsa or contextual prsa'}
--------------------------------------------------
1 awareness 2.4426557483831942e-06 because awareness pictured to the topics is that they will discover by prsa or lda 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:05:09,050 --> 00:05:13,150': 'awareness pictured to the topics is that they will discover by prsa or lda', '4 - 9 - 3.7 Feedback in Vector Space Model- Rocchio (00-12-05).srt : 00:11:15,040 --> 00:11:19,660': 'and this is especially true for pseudo awareness feedback'}
--------------------------------------------------
2 pictured 1.2213278741915971e-06 because awareness pictured to the topics is that they will discover by prsa or lda 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:05:09,050 --> 00:05:13,150': 'awareness pictured to the topics is that they will discover by prsa or lda'}
--------------------------------------------------
0 aand 1.2213278741915971e-06 aand they are not necessarily correlated with time series 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:05:24,685 --> 00:05:28,710': 'aand they are not necessarily correlated with time series'}
--------------------------------------------------
15 causally 1.7445380593498679e-06 and then were going to use external time series to assess which topic is more causally related or correlated with the external time series 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:06:14,540 --> 00:06:19,140': 'which topic is more causally related or correlated with the external time series'}
--------------------------------------------------
15 approached 1.2213278741915971e-06 now we could have stopped here and that would be just like what the simple approached that i talked about earlier then we can get to these topics and call them causal topics 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:06:29,660 --> 00:06:33,530': 'that would be just like what the simple approached that i talked about earlier'}
--------------------------------------------------
17 analyzeword 1.2213278741915971e-06 and then we can further analyze the components at work in the topic and then try to analyzeword level correlation 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:09:32,180 --> 00:09:35,380': 'then try to analyzeword level correlation'}
--------------------------------------------------
17 ultimate 1.2213278741915971e-06 so this whole process is just a heuristic way of optimizing causality and coherence and that our ultimate goal 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:09:50,460 --> 00:09:52,840': 'coherence and that our ultimate goal '}
--------------------------------------------------
4 cementric 1.2213278741915971e-06 it might not be cementric connected 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:10:14,770 --> 00:10:17,820': 'it might not be cementric connected '}
--------------------------------------------------
7 xt 2.2677482445081393e-06 now the the question here is does xt cause yt 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:11:52,783 --> 00:11:57,560': 'now the the question here is does xt cause yt'}
--------------------------------------------------
7 regressive 1.2213278741915971e-06 basically youre going to have all the regressive model to use the history information of y to predict itself 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:12:55,040 --> 00:12:58,710': 'basically youre going to have all the regressive model to'}
--------------------------------------------------
8 insignificant 1.2213278741915971e-06 if on the other hand the difference is insignificant and that would mean x does not really have a cause or relation why 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:13:32,080 --> 00:13:35,670': 'if on the other hand the difference is insignificant and'}
--------------------------------------------------
26 wise 2.4426557483831942e-06 american airlines and apple and the goal is to see if we inject the sum time series contest whether we can actually get topics that are wise for the time series 
{'4 - 6 - 3.6 Text Clustering- Evaluation (00-10-11).srt : 00:05:19,098 --> 00:05:25,120': 'now procedure wise we also would create a test set with text objects for', '5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:14:21,230 --> 00:14:26,580': 'whether we can actually get topics that are wise for the time series'}
--------------------------------------------------
7 underlined 1.2213278741915971e-06 and particularly if you look at the underlined words here in the american airlines result and you see airlines airport air united trade or terrorism etc 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:14:47,860 --> 00:14:51,030': 'and particularly if you look at the underlined words here'}
--------------------------------------------------
20 air 2.006143151929004e-06 and particularly if you look at the underlined words here in the american airlines result and you see airlines airport air united trade or terrorism etc 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:14:54,280 --> 00:14:59,090': 'airport air united trade or terrorism etc'}
--------------------------------------------------
24 terrorism 2.2677482445081393e-06 and particularly if you look at the underlined words here in the american airlines result and you see airlines airport air united trade or terrorism etc 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:14:54,280 --> 00:14:59,090': 'airport air united trade or terrorism etc'}
--------------------------------------------------
9 served 1.4829329667707326e-06 so that just means the time series has effectively served as a context to bias the discovery of topics 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:15:19,880 --> 00:15:24,410': 'has effectively served as a context to bias the discovery of topics'}
--------------------------------------------------
11 filtered 2.2677482445081393e-06 now here i should mention that the text data has been filtered by using only the articles that mention these candidate names 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:16:35,710 --> 00:16:40,260': 'now here i should mention that the text data has been filtered by using'}
--------------------------------------------------
5 wikipedia 1.7445380593498679e-06 and also i was discussing wikipedia right 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:17:17,250 --> 00:17:21,960': 'and also i was discussing wikipedia right'}
--------------------------------------------------
8 casuality 1.2213278741915971e-06 and the second one is reading about granger casuality text 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:17:48,380 --> 00:17:52,965': 'and the second one is reading about granger casuality text'}
--------------------------------------------------
5 inform 1.7445380593498679e-06 because they can help us inform new knowledge about the world 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:18:09,070 --> 00:18:13,340': 'because they can help us inform new knowledge about the world'}
--------------------------------------------------
4 wider 1.2213278741915971e-06 and this has a wider spread application 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:18:23,950 --> 00:18:25,609': 'and this has a wider spread application '}
--------------------------------------------------
21 cruel 1.2213278741915971e-06 because for this purpose the prediction purpose we generally would like to combine nontext data and text data together as much cruel as possible for prediction 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:18:37,940 --> 00:18:41,740': 'as much cruel as possible for prediction '}
--------------------------------------------------
2 curse 1.2213278741915971e-06 now of curse the vector space model is a special case of this 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:00:32,720 --> 00:00:37,010': 'now of curse the vector space model is a special case of this'}
--------------------------------------------------
18 adjuster 1.2213278741915971e-06 and then finally this adjustment function would then consider the document level or query level factors through further adjuster score for example document lens inaudible 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:02:46,134 --> 00:02:51,260': 'the document level or query level factors through further adjuster score'}
--------------------------------------------------
41 ef 1.7445380593498679e-06 then for each entry d sub j and f sub j a particular match of the term in this particular document d sub j were going to computer the function g that would give us something like a t of i ef weights of this term 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:04:15,690 --> 00:04:20,860': 'that would give us something like a t of i ef weights of this term'}
--------------------------------------------------
11 spot 2.2677482445081393e-06 and we add  or we locate another score coming in the spot d and add  to it 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:07:33,490 --> 00:07:39,560': 'and we add or we locate another score coming in the spot d and add to it'}
--------------------------------------------------
13 ti 1.2213278741915971e-06 and finally the d gets a  because the information the term information occurred ti in five times in this document 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:07:44,503 --> 00:07:50,450': 'the term information occurred ti in five times in this document'}
--------------------------------------------------
3 completes 1.2213278741915971e-06 okay so this completes the processing of all the entries in the inverted index for information 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:07:50,450 --> 00:07:54,035': 'okay so this completes the processing of all the entries in the'}
--------------------------------------------------
4 contributions 2.006143151929004e-06 it processed all the contributions of matching information in this four documents 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:07:56,500 --> 00:08:00,080': 'it processed all the contributions of matching information in this'}
--------------------------------------------------
10 accumulating 1.2213278741915971e-06 so this time were going to do change the score accumulating d sees it already allocate 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:08:28,900 --> 00:08:32,230': 'accumulating d sees it already allocate'}
--------------------------------------------------
1 sc 1.2213278741915971e-06 d sc score is increased because of the match both information and the security 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:08:41,495 --> 00:08:46,519': 'd sc score is increased because of the match both information and the security'}
--------------------------------------------------
15 dand 1.2213278741915971e-06 go to the next step entry that d and  so weve updated the score for dand again we add  to d so d goes from  to  
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:08:53,574 --> 00:08:59,350': 'dand again we add to d so d goes from to'}
--------------------------------------------------
5 equated 1.2213278741915971e-06 since we have not yet equated a score accumulator d to d at this point we allocate one so those scores on the last row are the final scores for these documents 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:09:01,930 --> 00:09:06,552': 'since we have not yet equated a score accumulator d to d'}
--------------------------------------------------
32 diplomacy 1.2213278741915971e-06 a rare term will match fewer documents and then the score confusion will be higher because the idf value will be higher and and then it allows us to attach the most diplomacy documents first 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:10:39,680 --> 00:10:45,250': 'then it allows us to attach the most diplomacy documents first'}
--------------------------------------------------
3 pruning 1.2213278741915971e-06 so it helps pruning some non promising ones if we dont need so many documents to be returned to the user 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:10:45,250 --> 00:10:48,830': 'so it helps pruning some non promising ones if we dont need so'}
--------------------------------------------------
6 incorporated 2.2677482445081393e-06 so they can inaudible when we incorporated a one way process each query term 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:10:59,850 --> 00:11:03,560': 'so they can inaudible when we incorporated a one way process each'}
--------------------------------------------------
2 perhapsidf 1.2213278741915971e-06 or maybe perhapsidf value has already been precomputed when we index the document 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:11:09,990 --> 00:11:16,940': 'or maybe perhapsidf value has already been precomputed when we index the document'}
--------------------------------------------------
30 cov 1.2213278741915971e-06 so this is the basic idea of using inverted index for faster search and works well for all kinds of formulas that are of the general form and this generally cov the general form covers actually most state of the art retrieval functions 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:11:46,410 --> 00:11:51,239': 'cov the general form covers actually most state of the art retrieval functions'}
--------------------------------------------------
4 tricks 2.4426557483831942e-06 so there are some tricks to further improve the efficiency some general mac tech techniques include caching 
{'5 - 1 - 4.1 Web Search- Introduction & Web Crawler (00-11-05).srt : 00:03:10,520 --> 00:03:15,410': 'that it not easy to spam the search engine with particular tricks', '3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:11:53,750 --> 00:11:58,850': 'so there are some tricks to further improve the efficiency some general mac'}
--------------------------------------------------
12 mac 2.2677482445081393e-06 so there are some tricks to further improve the efficiency some general mac tech techniques include caching 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:11:53,750 --> 00:11:58,850': 'so there are some tricks to further improve the efficiency some general mac'}
--------------------------------------------------
16 inin 1.7445380593498679e-06 we only want to return high quality subset of documents that likely ranked on the top inin for that purpose we can then prune the accumulators 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:12:47,510 --> 00:12:51,850': 'on the top inin for that purpose we can then prune the accumulators'}
--------------------------------------------------
23 prune 1.2213278741915971e-06 we only want to return high quality subset of documents that likely ranked on the top inin for that purpose we can then prune the accumulators 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:12:47,510 --> 00:12:51,850': 'on the top inin for that purpose we can then prune the accumulators'}
--------------------------------------------------
6 webscale 1.7445380593498679e-06 and to scale up to the webscale we need to special to have the special techniques to do parallel processing and to distribute the storage of files on multiple machines 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:13:12,000 --> 00:13:15,082': 'and to scale up to the webscale we need to special'}
--------------------------------------------------
22 distribute 1.2213278741915971e-06 and to scale up to the webscale we need to special to have the special techniques to do parallel processing and to distribute the storage of files on multiple machines 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:13:18,499 --> 00:13:21,792': 'to distribute the storage of files on multiple machines'}
--------------------------------------------------
25 incremented 1.2213278741915971e-06 you can use it to build a search engine very quickly the downside is that it not that easy to extend it and the algorithms incremented there are not the most advanced algorithms 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:13:55,678 --> 00:14:00,442': 'the algorithms incremented there are not the most advanced algorithms'}
--------------------------------------------------
0 lemur 2.4426557483831942e-06 lemur or indri is another toolkit that that does not have such a nice support application as lucene 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:14:00,442 --> 00:14:03,745': 'lemur or indri is another toolkit that ', '3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:14:25,350 --> 00:14:32,120': 'so that maybe in between lemur or lucene or'}
--------------------------------------------------
2 indri 1.2213278741915971e-06 lemur or indri is another toolkit that that does not have such a nice support application as lucene 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:14:00,442 --> 00:14:03,745': 'lemur or indri is another toolkit that '}
--------------------------------------------------
0 terrier 2.006143151929004e-06 terrier is yet another toolkit that also has good support for quotation capability and some advanced algorithms 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:14:16,410 --> 00:14:21,230': 'terrier is yet another toolkit that also has good support for'}
--------------------------------------------------
5 lemur 2.4426557483831942e-06 so that maybe in between lemur or lucene or maybe rather combining the strands of both so that also useful toolkit 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:14:00,442 --> 00:14:03,745': 'lemur or indri is another toolkit that ', '3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:14:25,350 --> 00:14:32,120': 'so that maybe in between lemur or lucene or'}
--------------------------------------------------
6 touching 2.4426557483831942e-06 and we exploit zipf law avoid touching many documents that dont match any query term 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:03:15,650 --> 00:03:21,470': 'may not be exactly about sports or only marginally touching sports', '3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:15:59,960 --> 00:16:03,870': 'and we exploit zipf law avoid touching many documents'}
--------------------------------------------------
5 mm 2.4426557483831942e-06 so these basic techniques have mm have great potential for further scanning output using distribution to withstand parallel processing and the caching 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:16:13,390 --> 00:16:18,490': 'so these basic techniques have mm have great potential for further scanning', '4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt : 00:16:22,930 --> 00:16:26,790': 'picks up mm some related words to the query'}
--------------------------------------------------
16 withstand 1.2213278741915971e-06 so these basic techniques have mm have great potential for further scanning output using distribution to withstand parallel processing and the caching 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:16:18,490 --> 00:16:23,570': 'output using distribution to withstand parallel processing and the caching'}
--------------------------------------------------
9 scare 1.2213278741915971e-06 the first one is a classic textbook on the scare the efficiency of inverted index and the compression techniques and how to in general build a efficient search engine in terms of the space overhead and speed 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:16:31,050 --> 00:16:38,928': 'the first one is a classic textbook on the scare the efficiency of inverted index and'}
--------------------------------------------------
7 microfunction 2.4426557483831942e-06 and then well be thinking about the microfunction or write down the microfunction to capture more formally how likely a data point will be obtained from this model 
{'3 - 6 - 2.6 Probabilistic Topic Models- Mining One Topic (00-12-21).srt : 00:01:48,770 --> 00:01:54,210': 'and then well be thinking about the microfunction or', '3 - 6 - 2.6 Probabilistic Topic Models- Mining One Topic (00-12-21).srt : 00:01:54,210 --> 00:02:00,480': 'write down the microfunction to capture more formally how likely'}
--------------------------------------------------
12 microfunction 2.4426557483831942e-06 and then well be thinking about the microfunction or write down the microfunction to capture more formally how likely a data point will be obtained from this model 
{'3 - 6 - 2.6 Probabilistic Topic Models- Mining One Topic (00-12-21).srt : 00:01:48,770 --> 00:01:54,210': 'and then well be thinking about the microfunction or', '3 - 6 - 2.6 Probabilistic Topic Models- Mining One Topic (00-12-21).srt : 00:01:54,210 --> 00:02:00,480': 'write down the microfunction to capture more formally how likely'}
--------------------------------------------------
3 argue 1.7445380593498679e-06 and then we argue our interest in estimating those parameters for example by maximizing the likelihood which will lead to maximum likelihood estimated 
{'3 - 6 - 2.6 Probabilistic Topic Models- Mining One Topic (00-12-21).srt : 00:02:10,370 --> 00:02:15,780': 'and then we argue our interest in estimating those parameters for example'}
--------------------------------------------------
6 rewritten 2.4426557483831942e-06 so in this line we have rewritten the formula into a product over all the unique words in the vocabulary w sub  through w sub m now this is different from the previous line 
{'3 - 6 - 2.6 Probabilistic Topic Models- Mining One Topic (00-12-21).srt : 00:03:52,580 --> 00:03:58,550': 'so in this line we have rewritten the formula into a product', '4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:01:18,530 --> 00:01:21,681': 'this can be then rewritten as this '}
--------------------------------------------------
3 macroed 1.2213278741915971e-06 and this is macroed by the logarithm of a probability 
{'3 - 6 - 2.6 Probabilistic Topic Models- Mining One Topic (00-12-21).srt : 00:06:48,105 --> 00:06:54,980': 'and this is macroed by the logarithm of a probability'}
--------------------------------------------------
12 approace 1.2213278741915971e-06 so one way to solve the problem is to use lagrange multiplier approace 
{'3 - 6 - 2.6 Probabilistic Topic Models- Mining One Topic (00-12-21).srt : 00:07:18,621 --> 00:07:23,234': 'so one way to solve the problem is to use lagrange multiplier approace'}
--------------------------------------------------
18 unconstrained 1.7445380593498679e-06 now the idea of this approach is just to turn the constraint optimization into in some sense an unconstrained optimizing problem 
{'3 - 6 - 2.6 Probabilistic Topic Models- Mining One Topic (00-12-21).srt : 00:08:10,432 --> 00:08:14,800': 'in some sense an unconstrained optimizing problem'}
--------------------------------------------------
28 ns 2.4426557483831942e-06 and if you look at this formula it turns out that it actually very intuitive because this is just the normalized count of these words by the document ns which is also a sum of all the counts of words in the document 
{'3 - 6 - 2.6 Probabilistic Topic Models- Mining One Topic (00-12-21).srt : 00:09:37,280 --> 00:09:43,089': 'because this is just the normalized count of these words by the document ns', '4 - 12 - 3.12 Text Categorization- Evaluation Part 1 (00-14-12).srt : 00:04:46,360 --> 00:04:53,708': 'so well see all combinations of this ns yes and nos minus and pluses'}
--------------------------------------------------
4 mess 1.2213278741915971e-06 so after all this mess after all we have just obtained something that very intuitive and this will be just our intuition where we want to maximize the data by assigning as much probability mass as possible to all the observed the words here 
{'3 - 6 - 2.6 Probabilistic Topic Models- Mining One Topic (00-12-21).srt : 00:09:47,751 --> 00:09:52,157': 'so after all this mess after all '}
--------------------------------------------------
24 closed 2.4426557483831942e-06 in general though when the likelihood function is very complicated were not going to be able to solve the optimization problem by having a closed form formula 
{'3 - 6 - 2.6 Probabilistic Topic Models- Mining One Topic (00-12-21).srt : 00:10:46,303 --> 00:10:50,919': 'going to be able to solve the optimization problem by having a closed form formula', '4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt : 00:12:49,240 --> 00:12:53,560': 'basically we can assume a closed neighbor would have more say'}
--------------------------------------------------
26 extraneously 1.2213278741915971e-06 and then in the end you also see there is more probability of words that are not really related to the topic but they might be extraneously mentioned in the document 
{'3 - 6 - 2.6 Probabilistic Topic Models- Mining One Topic (00-12-21).srt : 00:11:40,017 --> 00:11:44,320': 'they might be extraneously mentioned in the document'}
--------------------------------------------------
18 relearned 1.2213278741915971e-06 as we have discussed before in the case of feedback the task of a text retrieval system is relearned from examples to improve retrieval accuracy 
{'4 - 9 - 3.7 Feedback in Vector Space Model- Rocchio (00-12-05).srt : 00:00:29,730 --> 00:00:34,910': 'a text retrieval system is relearned from examples to improve retrieval accuracy'}
--------------------------------------------------
8 escaped 1.2213278741915971e-06 they can also be the documents that are escaped by users 
{'4 - 9 - 3.7 Feedback in Vector Space Model- Rocchio (00-12-05).srt : 00:00:48,790 --> 00:00:52,970': 'they can also be the documents that are escaped by users'}
--------------------------------------------------
13 twodimensional 2.2677482445081393e-06 so the idea is quite simple we illustrate this idea by using a twodimensional display of all the documents in the collection and also the query vector 
{'4 - 9 - 3.7 Feedback in Vector Space Model- Rocchio (00-12-05).srt : 00:01:47,490 --> 00:01:53,060': 'so the idea is quite simple we illustrate this idea by using a twodimensional'}
--------------------------------------------------
3 minuses 1.2213278741915971e-06 and then these minuses are negative documents like this 
{'4 - 9 - 3.7 Feedback in Vector Space Model- Rocchio (00-12-05).srt : 00:02:27,090 --> 00:02:32,360': 'and then these minuses are negative documents like this'}
--------------------------------------------------
0 algebraically 1.7445380593498679e-06 algebraically it just means we have this formula 
{'4 - 9 - 3.7 Feedback in Vector Space Model- Rocchio (00-12-05).srt : 00:04:13,820 --> 00:04:17,340': 'algebraically it just means we have this formula'}
--------------------------------------------------
2 subtracted 1.7445380593498679e-06 when we subtracted this part we kind of move the query vector away from that centroid so this is the main idea of rocchio feedback 
{'4 - 9 - 3.7 Feedback in Vector Space Model- Rocchio (00-12-05).srt : 00:05:05,760 --> 00:05:12,060': 'when we subtracted this part we kind of move the query vector away from that'}
--------------------------------------------------
8 nested 1.2213278741915971e-06 let also look at the centroid of the nested documents 
{'4 - 9 - 3.7 Feedback in Vector Space Model- Rocchio (00-12-05).srt : 00:07:10,010 --> 00:07:13,498': 'let also look at the centroid of the nested documents'}
--------------------------------------------------
5 outlier 1.2213278741915971e-06 so we have a parameter outlier controlling the original query term weight that  
{'4 - 9 - 3.7 Feedback in Vector Space Model- Rocchio (00-12-05).srt : 00:07:38,880 --> 00:07:45,210': 'so we have a parameter outlier controlling the original query term weight that'}
--------------------------------------------------
0 conduit 1.2213278741915971e-06 conduit by a gamma here and this weight has come from of course the nective centroid here 
{'4 - 9 - 3.7 Feedback in Vector Space Model- Rocchio (00-12-05).srt : 00:08:04,594 --> 00:08:08,100': 'conduit by a gamma here and '}
--------------------------------------------------
14 nective 1.2213278741915971e-06 conduit by a gamma here and this weight has come from of course the nective centroid here 
{'4 - 9 - 3.7 Feedback in Vector Space Model- Rocchio (00-12-05).srt : 00:08:08,100 --> 00:08:12,550': 'this weight has come from of course the nective centroid here'}
--------------------------------------------------
4 concern 1.7445380593498679e-06 this is for efficiency concern 
{'4 - 9 - 3.7 Feedback in Vector Space Model- Rocchio (00-12-05).srt : 00:09:38,690 --> 00:09:39,890': 'this is for efficiency concern '}
--------------------------------------------------
8 distract 1.7445380593498679e-06 one reason is because negative documents tend to distract the query in all directions so when you take the average it doesnt really tell you where exactly it should be moving to 
{'4 - 9 - 3.7 Feedback in Vector Space Model- Rocchio (00-12-05).srt : 00:09:57,320 --> 00:10:02,210': 'tend to distract the query in all directions so when you take'}
--------------------------------------------------
5 sometimesw 1.2213278741915971e-06 so that also means that sometimesw we dont have those negative examples but note that in in some cases in difficult queries where most top random results are negative 
{'4 - 9 - 3.7 Feedback in Vector Space Model- Rocchio (00-12-05).srt : 00:10:13,350 --> 00:10:19,250': 'so that also means that sometimesw we dont have those negative examples but'}
--------------------------------------------------
2 afterwards 1.7445380593498679e-06 negative feedback afterwards is very useful 
{'4 - 9 - 3.7 Feedback in Vector Space Model- Rocchio (00-12-05).srt : 00:10:24,570 --> 00:10:26,390': 'negative feedback afterwards is very useful'}
--------------------------------------------------
10 drifting 2.4426557483831942e-06 so in order to prevent the us from overfitting or drifting 
{'4 - 9 - 3.7 Feedback in Vector Space Model- Rocchio (00-12-05).srt : 00:10:55,850 --> 00:11:01,700': 'so in order to prevent the us from overfitting or drifting', '4 - 9 - 3.7 Feedback in Vector Space Model- Rocchio (00-12-05).srt : 00:11:01,700 --> 00:11:07,040': 'a type of drift prevent type of drifting due to the bias toward the'}
--------------------------------------------------
3 drift 1.2213278741915971e-06 a type of drift prevent type of drifting due to the bias toward the the feedback examples 
{'4 - 9 - 3.7 Feedback in Vector Space Model- Rocchio (00-12-05).srt : 00:11:01,700 --> 00:11:07,040': 'a type of drift prevent type of drifting due to the bias toward the'}
--------------------------------------------------
7 drifting 2.4426557483831942e-06 a type of drift prevent type of drifting due to the bias toward the the feedback examples 
{'4 - 9 - 3.7 Feedback in Vector Space Model- Rocchio (00-12-05).srt : 00:10:55,850 --> 00:11:01,700': 'so in order to prevent the us from overfitting or drifting', '4 - 9 - 3.7 Feedback in Vector Space Model- Rocchio (00-12-05).srt : 00:11:01,700 --> 00:11:07,040': 'a type of drift prevent type of drifting due to the bias toward the'}
--------------------------------------------------
7 awareness 2.4426557483831942e-06 and this is especially true for pseudo awareness feedback 
{'5 - 10 - 4.10 Contextual Text Mining- Mining Casual Topics with Time Series Supervision (00-19-37).srt : 00:05:09,050 --> 00:05:13,150': 'awareness pictured to the topics is that they will discover by prsa or lda', '4 - 9 - 3.7 Feedback in Vector Space Model- Rocchio (00-12-05).srt : 00:11:15,040 --> 00:11:19,660': 'and this is especially true for pseudo awareness feedback'}
--------------------------------------------------
2 ro 1.2213278741915971e-06 and the ro rocchio method is usually robust and effective 
{'4 - 9 - 3.7 Feedback in Vector Space Model- Rocchio (00-12-05).srt : 00:11:45,010 --> 00:11:48,500': 'and the ro rocchio method is usually robust and effective'}
--------------------------------------------------
3 censored 1.7445380593498679e-06 so it a censored data 
{'5 - 11 - 4.6  Recommender Systems- Content-based Filtering - Part 2 (00-10-42).srt : 00:00:50,050 --> 00:00:52,090': 'so it a censored data '}
--------------------------------------------------
16 misses 1.7445380593498679e-06 well we could lower the threshold a little bit and do we just deliver some near misses to the user to see what the user would respond so see how the user will would respond to this extra document 
{'5 - 11 - 4.6  Recommender Systems- Content-based Filtering - Part 2 (00-10-42).srt : 00:02:07,550 --> 00:02:13,250': 'misses to the user to see what the user would respond so'}
--------------------------------------------------
4 dilemma 1.7445380593498679e-06 so this is a dilemma 
{'5 - 11 - 4.6  Recommender Systems- Content-based Filtering - Part 2 (00-10-42).srt : 00:02:51,920 --> 00:02:53,700': 'so this is a dilemma '}
--------------------------------------------------
1 nonelective 1.2213278741915971e-06 still nonelective at least 
{'5 - 11 - 4.6  Recommender Systems- Content-based Filtering - Part 2 (00-10-42).srt : 00:05:37,875 --> 00:05:41,305': 'still nonelective at least '}
--------------------------------------------------
13 incorporation 1.7445380593498679e-06 so you can see the formula of the threshold will be just the incorporation of the zero utility threshold and the optimal between the threshold 
{'5 - 11 - 4.6  Recommender Systems- Content-based Filtering - Part 2 (00-10-42).srt : 00:06:13,200 --> 00:06:16,560': 'so you can see the formula of the threshold will be just the incorporation'}
--------------------------------------------------
1 bigger 1.4829329667707326e-06 becomes bigger than it would actually encourage less exploration 
{'5 - 11 - 4.6  Recommender Systems- Content-based Filtering - Part 2 (00-10-42).srt : 00:07:43,330 --> 00:07:50,820': 'becomes bigger than it would actually encourage less exploration'}
--------------------------------------------------
16 exhausted 1.7445380593498679e-06 and that just means if we have seen few examples were not sure whether we have exhausted the space of interests 
{'5 - 11 - 4.6  Recommender Systems- Content-based Filtering - Part 2 (00-10-42).srt : 00:07:59,123 --> 00:08:04,094': 'were not sure whether we have exhausted the space of interests'}
--------------------------------------------------
3 explorationexploration 1.7445380593498679e-06 and explicitly addresses explorationexploration tradeoff 
{'5 - 11 - 4.6  Recommender Systems- Content-based Filtering - Part 2 (00-10-42).srt : 00:08:43,710 --> 00:08:47,840': 'and explicitly addresses explorationexploration tradeoff'}
--------------------------------------------------
14 safeguard 1.7445380593498679e-06 and it kind of uses a zero in this threshold point as a a safeguard 
{'5 - 11 - 4.6  Recommender Systems- Content-based Filtering - Part 2 (00-10-42).srt : 00:08:47,840 --> 00:08:53,750': 'and it kind of uses a zero in this threshold point as a a safeguard'}
--------------------------------------------------
12 projects 2.2677482445081393e-06 and there are of course calls are more advanced than machine learning projects that have been proposed for solving these problems 
{'5 - 11 - 4.6  Recommender Systems- Content-based Filtering - Part 2 (00-10-42).srt : 00:09:26,105 --> 00:09:30,735': 'projects that have been proposed for solving these problems'}
--------------------------------------------------
6 dexpectations 1.2213278741915971e-06 so its easy because the user dexpectations though in this case the system takes initiative to push the information to the user 
{'5 - 14 - 4.7 Recommender Systems- Collaborative Filtering - Part 3 (00-04-45).srt : 00:00:20,553 --> 00:00:24,825': 'so its easy because the user dexpectations though in this case'}
--------------------------------------------------
9 noisy 2.2677482445081393e-06 so unless you recommend that all the you know noisy items or useless documents if you can recommend that some useful information uses general would appreciate it all right 
{'5 - 14 - 4.7 Recommender Systems- Collaborative Filtering - Part 3 (00-04-45).srt : 00:00:40,640 --> 00:00:45,670': 'noisy items or useless documents if you can recommend that'}
--------------------------------------------------
25 appreciate 1.7445380593498679e-06 so unless you recommend that all the you know noisy items or useless documents if you can recommend that some useful information uses general would appreciate it all right 
{'5 - 14 - 4.7 Recommender Systems- Collaborative Filtering - Part 3 (00-04-45).srt : 00:00:45,670 --> 00:00:49,850': 'some useful information uses general would appreciate it all right'}
--------------------------------------------------
26 decreased 1.2213278741915971e-06 if you wait for a few days well even if you can make accurate recommendation of the most relevant news only two days wouldnt be significantly decreased 
{'5 - 14 - 4.7 Recommender Systems- Collaborative Filtering - Part 3 (00-04-45).srt : 00:01:21,190 --> 00:01:26,690': 'the most relevant news only two days wouldnt be significantly decreased'}
--------------------------------------------------
9 sparseness 1.7445380593498679e-06 another reason why it hard it because of data sparseness 
{'5 - 14 - 4.7 Recommender Systems- Collaborative Filtering - Part 3 (00-04-45).srt : 00:01:28,140 --> 00:01:32,150': 'another reason why it hard it because of data sparseness'}
--------------------------------------------------
12 alleviate 2.2677482445081393e-06 and there are there are different strategies that we will use to alleviate the problem 
{'5 - 14 - 4.7 Recommender Systems- Collaborative Filtering - Part 3 (00-04-45).srt : 00:02:00,940 --> 00:02:04,850': 'there are different strategies that we will use to alleviate the problem'}
--------------------------------------------------
14 usable 2.2677482445081393e-06 so in the future we could anticipate for such a system to be more usable to a user 
{'5 - 14 - 4.7 Recommender Systems- Collaborative Filtering - Part 3 (00-04-45).srt : 00:03:16,600 --> 00:03:22,870': 'so in the future we could anticipate for such a system to be more usable to a user'}
--------------------------------------------------
4 isolated 1.2213278741915971e-06 the items are not isolated 
{'5 - 14 - 4.7 Recommender Systems- Collaborative Filtering - Part 3 (00-04-45).srt : 00:03:45,760 --> 00:03:47,300': 'the items are not isolated '}
--------------------------------------------------
7 handbook 2.2677482445081393e-06 here are some additional readings in the handbook called recommender systems 
{'5 - 14 - 4.7 Recommender Systems- Collaborative Filtering - Part 3 (00-04-45).srt : 00:04:09,650 --> 00:04:17,470': 'here are some additional readings in the handbook called recommender systems'}
--------------------------------------------------
10 substituted 1.7445380593498679e-06 a and b have paradigmatic relation if they can be substituted for each other 
{'2 - 7 - 1.7 Word Association Mining and Analysis (00-15-39).srt : 00:01:07,780 --> 00:01:11,700': 'if they can be substituted for each other '}
--------------------------------------------------
1 monday 1.7445380593498679e-06 similarly monday and tuesday have paradigmatical relation 
{'2 - 7 - 1.7 Word Association Mining and Analysis (00-15-39).srt : 00:01:58,320 --> 00:02:01,990': 'similarly monday and tuesday have paradigmatical relation'}
--------------------------------------------------
37 meaningless 2.4426557483831942e-06 however in general we can not replace cat with sit in a sentence or car with drive in the sentence to still get a valid sentence meaning that if we do that the sentence will become somewhat meaningless 
{'2 - 7 - 1.7 Word Association Mining and Analysis (00-15-39).srt : 00:02:59,590 --> 00:03:03,950': 'meaning that if we do that the sentence will become somewhat meaningless', '4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt : 00:12:53,800 --> 00:12:56,790': 'that are meaningless in the feedback '}
--------------------------------------------------
11 nonphrase 1.2213278741915971e-06 and they can even be more complex phrases than just a nonphrase 
{'2 - 7 - 1.7 Word Association Mining and Analysis (00-15-39).srt : 00:03:37,960 --> 00:03:44,710': 'and they can even be more complex phrases than just a nonphrase'}
--------------------------------------------------
9 cooccurrent 1.2213278741915971e-06 syntagmatical relation on the other hand is related to cooccurrent elements that tend to show up in the same sequence 
{'2 - 7 - 1.7 Word Association Mining and Analysis (00-15-39).srt : 00:04:25,415 --> 00:04:30,210': 'cooccurrent elements that tend to show up in the same sequence'}
--------------------------------------------------
4 complimentary 1.7445380593498679e-06 so these two are complimentary and are basic relations of words 
{'2 - 7 - 1.7 Word Association Mining and Analysis (00-15-39).srt : 00:04:33,150 --> 00:04:38,470': 'so these two are complimentary and are basic relations of words'}
--------------------------------------------------
24 expressions 1.7445380593498679e-06 and if we learn syntagmatic relations then we would be able to know the rules for putting together a larger expression based on component expressions 
{'2 - 7 - 1.7 Word Association Mining and Analysis (00-15-39).srt : 00:05:25,630 --> 00:05:32,400': 'the rules for putting together a larger expression based on component expressions'}
--------------------------------------------------
9 researches 1.2213278741915971e-06 and so those who are interested in information retrieval researches probably all favor sigir papers that something that we make 
{'5 - 12 - 4.7 Recommender Systems- Collaborative Filtering - Part 1 (00-06-20).srt : 00:02:12,929 --> 00:02:17,758': 'and so those who are interested in information retrieval researches probably'}
--------------------------------------------------
4 watched 1.7445380593498679e-06 so some users have watched movies they have rated those movies 
{'5 - 12 - 4.7 Recommender Systems- Collaborative Filtering - Part 1 (00-06-20).srt : 00:04:16,203 --> 00:04:21,521': 'so some users have watched movies they have rated those movies'}
--------------------------------------------------
33 central 1.7445380593498679e-06 so many item many entries have unknown values and what interesting here is we could potentially infer the value of a element in this matrix based on other values and that actually the central question in collaborative filtering 
{'5 - 12 - 4.7 Recommender Systems- Collaborative Filtering - Part 1 (00-06-20).srt : 00:04:49,360 --> 00:04:55,317': 'that actually the central question in collaborative filtering'}
--------------------------------------------------
3 ve 1.2213278741915971e-06 so this is ve very similar to other machine learning problems where we would know the values of the function on some training there that and we hope to predict the the values of this function on some test there 
{'5 - 12 - 4.7 Recommender Systems- Collaborative Filtering - Part 1 (00-06-20).srt : 00:05:18,615 --> 00:05:24,401': 'so this is ve very similar to other machine learning problems'}
--------------------------------------------------
7 dedicated 1.7445380593498679e-06 a reason that there are special conferences dedicated to the problem is a major conference devoted to the problem 
{'5 - 12 - 4.7 Recommender Systems- Collaborative Filtering - Part 1 (00-06-20).srt : 00:05:57,287 --> 00:06:02,970': 'a reason that there are special conferences dedicated to the problem'}
--------------------------------------------------
15 devoted 1.7445380593498679e-06 a reason that there are special conferences dedicated to the problem is a major conference devoted to the problem 
{'5 - 12 - 4.7 Recommender Systems- Collaborative Filtering - Part 1 (00-06-20).srt : 00:06:02,970 --> 00:06:06,390': 'is a major conference devoted to the problem'}
--------------------------------------------------
5 lexile 1.2213278741915971e-06 so this is a code lexile analysis or part of speech tagging 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:01:34,310 --> 00:01:38,290': 'so this is a code lexile analysis or part of speech tagging'}
--------------------------------------------------
8 syntaxing 1.2213278741915971e-06 and we need to pick out the the syntaxing categories of those words 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:01:38,290 --> 00:01:42,230': 'and we need to pick out the the syntaxing categories of those words'}
--------------------------------------------------
6 actor 1.2213278741915971e-06 in order to understand the speech actor of a sentence all right we say something to basically achieve some goal 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:04:08,740 --> 00:04:14,090': 'in order to understand the speech actor of a sentence all right'}
--------------------------------------------------
5 trouble 2.4426557483831942e-06 yet we humans have no trouble with understand that 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:04:46,940 --> 00:04:49,560': 'yet we humans have no trouble with understand that', '3 - 3 - 2.3 Topic Mining and Analysis- Probabilistic Topic Models (00-14-17).srt : 00:02:10,150 --> 00:02:15,430': 'like trouble which might be related to sports in general'}
--------------------------------------------------
1 instantly 1.7445380593498679e-06 we instantly will get everything and there is a reason for that 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:04:49,560 --> 00:04:53,750': 'we instantly will get everything and there is a reason for that'}
--------------------------------------------------
10 brain 1.2213278741915971e-06 that because we have a large knowledge base in our brain and we use common sense knowledge to help interpret the sentence 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:04:53,750 --> 00:04:57,490': 'that because we have a large knowledge base in our brain and'}
--------------------------------------------------
3 incapable 1.2213278741915971e-06 they are still incapable of doing reasoning and uncertainties 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:05:08,430 --> 00:05:12,520': 'they are still incapable of doing reasoning and uncertainties'}
--------------------------------------------------
11 receiver 1.2213278741915971e-06 we also keep a lot of ambiguities because we assume the receiver or the hearer could know how to discern an ambiguous word based on the knowledge or the context 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:05:49,550 --> 00:05:54,140': 'we also keep a lot of ambiguities because we assume the receiver or'}
--------------------------------------------------
14 hearer 1.7445380593498679e-06 we also keep a lot of ambiguities because we assume the receiver or the hearer could know how to discern an ambiguous word based on the knowledge or the context 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:05:54,140 --> 00:05:59,340': 'the hearer could know how to discern an ambiguous word'}
--------------------------------------------------
19 discern 2.2677482445081393e-06 we also keep a lot of ambiguities because we assume the receiver or the hearer could know how to discern an ambiguous word based on the knowledge or the context 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:05:54,140 --> 00:05:59,340': 'the hearer could know how to discern an ambiguous word'}
--------------------------------------------------
4 invent 2.4426557483831942e-06 there no need to invent a different word for different meanings 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:06:02,020 --> 00:06:05,340': 'there no need to invent a different word for different meanings', '5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:10:33,580 --> 00:10:37,320': 'other organisms or to invent new hours in yourself'}
--------------------------------------------------
0 conceded 1.2213278741915971e-06 conceded the wordlevel ambiguities 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:06:26,356 --> 00:06:30,740': 'conceded the wordlevel ambiguities '}
--------------------------------------------------
11 plant 2.4426557483831942e-06 so square root in math sense or the root of a plant 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:06:42,160 --> 00:06:45,120': 'so square root in math sense or the root of a plant', '2 - 3 - 1.3 Natural Language Content Analysis- Part 1 (00-12-48).srt : 00:07:06,190 --> 00:07:10,670': 'like in the square of or can be root of a plant'}
--------------------------------------------------
5 syntatic 1.2213278741915971e-06 so this is example of syntatic ambiguity 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:07:15,760 --> 00:07:20,410': 'right so this is example of syntatic ambiguity'}
--------------------------------------------------
21 disintegrate 2.4426557483831942e-06 now we generally dont have a problem with these ambiguities because we have a lot of background knowledge to help us disintegrate the ambiguity 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:07:49,700 --> 00:07:54,320': 'a lot of background knowledge to help us disintegrate the ambiguity', '3 - 3 - 2.3 Topic Mining and Analysis- Probabilistic Topic Models (00-14-17).srt : 00:05:07,930 --> 00:05:12,210': 'we can disintegrate the sense of word '}
--------------------------------------------------
8 knowl 1.2213278741915971e-06 it also would have to maintain a large knowl knowledge base of odd meanings of words and how they are connected to our common sense knowledge of the word 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:08:30,938 --> 00:08:35,900': 'it also would have to maintain a large knowl knowledge base of odd meanings of'}
--------------------------------------------------
8 organizations 1.2213278741915971e-06 for example recognizing the mentions of people locations organizations et cetera in text 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:10:34,661 --> 00:10:37,120': 'organizations et cetera in text '}
--------------------------------------------------
9 acquired 2.2677482445081393e-06 or this person met that person or this company acquired another company 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:10:47,220 --> 00:10:51,340': 'or this person met that person or this company acquired another company'}
--------------------------------------------------
11 languaging 1.2213278741915971e-06 such relations can be extracted by using the current and natural languaging processing techniques 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:10:54,870 --> 00:10:56,800': 'natural languaging processing techniques '}
--------------------------------------------------
6 disintegration 2.4426557483831942e-06 we can also do word sentence disintegration to some extent 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:11:03,010 --> 00:11:05,660': 'we can also do word sentence disintegration to some extent', '3 - 2 - 2.2 Topic Mining and Analysis- Term as Topic (00-11-31).srt : 00:10:59,200 --> 00:11:02,410': 'finally there is this problem of word sense disintegration'}
--------------------------------------------------
9 reimplementation 1.2213278741915971e-06 that probably also because we dont have complete semantic reimplementation for natural language text 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:12:03,053 --> 00:12:08,427': 'that probably also because we dont have complete semantic reimplementation for'}
--------------------------------------------------
14 restrictions 2.4426557483831942e-06 yet in some domains perhaps in limited domains when you have a lot of restrictions on the world of users you may be to may be able to perform inference to some extent but in general we cannot really do that reliably 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:12:16,540 --> 00:12:21,670': 'restrictions on the world of users you may be to may be able to perform', '5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:01:20,330 --> 00:01:25,550': 'because of the restrictions with domain we also have some advantages'}
--------------------------------------------------
29 grain 1.2213278741915971e-06 this looks like a simple task but think about the example here the two uses of off may have different syntactic categories if you try to make a fine grain distinctions 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:13:01,641 --> 00:13:04,830': 'to make a fine grain distinctions '}
--------------------------------------------------
30 distinctions 1.2213278741915971e-06 this looks like a simple task but think about the example here the two uses of off may have different syntactic categories if you try to make a fine grain distinctions 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:13:01,641 --> 00:13:04,830': 'to make a fine grain distinctions '}
--------------------------------------------------
29 superficial 1.2213278741915971e-06 so as a result we have robust and general natural language processing techniques that can process a lot of text data in a shallow way meaning we only do superficial analysis 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:14:20,963 --> 00:14:25,640': 'data in a shallow way meaning we only do superficial analysis'}
--------------------------------------------------
18 fail 1.7445380593498679e-06 on the other hand the deep understanding techniques tend not to scale up well meaning that they would fail on some unrestricted text 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:14:45,870 --> 00:14:50,830': 'meaning that they would fail on some unrestricted text'}
--------------------------------------------------
25 exposed 1.2213278741915971e-06 but because of it relevance to the topic that we talked about it useful for you to know the background in case you havent been exposed to that 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:15:40,970 --> 00:15:45,410': 'you to know the background in case you havent been exposed to that'}
--------------------------------------------------
14 chances 2.4426557483831942e-06 if you see matching of some of the query words in a text document chances are that that document is about the topic although there are exceptions right 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:17:03,440 --> 00:17:08,720': 'if you see matching of some of the query words in a text document chances', '2 - 9 - 1.9 Doc Length Normalization (00-18-56).srt : 00:02:04,930 --> 00:02:07,220': 'just naturally have better chances to match any query'}
--------------------------------------------------
26 exceptions 1.2213278741915971e-06 if you see matching of some of the query words in a text document chances are that that document is about the topic although there are exceptions right 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:17:08,720 --> 00:17:13,670': 'are that that document is about the topic although there are exceptions right'}
--------------------------------------------------
6 parentheses 1.7445380593498679e-06 of course i put in in parentheses but not all 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:17:35,760 --> 00:17:38,112': 'of course i put in in parentheses but not all'}
--------------------------------------------------
3 coffee 2.4426557483831942e-06 it could mean coffee or it could mean program language 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:18:11,060 --> 00:18:13,915': 'it could mean coffee or it could mean program language', '2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:18:43,710 --> 00:18:47,722': 'if java occurs in the document in a way that means coffee'}
--------------------------------------------------
12 referring 1.2213278741915971e-06 and that context can help us naturally prefer documents where java is referring to program language because those documents would probably match applet as well 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:18:35,040 --> 00:18:39,680': 'prefer documents where java is referring to program language'}
--------------------------------------------------
11 coffee 2.4426557483831942e-06 if java occurs in the document in a way that means coffee then you would never match applet or with very small probability 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:18:11,060 --> 00:18:13,915': 'it could mean coffee or it could mean program language', '2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:18:43,710 --> 00:18:47,722': 'if java occurs in the document in a way that means coffee'}
--------------------------------------------------
10 difficulties 1.7445380593498679e-06 so those techniques also helped us bypass some of the difficulties in natural language processing 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:19:35,350 --> 00:19:38,890': 'bypass some of the difficulties in natural language processing'}
--------------------------------------------------
3 launched 2.4426557483831942e-06 google has recently launched a knowledge graph 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:15:11,459 --> 00:15:16,722': 'government and was launched in or around that time', '2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:19:55,320 --> 00:19:58,910': 'google has recently launched a knowledge graph'}
--------------------------------------------------
12 dominant 1.2213278741915971e-06 and finally we also explained why bag of words representation remains the dominant representation used in modern search engines even though deeper nlp would be needed for future search engines 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:20:34,530 --> 00:20:39,420': 'remains the dominant representation used in modern search engines even though'}
--------------------------------------------------
5 replenishing 1.2213278741915971e-06 so the back of words replenishing is generally the main method used in modern search engines and it often sufficient for most of the search tasks 
{'5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:00:38,642 --> 00:00:43,488': 'so the back of words replenishing is generally'}
--------------------------------------------------
4 compass 1.2213278741915971e-06 but obviously for more compass search tasks then we need a deeper measurement processing techniques 
{'5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:00:52,373 --> 00:00:56,406': 'but obviously for more compass search tasks'}
--------------------------------------------------
12 measurement 1.4829329667707326e-06 but obviously for more compass search tasks then we need a deeper measurement processing techniques 
{'5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:00:56,406 --> 00:01:01,258': 'then we need a deeper measurement processing techniques'}
--------------------------------------------------
19 plural 1.7445380593498679e-06 and we then talked about a highlevel strategies for text access and we talked about push versus pull in plural 
{'5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:01:05,317 --> 00:01:09,837': 'text access and we talked about push versus pull in plural'}
--------------------------------------------------
5 leverageable 1.2213278741915971e-06 we also later talked about leverageable learning approach and that probabilistic model 
{'5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:01:47,528 --> 00:01:52,750': 'we also later talked about leverageable learning approach and'}
--------------------------------------------------
4 prepare 1.2213278741915971e-06 so that we can prepare the system to answer a query quickly and we talked about how to to fast research by using the inverted index and we then talked about how to evaluate the text retrieval system mainly introduced the cranfield evaluation methodology 
{'5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:02:33,274 --> 00:02:38,535': 'so that we can prepare the system to answer a query quickly and'}
--------------------------------------------------
2 accumulative 2.4426557483831942e-06 normalized discounted accumulative gain and also precision and record the two basic measures 
{'5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:03:14,832 --> 00:03:18,994': 'normalized discounted accumulative gain and also precision and', '3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).srt : 00:02:03,380 --> 00:02:08,860': 'we are infusing is called ntcg normalizer discount of accumulative gain'}
--------------------------------------------------
5 rock 1.2213278741915971e-06 and we talked about the rock you in the vector space model and the mixture model in the language modeling approach 
{'5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:03:24,967 --> 00:03:28,573': 'and we talked about the rock you in the vector space model and'}
--------------------------------------------------
15 pixels 2.4426557483831942e-06 feedback is very important technique especially considering the opportunity of learning from a lot of pixels on the web 
{'5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:03:36,837 --> 00:03:41,861': 'the opportunity of learning from a lot of pixels on the web', '3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:01:37,779 --> 00:01:43,140': 'or implicit feedback such as pixels so the user doesnt have to any anything extra'}
--------------------------------------------------
32 interacting 2.4426557483831942e-06 and here we talk about the how to use parallel indexing to resolve the scalability issue in indexing we introduce a mapreduce and then we talked about the how to using information interacting pull search 
{'5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:03:53,764 --> 00:03:59,055': 'then we talked about the how to using information interacting pull search', '5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:09:30,678 --> 00:09:36,564': 'i look forward to interacting with you at a future activity'}
--------------------------------------------------
5 interactions 2.2677482445081393e-06 we talked about some major interactions that we might assume in the future in improving the current generation of search engines 
{'5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:04:39,590 --> 00:04:44,473': 'we talked about some major interactions that we might assume'}
--------------------------------------------------
4 piece 1.2213278741915971e-06 now an obvious missing piece in this picture is the user you can see 
{'5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:05:06,402 --> 00:05:11,449': 'now an obvious missing piece in this '}
--------------------------------------------------
21 tutorials 1.7445380593498679e-06 and a main source is synthesis digital library where you can see a lot of short textbook or textbooks or long tutorials 
{'5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:06:26,064 --> 00:06:29,949': 'textbooks or long tutorials '}
--------------------------------------------------
5 journals 1.4829329667707326e-06 there are also some major journals and conferences listed over here that tend to have a lot of research papers related to the topic of this course 
{'5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:06:49,695 --> 00:06:54,980': 'there are also some major journals and conferences listed over here that'}
--------------------------------------------------
11 kits 2.4426557483831942e-06 and finally for more information about resources including readings and tool kits etc 
{'5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:07:01,390 --> 00:07:07,182': 'more information about resources including readings and tool kits etc', '3 - 16 - 2.16 Latent Dirichlet Allocation (LDA)- Part 2 (00-12-03).srt : 00:09:24,220 --> 00:09:29,100': 'see them when you use different tool kits for lda or when you read papers about'}
--------------------------------------------------
13 providence 2.4426557483831942e-06 these techniques are also essential in any text mining system to help provide providence and to help users interpret the inner patterns that the user would find through text data mining 
{'5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:08:13,727 --> 00:08:18,338': 'provide providence and to help users interpret the inner', '5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:15:47,901 --> 00:15:51,521': 'and this has to do with knowledge providence'}
--------------------------------------------------
20 inner 1.7445380593498679e-06 these techniques are also essential in any text mining system to help provide providence and to help users interpret the inner patterns that the user would find through text data mining 
{'5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:08:13,727 --> 00:08:18,338': 'provide providence and to help users interpret the inner'}
--------------------------------------------------
10 ana 1.2213278741915971e-06 so the text mining course or rather text mining and ana analytics course will be deal dealing with what to do once the user has found the information 
{'5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:08:29,518 --> 00:08:33,795': 'so the text mining course or rather text mining and ana'}
--------------------------------------------------
17 interacting 2.4426557483831942e-06 i hope you have found this course to be useful to you and i look forward to interacting with you at a future activity 
{'5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:03:53,764 --> 00:03:59,055': 'then we talked about the how to using information interacting pull search', '5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:09:30,678 --> 00:09:36,564': 'i look forward to interacting with you at a future activity'}
--------------------------------------------------
13 prefaces 1.2213278741915971e-06 and that makes it possible to mine text data to discover those latent prefaces of people which could be very useful to build an intelligent system to help people 
{'1 - 2 - Course Introduction (00-10-45).srt : 00:00:38,260 --> 00:00:41,730': 'text data to discover those latent prefaces of people'}
--------------------------------------------------
10 digesting 2.2677482445081393e-06 so it very high quality content yet we have difficulty digesting all the content 
{'1 - 2 - Course Introduction (00-10-45).srt : 00:00:53,490 --> 00:01:00,200': 'so it very high quality content yet we have difficulty digesting all the content'}
--------------------------------------------------
3 ipad 1.2213278741915971e-06 web search engines ipad all of you are using google or bing or another web search engine all the time 
{'1 - 2 - Course Introduction (00-10-45).srt : 00:01:45,720 --> 00:01:49,240': 'web search engines ipad all of you are using google or bing or'}
--------------------------------------------------
4 live 1.7445380593498679e-06 and we also have live research assistants 
{'1 - 2 - Course Introduction (00-10-45).srt : 00:01:51,470 --> 00:01:54,000': 'and we also have live research assistants '}
--------------------------------------------------
6 assistants 1.4829329667707326e-06 and we also have live research assistants 
{'1 - 2 - Course Introduction (00-10-45).srt : 00:01:51,470 --> 00:01:54,000': 'and we also have live research assistants '}
--------------------------------------------------
3 recommenders 1.2213278741915971e-06 literature the movie recommenders 
{'1 - 2 - Course Introduction (00-10-45).srt : 00:02:19,050 --> 00:02:20,970': 'literature the movie recommenders '}
--------------------------------------------------
17 inbox 2.4426557483831942e-06 for example email filter spam email filter this is actually to filter out the spams from your inbox all right 
{'1 - 2 - Course Introduction (00-10-45).srt : 00:02:27,800 --> 00:02:33,600': 'this is actually to filter out the spams from your inbox all right', '4 - 12 - 3.12 Text Categorization- Evaluation Part 1 (00-14-12).srt : 00:06:54,930 --> 00:06:59,930': 'it okay to occasionally let a spam email to come into your inbox'}
--------------------------------------------------
24 discard 1.2213278741915971e-06 but in nature these are similar systems in that they have to make a binary decision regarding whether to retain a particular document or discard it 
{'1 - 2 - Course Introduction (00-10-45).srt : 00:02:38,590 --> 00:02:42,700': 'regarding whether to retain a particular document or discard it'}
--------------------------------------------------
10 sorter 1.2213278741915971e-06 so for example in handling emails you might prefer automatic sorter that would automatically sort incoming emails into a proper folders that you created 
{'1 - 2 - Course Introduction (00-10-45).srt : 00:02:51,500 --> 00:02:56,180': 'you might prefer automatic sorter that would automatically'}
--------------------------------------------------
1 agencies 2.4426557483831942e-06 news agencies might be interested in categorizing news articles into all kinds of subject categories 
{'1 - 2 - Course Introduction (00-10-45).srt : 00:03:07,278 --> 00:03:11,550': 'news agencies might be interested in ', '4 - 7 - 3.7 Text Categorization- Motivation (00-14-37).srt : 00:05:25,110 --> 00:05:30,009': 'news agencies would like to assign predefined'}
--------------------------------------------------
1 oh 2.006143151929004e-06 and oh you can say mine text data 
{'1 - 2 - Course Introduction (00-10-45).srt : 00:03:22,180 --> 00:03:26,470': 'and oh you can say mine text data '}
--------------------------------------------------
23 pairings 1.2213278741915971e-06 and this step is called text mining where we use a number of techniques to mine the data to get useful knowledge or pairings 
{'1 - 2 - Course Introduction (00-10-45).srt : 00:05:54,300 --> 00:05:57,920': 'mine the data to get useful knowledge or pairings'}
--------------------------------------------------
6 hands 1.2213278741915971e-06 we also hope to provide a hands on experience on multiple aspects 
{'1 - 2 - Course Introduction (00-10-45).srt : 00:06:34,080 --> 00:06:38,300': 'we also hope to provide a hands on experience on multiple aspects'}
--------------------------------------------------
15 coauthoring 1.2213278741915971e-06 and this in time order and that also includes a book that and i are coauthoring now and we make some draft chapters available on this website 
{'1 - 2 - Course Introduction (00-10-45).srt : 00:08:16,223 --> 00:08:19,147': 'and i are coauthoring now and '}
--------------------------------------------------
8 highway 1.2213278741915971e-06 so in the example he turned off the highway verses he turned off the fan and the two offs actually have somewhat a differentness in their active categories and also its very difficult to get a complete the parsing correct 
{'2 - 4 - 1.4 Natural Language Content Analysis- Part 2 (00-04-25).srt : 00:00:21,970 --> 00:00:27,938': 'so in the example he turned off the highway verses he turned off the fan and'}
--------------------------------------------------
9 verses 1.2213278741915971e-06 so in the example he turned off the highway verses he turned off the fan and the two offs actually have somewhat a differentness in their active categories and also its very difficult to get a complete the parsing correct 
{'2 - 4 - 1.4 Natural Language Content Analysis- Part 2 (00-04-25).srt : 00:00:21,970 --> 00:00:27,938': 'so in the example he turned off the highway verses he turned off the fan and'}
--------------------------------------------------
14 fan 2.006143151929004e-06 so in the example he turned off the highway verses he turned off the fan and the two offs actually have somewhat a differentness in their active categories and also its very difficult to get a complete the parsing correct 
{'2 - 4 - 1.4 Natural Language Content Analysis- Part 2 (00-04-25).srt : 00:00:21,970 --> 00:00:27,938': 'so in the example he turned off the highway verses he turned off the fan and'}
--------------------------------------------------
18 offs 2.4426557483831942e-06 so in the example he turned off the highway verses he turned off the fan and the two offs actually have somewhat a differentness in their active categories and also its very difficult to get a complete the parsing correct 
{'2 - 4 - 1.4 Natural Language Content Analysis- Part 2 (00-04-25).srt : 00:00:27,938 --> 00:00:33,342': 'the two offs actually have somewhat a differentness in their active', '4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:08:49,240 --> 00:08:52,440': 'sometimes there are trade offs between multiple aspects like precision and'}
--------------------------------------------------
23 differentness 1.2213278741915971e-06 so in the example he turned off the highway verses he turned off the fan and the two offs actually have somewhat a differentness in their active categories and also its very difficult to get a complete the parsing correct 
{'2 - 4 - 1.4 Natural Language Content Analysis- Part 2 (00-04-25).srt : 00:00:27,938 --> 00:00:33,342': 'the two offs actually have somewhat a differentness in their active'}
--------------------------------------------------
8 summarized 1.7445380593498679e-06 so the state of the off can be summarized as follows 
{'2 - 4 - 1.4 Natural Language Content Analysis- Part 2 (00-04-25).srt : 00:01:01,493 --> 00:01:04,737': 'so the state of the off can be summarized as follows'}
--------------------------------------------------
20 backbone 2.4426557483831942e-06 so in practical applications we generally combine the two kinds of techniques with the general statistical and methods as a backbone as the basis 
{'2 - 4 - 1.4 Natural Language Content Analysis- Part 2 (00-04-25).srt : 00:02:21,880 --> 00:02:29,150': 'with the general statistical and methods as a backbone as the basis', '5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:11:41,120 --> 00:11:45,090': 'and these techniques are now the backbone techniques for'}
--------------------------------------------------
10 consuming 2.4426557483831942e-06 and so this is to minimize a human effort in consuming text data in some sense 
{'2 - 1 - 1.1 Overview Text Mining and Analytics- Part 1 (00-11-43).srt : 00:02:59,930 --> 00:03:05,030': 'and so this is to minimize a human effort in consuming text data in some sense', '5 - 6 - 4.6 Text-Based Prediction (00-12-08).srt : 00:03:43,350 --> 00:03:46,890': 'humans are the best in consuming or interpreting text data'}
--------------------------------------------------
21 shocking 1.2213278741915971e-06 for example we might be able to determine which product is more appealing to us or a better choice for a shocking decision 
{'2 - 1 - 1.1 Overview Text Mining and Analytics- Part 1 (00-11-43).srt : 00:03:31,510 --> 00:03:36,270': 'or a better choice for a shocking decision'}
--------------------------------------------------
6 supplies 1.2213278741915971e-06 so in this case text mining supplies knowledge for optimal decision making 
{'2 - 1 - 1.1 Overview Text Mining and Analytics- Part 1 (00-11-43).srt : 00:03:49,550 --> 00:03:55,131': 'so in this case text mining supplies knowledge for optimal decision making'}
--------------------------------------------------
6 preprocessor 1.2213278741915971e-06 first text retrieval can be a preprocessor for text mining 
{'2 - 1 - 1.1 Overview Text Mining and Analytics- Part 1 (00-11-43).srt : 00:05:18,278 --> 00:05:23,200': 'first text retrieval can be a preprocessor for text mining'}
--------------------------------------------------
11 turning 2.2677482445081393e-06 and this roughly corresponds to the interpretation of text mining as turning text data into actionable knowledge 
{'2 - 1 - 1.1 Overview Text Mining and Analytics- Part 1 (00-11-43).srt : 00:05:50,976 --> 00:05:56,350': 'mining as turning text data into actionable knowledge'}
--------------------------------------------------
9 longitude 1.2213278741915971e-06 the location specification for example in the form of longitude value and latitude value 
{'2 - 1 - 1.1 Overview Text Mining and Analytics- Part 1 (00-11-43).srt : 00:07:53,740 --> 00:07:57,140': 'example in the form of longitude value and latitude value'}
--------------------------------------------------
12 latitude 2.4426557483831942e-06 the location specification for example in the form of longitude value and latitude value 
{'4 - 1 - 3.1 Probabilistic Retrieval Model- Basic Idea (00-12-44).srt : 00:02:02,881 --> 00:02:06,048': 'which has latitude the pl function ', '2 - 1 - 1.1 Overview Text Mining and Analytics- Part 1 (00-11-43).srt : 00:07:53,740 --> 00:07:57,140': 'example in the form of longitude value and latitude value'}
--------------------------------------------------
2 sends 1.2213278741915971e-06 a network sends over the monitor network traffic or activities in the network and are reported 
{'2 - 1 - 1.1 Overview Text Mining and Analytics- Part 1 (00-11-43).srt : 00:07:57,140 --> 00:08:02,580': 'a network sends over the monitor network traffic'}
--------------------------------------------------
7 traffic 2.4426557483831942e-06 a network sends over the monitor network traffic or activities in the network and are reported 
{'2 - 1 - 1.1 Overview Text Mining and Analytics- Part 1 (00-11-43).srt : 00:07:57,140 --> 00:08:02,580': 'a network sends over the monitor network traffic', '3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:20:47,495 --> 00:20:52,601': 'and then so we we can reduce the amount of traffic and io'}
--------------------------------------------------
9 activities 2.4426557483831942e-06 a network sends over the monitor network traffic or activities in the network and are reported 
{'4 - 8 - 3.6 Feedback in Text Retrieval (00-06-49).srt : 00:05:39,545 --> 00:05:45,086': 'you know think about google and bing and they can collect a lot of user activities', '2 - 1 - 1.1 Overview Text Mining and Analytics- Part 1 (00-11-43).srt : 00:08:02,580 --> 00:08:04,873': 'or activities in the network and are reported'}
--------------------------------------------------
7 multimedia 1.7445380593498679e-06 numerical data categorical or relational data or multimedia data like video or speech 
{'2 - 1 - 1.1 Overview Text Mining and Analytics- Part 1 (00-11-43).srt : 00:09:30,830 --> 00:09:33,800': 'or multimedia data like video or speech '}
--------------------------------------------------
21 interactively 1.2213278741915971e-06 they try to model the conditional distribution of labels given the data directly rather than using bayes rule to compute that interactively as we have seen in naive bayes 
{'4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt : 00:01:36,990 --> 00:01:41,550': 'to compute that interactively as we have seen in naive bayes'}
--------------------------------------------------
4 advantageous 1.7445380593498679e-06 so this is potentially advantageous for doing text categorization 
{'4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt : 00:03:25,830 --> 00:03:31,410': 'so this is potentially advantageous for doing text categorization'}
--------------------------------------------------
9 odds 1.2213278741915971e-06 and this is very closely related to the log odds that i introduced in the naive bayes or log of probability ratio of the two categories that you have seen on the previous slide 
{'4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt : 00:03:46,610 --> 00:03:51,290': 'odds that i introduced in the naive bayes or log of probability ratio'}
--------------------------------------------------
18 moderate 1.2213278741915971e-06 the conditional likelihood here is basically to model why given observe the x so it not like a moderate x but rather were going to model this 
{'4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt : 00:07:41,829 --> 00:07:46,382': 'so it not like a moderate x but '}
--------------------------------------------------
3 occasion 1.2213278741915971e-06 so as another occasion when you compute the maximum likelihood data basically youll find a beta value a set of beta values that would maximize this conditional likelihood 
{'4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt : 00:10:00,957 --> 00:10:04,970': 'so as another occasion when you compute the maximum likelihood data'}
--------------------------------------------------
0 newton 1.4829329667707326e-06 newton method is a popular way to solve this problem there are other methods as well 
{'4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt : 00:10:20,130 --> 00:10:22,870': 'newton method is a popular way to solve this problem'}
--------------------------------------------------
10 objector 1.2213278741915971e-06 basically this is to find the neighbors of this text objector in the training data set 
{'4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt : 00:11:59,290 --> 00:12:03,981': 'basically this is to find the neighbors of this text objector in'}
--------------------------------------------------
5 closed 2.4426557483831942e-06 basically we can assume a closed neighbor would have more say about the category of the subject 
{'3 - 6 - 2.6 Probabilistic Topic Models- Mining One Topic (00-12-21).srt : 00:10:46,303 --> 00:10:50,919': 'going to be able to solve the optimization problem by having a closed form formula', '4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt : 00:12:49,240 --> 00:12:53,560': 'basically we can assume a closed neighbor would have more say'}
--------------------------------------------------
2 colored 1.2213278741915971e-06 and ive colored them differently and to show just different categories 
{'4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt : 00:14:32,360 --> 00:14:38,612': 'and ive colored them differently and to show just different categories'}
--------------------------------------------------
12 filled 2.2677482445081393e-06 now in this case let assume the closest neighbor is the box filled with diamonds 
{'4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt : 00:14:53,100 --> 00:14:59,264': 'now in this case let assume the closest neighbor is the box filled with diamonds'}
--------------------------------------------------
17 pink 1.2213278741915971e-06 in this case were going to include a lot of other solid field boxes in red or pink right 
{'4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt : 00:15:31,090 --> 00:15:32,970': 'pink right '}
--------------------------------------------------
34 locally 1.2213278741915971e-06 so the key assumption that we made in this approach is that the distribution of the label given the document probability a category given for example probability of theta i given document d is locally smooth 
{'4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt : 00:17:44,027 --> 00:17:51,620': 'example probability of theta i given document d is locally smooth'}
--------------------------------------------------
3 largely 2.2677482445081393e-06 because neighborhood is largely determined by our similarity function 
{'4 - 10 - 3.10 Text Categorization- Discriminative Classifier Part 1 (00-20-34).srt : 00:18:39,560 --> 00:18:43,610': 'because neighborhood is largely determined by our similarity function'}
--------------------------------------------------
12 instant 2.4426557483831942e-06 in this lecture were going to talk about how to improve the instant changing of the vector space model 
{'2 - 7 - 1.7 Vector Space Model- Improved Instantiation (00-16-52).srt : 00:00:07,738 --> 00:00:12,706': 'in this lecture were going to talk about how to improve the instant changing of', '2 - 7 - 1.7 Vector Space Model- Improved Instantiation (00-16-52).srt : 00:00:22,110 --> 00:00:30,010': 'were going to focus on how to improve the instant changing of this model'}
--------------------------------------------------
9 instant 2.4426557483831942e-06 were going to focus on how to improve the instant changing of this model 
{'2 - 7 - 1.7 Vector Space Model- Improved Instantiation (00-16-52).srt : 00:00:07,738 --> 00:00:12,706': 'in this lecture were going to talk about how to improve the instant changing of', '2 - 7 - 1.7 Vector Space Model- Improved Instantiation (00-16-52).srt : 00:00:22,110 --> 00:00:30,010': 'were going to focus on how to improve the instant changing of this model'}
--------------------------------------------------
8 couldnt 1.7445380593498679e-06 so the problem here is that this function couldnt capture the following characteristics 
{'2 - 7 - 1.7 Vector Space Model- Improved Instantiation (00-16-52).srt : 00:01:14,760 --> 00:01:20,470': 'so the problem here is that this function couldnt capture'}
--------------------------------------------------
7 gratitude 1.2213278741915971e-06 first we would like to give more gratitude to d because it matches the presidential more times than d 
{'2 - 7 - 1.7 Vector Space Model- Improved Instantiation (00-16-52).srt : 00:01:22,590 --> 00:01:27,280': 'first we would like to give more gratitude to d'}
--------------------------------------------------
11 substantiating 1.2213278741915971e-06 if we look back at the assumptions we have made while substantiating the vector space model we will realize that the problem is really coming from some of the assumptions 
{'2 - 7 - 1.7 Vector Space Model- Improved Instantiation (00-16-52).srt : 00:02:04,690 --> 00:02:08,996': 'while substantiating the vector space model we will realize that'}
--------------------------------------------------
3 concede 1.2213278741915971e-06 we have to concede a term frequency the count of a term being in the document 
{'2 - 7 - 1.7 Vector Space Model- Improved Instantiation (00-16-52).srt : 00:03:07,620 --> 00:03:13,130': 'we have to concede a term frequency the count of a term being in the document'}
--------------------------------------------------
11 cos 1.2213278741915971e-06 in fact it looks identical but inside of the sum of cos xi and yi are now different 
{'2 - 7 - 1.7 Vector Space Model- Improved Instantiation (00-16-52).srt : 00:04:14,270 --> 00:04:18,900': 'in fact it looks identical but inside of the sum of cos xi and'}
--------------------------------------------------
9 vsm 1.7445380593498679e-06 it doing something very similar to what the simplest vsm is doing 
{'2 - 7 - 1.7 Vector Space Model- Improved Instantiation (00-16-52).srt : 00:04:41,860 --> 00:04:47,710': 'it doing something very similar to what the simplest vsm is doing'}
--------------------------------------------------
5 currents 1.2213278741915971e-06 so if you count the currents of the water in the whole collection that we would see that about as much higher for this than presidential which it tends to occur only in some documents 
{'2 - 7 - 1.7 Vector Space Model- Improved Instantiation (00-16-52).srt : 00:07:46,170 --> 00:07:50,930': 'so if you count the currents of the water in the whole collection that we'}
--------------------------------------------------
6 moding 1.2213278741915971e-06 as one signal used in the moding retrieval functions 
{'2 - 7 - 1.7 Vector Space Model- Improved Instantiation (00-16-52).srt : 00:08:59,270 --> 00:09:04,030': 'as one signal used in the moding retrieval functions'}
--------------------------------------------------
11 reword 1.2213278741915971e-06 here we say inverse document frequency because we actually want to reword a word that doesnt occur in many documents 
{'2 - 7 - 1.7 Vector Space Model- Improved Instantiation (00-16-52).srt : 00:09:15,900 --> 00:09:20,990': 'here we say inverse document frequency because we actually want to reword a word'}
--------------------------------------------------
28 collectionk 1.2213278741915971e-06 so most specific inaudible idf can be defined as the logarithm of m plus one divided by k where m is the total number of documents in the collectionk is df or document frequency 
{'2 - 7 - 1.7 Vector Space Model- Improved Instantiation (00-16-52).srt : 00:10:06,180 --> 00:10:11,730': 'documents in the collectionk is df or document frequency'}
--------------------------------------------------
10 kernalization 1.2213278741915971e-06 but it also clear that if we use a linear kernalization like what shown here with this line then it may not be as reasonable as the standard idf 
{'2 - 7 - 1.7 Vector Space Model- Improved Instantiation (00-16-52).srt : 00:11:15,190 --> 00:11:20,810': 'but it also clear that if we use a linear kernalization like what'}
--------------------------------------------------
7 kernelization 1.2213278741915971e-06 but if you look at the linear kernelization at this point there is there some difference 
{'2 - 7 - 1.7 Vector Space Model- Improved Instantiation (00-16-52).srt : 00:12:14,010 --> 00:12:17,200': 'but if you look at the linear kernelization at this point there is'}
--------------------------------------------------
11 validated 1.7445380593498679e-06 well of course which one works better still has to be validated by using the empirically related data set 
{'2 - 7 - 1.7 Vector Space Model- Improved Instantiation (00-16-52).srt : 00:12:32,990 --> 00:12:38,723': 'well of course which one works better still has to be validated'}
--------------------------------------------------
3 rail 1.2213278741915971e-06 because it matched rail word where as d matched common word 
{'2 - 7 - 1.7 Vector Space Model- Improved Instantiation (00-16-52).srt : 00:14:01,250 --> 00:14:06,460': 'because it matched rail word where as d matched common word'}
--------------------------------------------------
9 placement 1.2213278741915971e-06 so the improvement most of it is on the placement of the vector 
{'2 - 7 - 1.7 Vector Space Model- Improved Instantiation (00-16-52).srt : 00:16:08,470 --> 00:16:12,340': 'so the improvement most of it is on the placement of the vector'}
--------------------------------------------------
16 infrequently 1.2213278741915971e-06 where we give higher weight to a term that occurred many times in the document but infrequently in the whole collection 
{'2 - 7 - 1.7 Vector Space Model- Improved Instantiation (00-16-52).srt : 00:16:20,155 --> 00:16:23,330': 'but infrequently in the whole collection '}
--------------------------------------------------
6 emote 1.2213278741915971e-06 here i used the xi to emote the feature 
{'5 - 7 - 4.4 Learning to Rank - Part 2 (00-05-54).srt : 00:00:39,730 --> 00:00:47,390': 'here i used the xi to emote the feature '}
--------------------------------------------------
6 ze 1.2213278741915971e-06 so this transformation here would map ze range of real values 
{'5 - 7 - 4.4 Learning to Rank - Part 2 (00-05-54).srt : 00:01:53,900 --> 00:01:58,758': 'so this transformation here would map ze '}
--------------------------------------------------
22 efficients 1.2213278741915971e-06 so this allows us then to connect to the probability of relevance which is between  and  to a linear combination of arbitrary efficients 
{'5 - 7 - 4.4 Learning to Rank - Part 2 (00-05-54).srt : 00:02:17,603 --> 00:02:23,000': 'which is between and to a linear combination of arbitrary efficients'}
--------------------------------------------------
5 nonactive 1.2213278741915971e-06 now this form is created nonactive 
{'5 - 7 - 4.4 Learning to Rank - Part 2 (00-05-54).srt : 00:02:39,460 --> 00:02:42,620': 'now this form is created nonactive '}
--------------------------------------------------
4 pluck 1.2213278741915971e-06 we just need to pluck in the x the xis 
{'5 - 7 - 4.4 Learning to Rank - Part 2 (00-05-54).srt : 00:06:57,890 --> 00:07:00,970': 'we just need to pluck in the x the xis '}
--------------------------------------------------
3 nickname 2.4426557483831942e-06 i have a nickname cheng 
{'1 - 1 - Course Welcome (00-03-11).srt : 00:00:13,984 --> 00:00:16,430': 'i have a nickname cheng ', '1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:00:13,940 --> 00:00:15,110': 'i have a nickname cheng '}
--------------------------------------------------
13 jiawei 1.4829329667707326e-06 in addition to this course there are four other courses offered by professor jiawei han professor john hart and me followed by a capstone project course that all of us will teach together 
{'1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:00:39,380 --> 00:00:44,770': 'professor jiawei han professor john hart and me followed by'}
--------------------------------------------------
17 hart 2.4426557483831942e-06 in addition to this course there are four other courses offered by professor jiawei han professor john hart and me followed by a capstone project course that all of us will teach together 
{'1 - 1 - Course Welcome (00-03-11).srt : 00:01:52,330 --> 00:01:57,229': 'and data mine data visualization covered by professor jung hart is about', '1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:00:39,380 --> 00:00:44,770': 'professor jiawei han professor john hart and me followed by'}
--------------------------------------------------
23 capstone 2.4426557483831942e-06 in addition to this course there are four other courses offered by professor jiawei han professor john hart and me followed by a capstone project course that all of us will teach together 
{'1 - 1 - Course Welcome (00-03-11).srt : 00:01:15,270 --> 00:01:19,250': 'plus one capstone project course in the end', '1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:00:44,770 --> 00:00:49,370': 'a capstone project course that all of us will teach together'}
--------------------------------------------------
31 teach 2.4426557483831942e-06 in addition to this course there are four other courses offered by professor jiawei han professor john hart and me followed by a capstone project course that all of us will teach together 
{'1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:00:44,770 --> 00:00:49,370': 'a capstone project course that all of us will teach together', '1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:05:01,657 --> 00:05:06,860': 'to learn about the knowledge that we teach in this course'}
--------------------------------------------------
4 pressing 1.7445380593498679e-06 this course addresses a pressing need for harnessing big text data 
{'1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:01:28,312 --> 00:01:33,490': 'this course addresses a pressing need for harnessing big text data'}
--------------------------------------------------
4 growing 2.4426557483831942e-06 text data has been growing dramatically recently mostly because of the advance of technologies deployed on the web that would enable people to quickly generate text data 
{'1 - 1 - Course Welcome (00-03-11).srt : 00:02:11,210 --> 00:02:15,130': 'and the data is everywhere is growing rapidly so', '1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:01:35,708 --> 00:01:39,651': 'text data has been growing dramatically recently'}
--------------------------------------------------
5 dramatically 2.2677482445081393e-06 text data has been growing dramatically recently mostly because of the advance of technologies deployed on the web that would enable people to quickly generate text data 
{'1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:01:35,708 --> 00:01:39,651': 'text data has been growing dramatically recently'}
--------------------------------------------------
11 advance 2.2677482445081393e-06 text data has been growing dramatically recently mostly because of the advance of technologies deployed on the web that would enable people to quickly generate text data 
{'1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:01:39,651 --> 00:01:44,582': 'mostly because of the advance of technologies deployed on the web'}
--------------------------------------------------
14 deployed 1.2213278741915971e-06 text data has been growing dramatically recently mostly because of the advance of technologies deployed on the web that would enable people to quickly generate text data 
{'1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:01:39,651 --> 00:01:44,582': 'mostly because of the advance of technologies deployed on the web'}
--------------------------------------------------
0 blogs 1.2213278741915971e-06 blogs are another kind of new text data that are being generated quickly by people 
{'1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:02:13,160 --> 00:02:17,040': 'blogs are another kind of new text data that'}
--------------------------------------------------
2 vast 2.006143151929004e-06 it a vast amount of knowledge of all the text and data in these literature articles 
{'1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:03:03,350 --> 00:03:06,630': 'it a vast amount of knowledge of '}
--------------------------------------------------
4 forums 1.2213278741915971e-06 of course there are forums as well 
{'1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:03:21,400 --> 00:03:23,020': 'of course there are forums as well '}
--------------------------------------------------
6 terminologies 1.7445380593498679e-06 different communities tend to use different terminologies yet theyre starting very similar problems 
{'1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:05:37,920 --> 00:05:41,280': 'different communities tend to use different terminologies yet'}
--------------------------------------------------
15 useable 1.2213278741915971e-06 so there are many such examples where we can leverage the text data to discover useable knowledge to optimize our decision 
{'1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:06:02,100 --> 00:06:05,680': 'to discover useable knowledge to optimize our decision'}
--------------------------------------------------
7 technologiesyet 1.2213278741915971e-06 so these are two very much related technologiesyet they have somewhat different purposes 
{'1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:06:13,300 --> 00:06:17,480': 'so these are two very much related technologiesyet'}
--------------------------------------------------
12 pipeline 2.006143151929004e-06 this course covers text mining which is a second step in this pipeline that can be used to further process the small amount of relevant data to extract the knowledge or to help people digest the text data easily 
{'1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:06:51,470 --> 00:06:57,360': 'this course covers text mining which is a second step in this pipeline'}
--------------------------------------------------
16 selfcontained 1.4829329667707326e-06 if you have not taken the text retrieval course it also fine because this course is selfcontained and you can certainly understand all of the materials without a problem 
{'1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:07:39,820 --> 00:07:45,010': 'it also fine because this course is selfcontained and'}
--------------------------------------------------
7 binding 1.2213278741915971e-06 so so far we have talked about binding judgements that means a documents is judged as being relevant or notrelevant 
{'3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).srt : 00:00:27,460 --> 00:00:31,180': 'so so far we have talked about binding judgements'}
--------------------------------------------------
19 notrelevant 1.7445380593498679e-06 so so far we have talked about binding judgements that means a documents is judged as being relevant or notrelevant 
{'3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).srt : 00:00:31,180 --> 00:00:34,159': 'that means a documents is judged as being relevant or notrelevant'}
--------------------------------------------------
33 vinyl 1.2213278741915971e-06 now how do we evaluate such a new system using these judgements of use of the map doesnt work average of precision doesnt work precision and record doesnt work because they rely on vinyl judgement 
{'3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).srt : 00:01:23,330 --> 00:01:26,839': 'record doesnt work because they rely on vinyl judgement'}
--------------------------------------------------
16 infusing 1.2213278741915971e-06 and the reason why we call it a gain is because the measure that we are infusing is called ntcg normalizer discount of accumulative gain 
{'3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).srt : 00:02:03,380 --> 00:02:08,860': 'we are infusing is called ntcg normalizer discount of accumulative gain'}
--------------------------------------------------
19 ntcg 1.2213278741915971e-06 and the reason why we call it a gain is because the measure that we are infusing is called ntcg normalizer discount of accumulative gain 
{'3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).srt : 00:02:03,380 --> 00:02:08,860': 'we are infusing is called ntcg normalizer discount of accumulative gain'}
--------------------------------------------------
23 accumulative 2.4426557483831942e-06 and the reason why we call it a gain is because the measure that we are infusing is called ntcg normalizer discount of accumulative gain 
{'5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:03:14,832 --> 00:03:18,994': 'normalized discounted accumulative gain and also precision and', '3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).srt : 00:02:03,380 --> 00:02:08,860': 'we are infusing is called ntcg normalizer discount of accumulative gain'}
--------------------------------------------------
8 positua 1.2213278741915971e-06 so if we use a stops at the positua that just a three 
{'3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).srt : 00:02:55,815 --> 00:02:59,405': 'so if we use a stops at the positua that just a three'}
--------------------------------------------------
8 spending 1.7445380593498679e-06 of course this is at the cost of spending more time to examine the list 
{'3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).srt : 00:03:08,220 --> 00:03:13,200': 'of course this is at the cost of spending more time to examine the list'}
--------------------------------------------------
19 examines 1.7445380593498679e-06 so cumulative gain gives us some idea about how much total gain the user would have if the user examines all these documents 
{'3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).srt : 00:03:16,320 --> 00:03:21,690': 'how much total gain the user would have if the user examines all these documents'}
--------------------------------------------------
7 letter 1.7445380593498679e-06 now in ndcg we also have another letter here d discounted cumulative gain 
{'3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).srt : 00:03:21,690 --> 00:03:28,130': 'now in ndcg we also have another letter here d discounted cumulative gain', '4 - 12 - 3.12 Text Categorization- Evaluation Part 1 (00-14-12).srt : 00:13:47,443 --> 00:13:53,443': 'there is an extreme case where you have for one letter and one for the other'}
--------------------------------------------------
5 lister 1.2213278741915971e-06 then our ideal ranked the lister would have put all these nine documents on the very top 
{'3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).srt : 00:06:53,840 --> 00:06:56,720': 'then our ideal ranked the lister '}
--------------------------------------------------
5 idealdcg 2.2677482445081393e-06 like so here and this idealdcg would be used as a normalizer 
{'3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).srt : 00:07:33,915 --> 00:07:40,015': 'like so here and this idealdcg would be used as a normalizer'}
--------------------------------------------------
8 idealist 2.4426557483831942e-06 that when youre relevance is in fact the idealist 
{'3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).srt : 00:08:01,650 --> 00:08:06,270': 'that when youre relevance is in fact the idealist', '3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:17:18,330 --> 00:17:23,800': 'so suppose the encoded idealist is x x x et cetera'}
--------------------------------------------------
8 numberization 1.2213278741915971e-06 well you can see this transformation or this numberization doesnt really affect the relative comparison of systems for just one topic because this ideal dcg is the same for all the systems 
{'3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).srt : 00:08:14,900 --> 00:08:18,940': 'well you can see this transformation or this numberization'}
--------------------------------------------------
4 quires 1.2213278741915971e-06 those are again easy quires 
{'3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).srt : 00:09:21,060 --> 00:09:23,360': 'those are again easy quires '}
--------------------------------------------------
3 purists 1.2213278741915971e-06 making all the purists contribute equal to the average 
{'3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).srt : 00:09:27,210 --> 00:09:31,690': 'making all the purists contribute equal to the average'}
--------------------------------------------------
19 comparability 2.2677482445081393e-06 and it would discount the contribution from a lowly ranked document and finally it would do normalization to ensure comparability across queries 
{'3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).srt : 00:10:29,535 --> 00:10:35,789': 'and finally it would do normalization to ensure comparability across queries'}
--------------------------------------------------
7 imperative 2.4426557483831942e-06 there we mentioned that text retrieval is imperative to find the problem 
{'4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).srt : 00:01:52,580 --> 00:01:55,750': 'to use imperative data to estimate this probability', '3 - 4 - 2.4 Evaluation of TR Systems (00-10-10).srt : 00:01:08,440 --> 00:01:14,390': 'there we mentioned that text retrieval is imperative to find the problem'}
--------------------------------------------------
9 matters 1.2213278741915971e-06 because how can we get users involved in in matters and how can we draw a fair comparison of different methods 
{'3 - 4 - 2.4 Evaluation of TR Systems (00-10-10).srt : 00:01:28,810 --> 00:01:32,430': 'because how can we get users involved in in matters and'}
--------------------------------------------------
25 regional 1.2213278741915971e-06 the second reason is basically what i just said but there is also another reason which is to assess the actual utility of a test regional system 
{'3 - 4 - 2.4 Evaluation of TR Systems (00-10-10).srt : 00:01:47,510 --> 00:01:51,930': 'which is to assess the actual utility of a test regional system'}
--------------------------------------------------
10 attitude 1.2213278741915971e-06 the main idea that people have proposed before using a attitude evaluate a text retrieval algorithm is called the cranfield evaluation methodology 
{'3 - 4 - 2.4 Evaluation of TR Systems (00-10-10).srt : 00:04:23,110 --> 00:04:28,840': 'the main idea that people have proposed before using a attitude evaluate'}
--------------------------------------------------
4 laboratory 1.7445380593498679e-06 it a methodology for laboratory test of system components it actually a methodology that has been very useful not just for search engine evaluation 
{'3 - 4 - 2.4 Evaluation of TR Systems (00-10-10).srt : 00:04:40,145 --> 00:04:46,545': 'it a methodology for laboratory test of system components it actually'}
--------------------------------------------------
11 reusable 1.7445380593498679e-06 so the basic idea of this approach is itll build a reusable test collections and define measures 
{'3 - 4 - 2.4 Evaluation of TR Systems (00-10-10).srt : 00:05:20,100 --> 00:05:25,140': 'so the basic idea of this approach is itll build a reusable test collections'}
--------------------------------------------------
15 setter 1.2213278741915971e-06 so this is remember we talked about task of computing approximation of the relevant document setter 
{'3 - 4 - 2.4 Evaluation of TR Systems (00-10-10).srt : 00:08:08,400 --> 00:08:09,880': 'relevant document setter '}
--------------------------------------------------
7 reinforce 1.2213278741915971e-06 so you then you would have iterative reinforce each other because you can point it to some good hubs 
{'5 - 5 - 4.3 Link Analysis - Part 3 (00-05-59).srt : 00:01:17,880 --> 00:01:20,570': 'so you then you would have iterative reinforce each other'}
--------------------------------------------------
30 appoints 1.2213278741915971e-06 right again it the same graph and then were going to define the top score of page as a sum of the authority scores of all the pages that it appoints to 
{'5 - 5 - 4.3 Link Analysis - Part 3 (00-05-59).srt : 00:02:01,039 --> 00:02:06,965': 'page as a sum of the authority scores of all the pages that it appoints to'}
--------------------------------------------------
3 fo 1.2213278741915971e-06 in the matrix fo format 
{'5 - 5 - 4.3 Link Analysis - Part 3 (00-05-59).srt : 00:02:41,690 --> 00:02:44,600': 'in the matrix fo format '}
--------------------------------------------------
3 plugin 1.2213278741915971e-06 you can also plugin the authority equation into the first one 
{'5 - 5 - 4.3 Link Analysis - Part 3 (00-05-59).srt : 00:03:19,410 --> 00:03:26,670': 'you can also plugin the authority equation into the first one'}
--------------------------------------------------
18 mer 1.2213278741915971e-06 now the difference between this and page is that now the matrix is actually a multiplication of the mer adjacency matrix and its transpose 
{'5 - 5 - 4.3 Link Analysis - Part 3 (00-05-59).srt : 00:04:03,410 --> 00:04:07,842': 'is actually a multiplication of the mer adjacency matrix and its transpose'}
--------------------------------------------------
12 debate 1.2213278741915971e-06 and so the arrows of these are exactly the same in the debate rank 
{'5 - 5 - 4.3 Link Analysis - Part 3 (00-05-59).srt : 00:04:34,570 --> 00:04:37,740': 'and so the arrows of these are exactly the same in the debate rank'}
--------------------------------------------------
8 grooves 1.2213278741915971e-06 and this would allow us to control the grooves of value 
{'5 - 5 - 4.3 Link Analysis - Part 3 (00-05-59).srt : 00:04:47,880 --> 00:04:50,980': 'and this would allow us to control the grooves of value'}
--------------------------------------------------
3 grew 1.2213278741915971e-06 otherwise they would grew larger and larger 
{'5 - 5 - 4.3 Link Analysis - Part 3 (00-05-59).srt : 00:04:50,980 --> 00:04:53,980': 'otherwise they would grew larger and larger'}
--------------------------------------------------
8 ranging 2.4426557483831942e-06 and these scores can then be used in ranging to start the pagerank scores 
{'5 - 5 - 4.3 Link Analysis - Part 3 (00-05-59).srt : 00:05:03,920 --> 00:05:08,637': 'and these scores can then be used in ranging to start the pagerank scores', '5 - 6 - 4.4 Learning to Rank Part 1 (00-13-09).srt : 00:04:53,272 --> 00:05:00,022': 'so in general the fit such hypothesize ranging function to the training day'}
--------------------------------------------------
15 programmers 1.7445380593498679e-06 one is the google file system that a general distributed file system that can help programmers manage files stored on a cluster of machines 
{'5 - 2 - 4.2 Web Indexing (00-17-19).srt : 00:01:25,522 --> 00:01:30,890': 'that can help programmers manage files stored on a cluster of machines'}
--------------------------------------------------
5 centralized 2.2677482445081393e-06 it uses a very simple centralized management mechanism to manage all the specific locations of files 
{'5 - 2 - 4.2 Web Indexing (00-17-19).srt : 00:01:53,790 --> 00:01:58,768': 'it uses a very simple centralized management mechanism to manage'}
--------------------------------------------------
2 obtains 1.7445380593498679e-06 and that obtains specific locations of the files that they want to process 
{'5 - 2 - 4.2 Web Indexing (00-17-19).srt : 00:02:15,801 --> 00:02:21,420': 'and that obtains specific locations of the files that they want to process'}
--------------------------------------------------
26 sits 1.2213278741915971e-06 and once the gfs client obtained the specific information about the files then the application client can talk to the specific servers where the data actually sits directly 
{'5 - 2 - 4.2 Web Indexing (00-17-19).srt : 00:02:35,848 --> 00:02:40,250': 'servers where the data actually sits directly'}
--------------------------------------------------
5 stores 1.7445380593498679e-06 so when this file system stores the files on machines the system also would create a fixed sizes of chunks 
{'5 - 2 - 4.2 Web Indexing (00-17-19).srt : 00:02:46,020 --> 00:02:49,730': 'so when this file system stores the files on machines'}
--------------------------------------------------
12 megabytes 1.7445380593498679e-06 so the data files are separate into many chunks each chunk is  megabytes so it pretty big 
{'5 - 2 - 4.2 Web Indexing (00-17-19).srt : 00:03:01,460 --> 00:03:05,100': 'each chunk is megabytes so it pretty big'}
--------------------------------------------------
3 replicated 2.2677482445081393e-06 these chunks are replicated to ensure reliability 
{'5 - 2 - 4.2 Web Indexing (00-17-19).srt : 00:03:09,090 --> 00:03:12,520': 'these chunks are replicated to ensure reliability'}
--------------------------------------------------
5 hiding 2.4426557483831942e-06 and so this framework is hiding a lot of low level features from the programmer 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:01:28,830 --> 00:01:33,990': 'so in generally hiding marginal space such as', '5 - 2 - 4.2 Web Indexing (00-17-19).srt : 00:04:05,668 --> 00:04:10,852': 'and so this framework is hiding a lot of '}
--------------------------------------------------
18 balancing 1.7445380593498679e-06 so some of the low level details hidden in the framework including the specific natural communications or load balancing or where the tasks are executed all these details are hidden from the programmer 
{'5 - 2 - 4.2 Web Indexing (00-17-19).srt : 00:04:33,120 --> 00:04:39,520': 'including the specific natural communications or load balancing'}
--------------------------------------------------
24 executed 1.2213278741915971e-06 so some of the low level details hidden in the framework including the specific natural communications or load balancing or where the tasks are executed all these details are hidden from the programmer 
{'5 - 2 - 4.2 Web Indexing (00-17-19).srt : 00:04:39,520 --> 00:04:46,630': 'or where the tasks are executed all these details are hidden from the programmer'}
--------------------------------------------------
9 builtin 1.7445380593498679e-06 there is also a nice feature which is the builtin fault tolerance 
{'5 - 2 - 4.2 Web Indexing (00-17-19).srt : 00:04:47,880 --> 00:04:52,250': 'there is also a nice feature which is the builtin fault tolerance'}
--------------------------------------------------
4 dispatch 1.7445380593498679e-06 so it would automatically dispatch the task on other servers that can do the job 
{'5 - 2 - 4.2 Web Indexing (00-17-19).srt : 00:05:05,300 --> 00:05:11,600': 'so it would automatically dispatch the task on other servers that can do the job'}
--------------------------------------------------
30 keys 2.2677482445081393e-06 each is processed in parallel first by map and then in the process after we reach the reduce stage then much more reduce functions can also further process the different keys and their associated values in parallel 
{'5 - 2 - 4.2 Web Indexing (00-17-19).srt : 00:07:53,677 --> 00:08:00,390': 'the different keys and their associated values in parallel'}
--------------------------------------------------
2 parallelize 2.2677482445081393e-06 we can parallelize lines in this input file 
{'5 - 2 - 4.2 Web Indexing (00-17-19).srt : 00:09:02,910 --> 00:09:06,420': 'we can parallelize lines in this input file'}
--------------------------------------------------
2 keyvalue 1.4829329667707326e-06 so this keyvalue pair will be sent to a map function 
{'5 - 2 - 4.2 Web Indexing (00-17-19).srt : 00:09:32,240 --> 00:09:36,240': 'so this keyvalue pair will be sent to a map function'}
--------------------------------------------------
9 concatenate 1.7445380593498679e-06 so all you need to do is simply to concatenate them into a continuous chunk of data and this can be then retained into a file system 
{'5 - 2 - 4.2 Web Indexing (00-17-19).srt : 00:14:30,900 --> 00:14:37,680': 'so all you need to do is simply to concatenate them into a continuous chunk'}
--------------------------------------------------
16 associative 1.7445380593498679e-06 in the case of map it going to count the occurrences of a word using an associative array and will output all the counts together with the document id here 
{'5 - 2 - 4.2 Web Indexing (00-17-19).srt : 00:15:20,413 --> 00:15:26,180': 'it going to count the occurrences of a word using an associative array'}
--------------------------------------------------
10 concatenates 1.7445380593498679e-06 so this the reduce function on the other hand simply concatenates all the input that it has been given and then put them together as one single entry for this key 
{'5 - 2 - 4.2 Web Indexing (00-17-19).srt : 00:15:37,250 --> 00:15:42,400': 'the reduce function on the other hand simply concatenates'}
--------------------------------------------------
8 frameworks 1.2213278741915971e-06 note that the both the gfs and mapreduce frameworks are very general so they can also support many other applications 
{'5 - 2 - 4.2 Web Indexing (00-17-19).srt : 00:16:57,850 --> 00:17:02,059': 'note that the both the gfs and mapreduce frameworks are very general so'}
--------------------------------------------------
10 websites 2.4426557483831942e-06 for example they can be documents terms passages sentences or websites and then ill go group similar text objects together 
{'4 - 1 - 3.1 Text Clustering- Motivation (00-15-52).srt : 00:01:14,510 --> 00:01:21,510': 'sentences or websites and then ill go group similar text objects together', '4 - 1 - 3.1 Text Clustering- Motivation (00-15-52).srt : 00:07:24,600 --> 00:07:27,440': 'so for example we might cluster websites '}
--------------------------------------------------
32 chips 1.2213278741915971e-06 now if i ask you what are some natural structures or natural groups where you if you look at it and you might agree that we can group these objects based on chips or their locations on this two dimensional space 
{'4 - 1 - 3.1 Text Clustering- Motivation (00-15-52).srt : 00:01:39,690 --> 00:01:47,790': 'if you look at it and you might agree that we can group these objects based on chips'}
--------------------------------------------------
8 agreement 1.7445380593498679e-06 and they may not be so much this agreement about these three clusters but it really depends on the perspective to look at the objects 
{'4 - 1 - 3.1 Text Clustering- Motivation (00-15-52).srt : 00:01:56,940 --> 00:02:01,360': 'and they may not be so much this agreement about'}
--------------------------------------------------
26 functionally 1.2213278741915971e-06 well it depends on how if we look at the physical properties of car and horse they are very different but if you look at them functionally a car and a horse can both be transportation tools 
{'4 - 1 - 3.1 Text Clustering- Motivation (00-15-52).srt : 00:03:16,000 --> 00:03:20,630': 'if you look at them functionally a car and a horse'}
--------------------------------------------------
35 transportation 1.7445380593498679e-06 well it depends on how if we look at the physical properties of car and horse they are very different but if you look at them functionally a car and a horse can both be transportation tools 
{'4 - 1 - 3.1 Text Clustering- Motivation (00-15-52).srt : 00:03:20,630 --> 00:03:23,650': 'can both be transportation tools '}
--------------------------------------------------
3 ought 1.2213278741915971e-06 and so it ought to make the clustering problem well defined 
{'4 - 1 - 3.1 Text Clustering- Motivation (00-15-52).srt : 00:03:32,740 --> 00:03:37,700': 'and so it ought to make the clustering problem well defined'}
--------------------------------------------------
8 steer 2.2677482445081393e-06 for example you might think well we can steer a group by ships so that would give us cluster that looks like this 
{'4 - 1 - 3.1 Text Clustering- Motivation (00-15-52).srt : 00:04:47,810 --> 00:04:53,510': 'can steer a group by ships so that would give us cluster that looks like this'}
--------------------------------------------------
12 ships 1.2213278741915971e-06 for example you might think well we can steer a group by ships so that would give us cluster that looks like this 
{'4 - 1 - 3.1 Text Clustering- Motivation (00-15-52).srt : 00:04:47,810 --> 00:04:53,510': 'can steer a group by ships so that would give us cluster that looks like this'}
--------------------------------------------------
23 ripple 1.2213278741915971e-06 now once weve got those text objects then we can cluster the segments that weve got to discover interesting clusters that might also ripple in the subtopics 
{'4 - 1 - 3.1 Text Clustering- Motivation (00-15-52).srt : 00:06:50,850 --> 00:06:56,908': 'discover interesting clusters that might also ripple in the subtopics'}
--------------------------------------------------
6 websites 2.4426557483831942e-06 so for example we might cluster websites 
{'4 - 1 - 3.1 Text Clustering- Motivation (00-15-52).srt : 00:01:14,510 --> 00:01:21,510': 'sentences or websites and then ill go group similar text objects together', '4 - 1 - 3.1 Text Clustering- Motivation (00-15-52).srt : 00:07:24,600 --> 00:07:27,440': 'so for example we might cluster websites '}
--------------------------------------------------
3 trigger 1.7445380593498679e-06 so we can trigger all the articles published by also as one unit for clustering 
{'4 - 1 - 3.1 Text Clustering- Motivation (00-15-52).srt : 00:07:39,065 --> 00:07:44,573': 'so we can trigger all the articles published by also as one unit for'}
--------------------------------------------------
21 antithesis 1.2213278741915971e-06 so in order to get the meaning we would have to map these phrases and these structures into some real world antithesis that we have in our mind 
{'2 - 3 - 1.3 Natural Language Content Analysis- Part 1 (00-12-48).srt : 00:02:34,530 --> 00:02:39,860': 'these structures into some real world antithesis that we have in our mind'}
--------------------------------------------------
8 arguments 1.2213278741915971e-06 so chasing is a predicate here with three arguments d b and p 
{'2 - 3 - 1.3 Natural Language Content Analysis- Part 1 (00-12-48).srt : 00:03:18,334 --> 00:03:23,720': 'three arguments d b and p '}
--------------------------------------------------
11 requesting 1.2213278741915971e-06 and finally we might even further infer what this sentence is requesting or why the person who say it in a sentence is saying the sentence 
{'2 - 3 - 1.3 Natural Language Content Analysis- Part 1 (00-12-48).srt : 00:03:58,485 --> 00:04:06,170': 'what this sentence is requesting '}
--------------------------------------------------
4 killer 1.2213278741915971e-06 ambiguity is a main killer 
{'2 - 3 - 1.3 Natural Language Content Analysis- Part 1 (00-12-48).srt : 00:06:16,550 --> 00:06:19,380': 'ambiguity is a main killer '}
--------------------------------------------------
3 worldlevel 1.2213278741915971e-06 think about the worldlevel ambiguity 
{'2 - 3 - 1.3 Natural Language Content Analysis- Part 1 (00-12-48).srt : 00:06:51,280 --> 00:06:53,390': 'think about the worldlevel ambiguity '}
--------------------------------------------------
22 plant 2.4426557483831942e-06 root also has multiple meanings it can be of mathematical sense like in the square of or can be root of a plant 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:06:42,160 --> 00:06:45,120': 'so square root in math sense or the root of a plant', '2 - 3 - 1.3 Natural Language Content Analysis- Part 1 (00-12-48).srt : 00:07:06,190 --> 00:07:10,670': 'like in the square of or can be root of a plant'}
--------------------------------------------------
17 perfectly 2.2677482445081393e-06 because of these problems the state of the art natural language processing techniques can not do anything perfectly 
{'2 - 3 - 1.3 Natural Language Content Analysis- Part 1 (00-12-48).srt : 00:08:57,614 --> 00:09:01,410': 'techniques can not do anything perfectly '}
--------------------------------------------------
5 mechanical 1.4829329667707326e-06 because we cannot rely on mechanical approaches or computational methods to understand the language precisely 
{'2 - 3 - 1.3 Natural Language Content Analysis- Part 1 (00-12-48).srt : 00:11:50,320 --> 00:11:54,390': 'because we cannot rely on mechanical approaches or'}
--------------------------------------------------
29 preorder 1.2213278741915971e-06 so far in the lectures about the vector space model we have used the various signals from the document to assess the matching of the document though with a preorder 
{'2 - 9 - 1.9 Doc Length Normalization (00-18-56).srt : 00:00:33,310 --> 00:00:37,480': 'assess the matching of the document though with a preorder'}
--------------------------------------------------
18 matchings 1.7445380593498679e-06 if you look at the matching of these query words we see that in d there are more matchings of the query words but one might reason that d may have matched these query words 
{'2 - 9 - 1.9 Doc Length Normalization (00-18-56).srt : 00:01:10,280 --> 00:01:14,380': 'are more matchings of the query words but '}
--------------------------------------------------
15 chances 2.4426557483831942e-06 so in this sense we should penalize no documents because they just naturally have better chances to match any query 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:17:03,440 --> 00:17:08,720': 'if you see matching of some of the query words in a text document chances', '2 - 9 - 1.9 Doc Length Normalization (00-18-56).srt : 00:02:04,930 --> 00:02:07,220': 'just naturally have better chances to match any query'}
--------------------------------------------------
11 concatenated 1.7445380593498679e-06 now consider another case of a long document where we simply concatenated a lot of abstracts of different papers 
{'2 - 9 - 1.9 Doc Length Normalization (00-18-56).srt : 00:03:24,010 --> 00:03:29,450': 'where we simply concatenated a lot of abstracts of different papers'}
--------------------------------------------------
15 abstracts 1.7445380593498679e-06 now consider another case of a long document where we simply concatenated a lot of abstracts of different papers 
{'2 - 9 - 1.9 Doc Length Normalization (00-18-56).srt : 00:03:24,010 --> 00:03:29,450': 'where we simply concatenated a lot of abstracts of different papers'}
--------------------------------------------------
4 illustrator 1.2213278741915971e-06 so this is an illustrator that using this slide 
{'2 - 9 - 1.9 Doc Length Normalization (00-18-56).srt : 00:04:20,785 --> 00:04:23,475': 'so this is an illustrator that using this slide'}
--------------------------------------------------
27 worrying 2.2677482445081393e-06 this not only gives us some sense about the how this document is compared with the average document length but also gives us a benefit of not worrying about the unit of length we can measure the length by words or by characters 
{'2 - 9 - 1.9 Doc Length Normalization (00-18-56).srt : 00:05:07,890 --> 00:05:13,509': 'also gives us a benefit of not worrying about the unit of'}
--------------------------------------------------
5 conjured 1.2213278741915971e-06 the degree of penalization is conjured by b 
{'2 - 9 - 1.9 Doc Length Normalization (00-18-56).srt : 00:06:09,030 --> 00:06:11,470': 'the degree of penalization is conjured by b'}
--------------------------------------------------
3 plucking 1.2213278741915971e-06 so if were plucking this length normalization factor into the vector space model ranking functions that we have already examined 
{'2 - 9 - 1.9 Doc Length Normalization (00-18-56).srt : 00:06:29,440 --> 00:06:35,224': 'so if were plucking this length normalization factor into'}
--------------------------------------------------
3 played 1.7445380593498679e-06 and this has played an important role in uhdetermining the factors of the function 
{'2 - 9 - 1.9 Doc Length Normalization (00-18-56).srt : 00:09:35,010 --> 00:09:40,030': 'and this has played an important role in uhdetermining the factors of'}
--------------------------------------------------
8 uhdetermining 1.2213278741915971e-06 and this has played an important role in uhdetermining the factors of the function 
{'2 - 9 - 1.9 Doc Length Normalization (00-18-56).srt : 00:09:35,010 --> 00:09:40,030': 'and this has played an important role in uhdetermining the factors of'}
--------------------------------------------------
7 semantica 1.2213278741915971e-06 we can even use late in the semantica an answer sort of find in the sum cluster 
{'2 - 9 - 1.9 Doc Length Normalization (00-18-56).srt : 00:10:29,150 --> 00:10:34,338': 'we can even use late in the semantica an answer sort of find in the sum cluster'}
--------------------------------------------------
5 legend 1.7445380593498679e-06 so words that represent a legend of concept as one 
{'2 - 9 - 1.9 Doc Length Normalization (00-18-56).srt : 00:10:34,338 --> 00:10:39,660': 'so words that represent a legend of concept as one'}
--------------------------------------------------
19 fundamentally 1.7445380593498679e-06 but there has been also further development in improving bm although none of these works have changed the bm fundamentally 
{'2 - 9 - 1.9 Doc Length Normalization (00-18-56).srt : 00:13:10,400 --> 00:13:15,620': 'none of these works have changed the bm fundamentally'}
--------------------------------------------------
13 reasearch 1.2213278741915971e-06 for example you might consider title field the abstract or body of the reasearch article or even anchor text on the web pages 
{'2 - 9 - 1.9 Doc Length Normalization (00-18-56).srt : 00:13:31,090 --> 00:13:37,020': 'body of the reasearch article or even anchor text on the web pages'}
--------------------------------------------------
13 tons 1.2213278741915971e-06 basically the ideal of bmf is to first combine the frequency counts of tons in all the fields and then apply bm 
{'2 - 9 - 1.9 Doc Length Normalization (00-18-56).srt : 00:14:05,900 --> 00:14:11,670': 'the frequency counts of tons in all the fields and then apply bm'}
--------------------------------------------------
9 recurrence 1.2213278741915971e-06 remember in the sublinear transformation of tf the first recurrence is very important then and contributes a large weight 
{'2 - 9 - 1.9 Doc Length Normalization (00-18-56).srt : 00:14:22,000 --> 00:14:27,270': 'the first recurrence is very important then and contributes a large weight'}
--------------------------------------------------
13 arresters 1.2213278741915971e-06 the other line of extension is called a bm plus and this line arresters have addressed the problem of over penalization of long documents by bm 
{'2 - 9 - 1.9 Doc Length Normalization (00-18-56).srt : 00:14:55,810 --> 00:15:02,150': 'the other line of extension is called a bm plus and this line arresters'}
--------------------------------------------------
5 bmplus 1.2213278741915971e-06 so the new formula called bmplus is empirically and analytically shown to be better than bm 
{'2 - 9 - 1.9 Doc Length Normalization (00-18-56).srt : 00:15:33,580 --> 00:15:38,600': 'so the new formula called bmplus is empirically and'}
--------------------------------------------------
8 enhances 1.2213278741915971e-06 it an excellent example of using empirical data enhances to suggest a need for length normalization and then further derived a length normalization formula 
{'2 - 9 - 1.9 Doc Length Normalization (00-18-56).srt : 00:18:06,090 --> 00:18:11,830': 'it an excellent example of using empirical data enhances to suggest a need'}
--------------------------------------------------
14 overpenalization 2.2677482445081393e-06 and finally the last paper has a discussion of improving bm to correct the overpenalization of long documents 
{'2 - 9 - 1.9 Doc Length Normalization (00-18-56).srt : 00:18:37,893 --> 00:18:43,761': 'bm to correct the overpenalization of long documents'}
--------------------------------------------------
19 domainspecific 2.2677482445081393e-06 but when we apply such a approach to a particular problem we might also be able to leverage some domainspecific heuristics 
{'3 - 2 - 2.2 Topic Mining and Analysis- Term as Topic (00-11-31).srt : 00:04:06,650 --> 00:04:12,020': 'we might also be able to leverage some domainspecific heuristics'}
--------------------------------------------------
10 greedy 2.2677482445081393e-06 and one way to do that is to do a greedy algorithm which is sometimes called a maximal marginal relevance ranking 
{'3 - 2 - 2.2 Topic Mining and Analysis- Term as Topic (00-11-31).srt : 00:05:15,080 --> 00:05:19,600': 'and one way to do that is to do a greedy algorithm'}
--------------------------------------------------
3 restriction 1.7445380593498679e-06 finally a main restriction of this approach is that we have only one term to describe the topic so it cannot really describe complicated topics 
{'3 - 2 - 2.2 Topic Mining and Analysis- Term as Topic (00-11-31).srt : 00:09:56,360 --> 00:10:02,325': 'finally a main restriction of this approach is that we have only one'}
--------------------------------------------------
2 lacks 1.7445380593498679e-06 first it lacks expressive power 
{'3 - 2 - 2.2 Topic Mining and Analysis- Term as Topic (00-11-31).srt : 00:10:23,310 --> 00:10:26,725': 'first it lacks expressive power '}
--------------------------------------------------
13 conversion 1.2213278741915971e-06 so it does not allow us to easily count related terms to order conversion to coverage of this topic 
{'3 - 2 - 2.2 Topic Mining and Analysis- Term as Topic (00-11-31).srt : 00:10:57,060 --> 00:10:59,200': 'conversion to coverage of this topic '}
--------------------------------------------------
8 disintegration 2.4426557483831942e-06 finally there is this problem of word sense disintegration 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:11:03,010 --> 00:11:05,660': 'we can also do word sentence disintegration to some extent', '3 - 2 - 2.2 Topic Mining and Analysis- Term as Topic (00-11-31).srt : 00:10:59,200 --> 00:11:02,410': 'finally there is this problem of word sense disintegration'}
--------------------------------------------------
5 favors 1.7445380593498679e-06 the first is that it favors matching one frequent term very well over matching more distinct terms 
{'2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:00:42,910 --> 00:00:47,090': 'the first is that it favors matching one frequent term'}
--------------------------------------------------
24 wording 1.2213278741915971e-06 that is itll convert the raw count of a word in the document into some weight that reflects our belief about how important this wording 
{'2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:02:19,870 --> 00:02:25,500': 'that reflects our belief about how important this wording'}
--------------------------------------------------
7 prevents 1.2213278741915971e-06 and this what we want because it prevents that kind of terms from dominating the scoring function 
{'2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:04:37,410 --> 00:04:42,500': 'and this what we want because it prevents '}
--------------------------------------------------
13 dominating 2.4426557483831942e-06 and this what we want because it prevents that kind of terms from dominating the scoring function 
{'2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:04:42,500 --> 00:04:46,335': 'that kind of terms from dominating the scoring function', '3 - 7 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01) .srt : 00:03:12,750 --> 00:03:18,510': 'youll realize in arithmetic mean the sum is dominating by large values'}
--------------------------------------------------
5 multiplies 1.7445380593498679e-06 so it k plus one multiplies by x divided by x plus k where k is a parameter 
{'2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:05:07,580 --> 00:05:11,870': 'so it k plus one multiplies by x divided by x plus k'}
--------------------------------------------------
2 puts 1.7445380593498679e-06 so this puts a very strict constraint on high frequency terms because their weight will never exceed k   
{'2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:05:37,590 --> 00:05:43,110': 'so this puts a very strict constraint on high frequency terms'}
--------------------------------------------------
11 overemphasizing 2.4426557483831942e-06 so we just talked about how to solve the problem of overemphasizing a frequently a frequently tongue 
{'2 - 5 - 1.5 Vector Space Model- Basic Idea (00-09-44).srt : 00:07:54,870 --> 00:08:01,610': 'or overemphasizing of matching this concept', '2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:06:14,735 --> 00:06:19,365': 'so we just talked about how to solve the problem of overemphasizing a frequently'}
--------------------------------------------------
16 tongue 1.2213278741915971e-06 so we just talked about how to solve the problem of overemphasizing a frequently a frequently tongue 
{'2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:06:19,365 --> 00:06:21,045': 'a frequently tongue '}
--------------------------------------------------
0 pop 2.4426557483831942e-06 pop that commonly used in retrieval 
{'3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt : 00:00:48,800 --> 00:00:53,820': 'and the other kind is from our pop board distribution that we are interested in', '2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:06:39,060 --> 00:06:42,290': 'pop that commonly used in retrieval '}
--------------------------------------------------
15 rewards 1.7445380593498679e-06 the idf function is giving a higher value for a lower k meaning that it rewards a rare term and the maximum value is log of m 
{'2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:07:20,190 --> 00:07:25,440': 'a lower k meaning that it rewards a rare term and'}
--------------------------------------------------
1 rarest 1.2213278741915971e-06 the rarest term in the whole collection 
{'2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:07:37,760 --> 00:07:40,010': 'the rarest term in the whole collection '}
--------------------------------------------------
8 populating 1.2213278741915971e-06 and that is to say a word that populating the collection in general 
{'2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:08:16,940 --> 00:08:21,609': 'and that is to say a word that populating the collection in general'}
--------------------------------------------------
5 frequenting 1.2213278741915971e-06 then it will still be frequenting the collective context documents 
{'2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:08:41,250 --> 00:08:46,201': 'then it will still be frequenting the collective context documents'}
--------------------------------------------------
14 simulates 1.2213278741915971e-06 this controls the upper bound and the kind of all to what extent it simulates the linear transformation 
{'2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:11:05,810 --> 00:11:11,835': 'to what extent it simulates the linear transformation'}
--------------------------------------------------
19 stabilized 1.2213278741915971e-06 but i kept it here because it constant and that useful in retrieval where it would give us a stabilized interpretation of parameter b 
{'2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:12:08,710 --> 00:12:15,080': 'in retrieval where it would give us a stabilized interpretation of parameter b'}
--------------------------------------------------
15 overlapped 1.2213278741915971e-06 you may recall this is sum that indicates all the possible words that can be overlapped between the two contacts 
{'2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:13:06,210 --> 00:13:11,560': 'all the possible words that can be overlapped between the two contacts'}
--------------------------------------------------
19 contacts 2.4426557483831942e-06 you may recall this is sum that indicates all the possible words that can be overlapped between the two contacts 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:08:11,250 --> 00:08:16,050': 'in other words we have extra switches that are tied to these contacts that will', '2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:13:06,210 --> 00:13:11,560': 'all the possible words that can be overlapped between the two contacts'}
--------------------------------------------------
9 replant 1.2213278741915971e-06 in general when we represent a term vector to replant a context with a term vector we would likely see some terms have higher weights and other terms have lower weights 
{'2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:13:50,270 --> 00:13:57,140': 'in general when we represent a term vector to replant'}
--------------------------------------------------
7 converted 1.7445380593498679e-06 so the idea is to use the converted implantation of the context 
{'2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:14:28,490 --> 00:14:34,170': 'so the idea is to use the converted implantation of the context'}
--------------------------------------------------
8 implantation 1.2213278741915971e-06 so the idea is to use the converted implantation of the context 
{'2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:14:28,490 --> 00:14:34,170': 'so the idea is to use the converted implantation of the context'}
--------------------------------------------------
14 weigh 1.2213278741915971e-06 but if we apply idf weighting as you see here we can then re weigh these terms based on idf 
{'2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:15:27,715 --> 00:15:34,060': 'we can then re weigh these terms based on idf'}
--------------------------------------------------
14 parathmatic 1.2213278741915971e-06 now of course this is only a byproduct of how approach is for discovering parathmatic relations 
{'2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:16:17,280 --> 00:16:19,369': 'discovering parathmatic relations '}
--------------------------------------------------
29 paradgratical 1.2213278741915971e-06 discovered in a joined manner by leveraging such associations namely syntactical relation words that are similar in yeah it also shows the relation between syntagmatic relation discovery and the paradgratical relations discovery 
{'2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:17:03,068 --> 00:17:08,940': 'the paradgratical relations discovery '}
--------------------------------------------------
3 house 1.2213278741915971e-06 for example someone house condition or the weather or etc 
{'5 - 6 - 4.6 Text-Based Prediction (00-12-08).srt : 00:01:59,480 --> 00:02:05,850': 'for example someone house condition or the weather or etc'}
--------------------------------------------------
5 consuming 2.4426557483831942e-06 humans are the best in consuming or interpreting text data 
{'2 - 1 - 1.1 Overview Text Mining and Analytics- Part 1 (00-11-43).srt : 00:02:59,930 --> 00:03:05,030': 'and so this is to minimize a human effort in consuming text data in some sense', '5 - 6 - 4.6 Text-Based Prediction (00-12-08).srt : 00:03:43,350 --> 00:03:46,890': 'humans are the best in consuming or interpreting text data'}
--------------------------------------------------
5 subarea 1.7445380593498679e-06 and there was actually a subarea in machine learning called active learning that has to do with this 
{'5 - 6 - 4.6 Text-Based Prediction (00-12-08).srt : 00:05:23,640 --> 00:05:28,080': 'and there was actually a subarea in machine learning called active learning'}
--------------------------------------------------
6 evoke 1.2213278741915971e-06 the goal here is mainly to evoke values of realworld variables 
{'5 - 6 - 4.6 Text-Based Prediction (00-12-08).srt : 00:08:55,530 --> 00:09:01,120': 'the goal here is mainly to evoke values of realworld variables'}
--------------------------------------------------
12 preparations 1.2213278741915971e-06 but in order to achieve the goal we can do some other preparations 
{'5 - 6 - 4.6 Text-Based Prediction (00-12-08).srt : 00:09:01,120 --> 00:09:07,030': 'but in order to achieve the goal we can do some other preparations'}
--------------------------------------------------
2 subtask 1.2213278741915971e-06 so one subtask could mine the content of text data like topic mining 
{'5 - 6 - 4.6 Text-Based Prediction (00-12-08).srt : 00:09:08,200 --> 00:09:14,060': 'so one subtask could mine the content of text data like topic mining'}
--------------------------------------------------
2 enlarge 1.2213278741915971e-06 it would enlarge the space of patterns of opinions of topics that we can mine from text and that well discuss more later 
{'5 - 6 - 4.6 Text-Based Prediction (00-12-08).srt : 00:09:48,040 --> 00:09:53,210': 'it would enlarge the space of patterns of opinions of topics that we can'}
--------------------------------------------------
8 testimony 1.2213278741915971e-06 one perspective we have nontext can help with testimony 
{'5 - 6 - 4.6 Text-Based Prediction (00-12-08).srt : 00:10:05,900 --> 00:10:10,300': 'one perspective we have nontext can help with testimony'}
--------------------------------------------------
5 highlight 1.7445380593498679e-06 and i will need to highlight some of them in the next lectures 
{'5 - 6 - 4.6 Text-Based Prediction (00-12-08).srt : 00:10:34,840 --> 00:10:37,730': 'and i will need to highlight some of them in the next lectures'}
--------------------------------------------------
7 interpretable 1.7445380593498679e-06 and this difference in text data is interpretable because text content is easy to digest 
{'5 - 6 - 4.6 Text-Based Prediction (00-12-08).srt : 00:11:13,700 --> 00:11:18,180': 'and this difference in text data is interpretable because text content is'}
--------------------------------------------------
9 notations 2.4426557483831942e-06 so here the general idea and we use some notations here so 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:08:16,713 --> 00:08:21,267': 'now i use this way so that it more consistent with what notations', '5 - 13 - 4.7 Recommender Systems- Collaborative Filtering - Part 2 (00-12-09).srt : 00:01:05,110 --> 00:01:11,700': 'so here the general idea and we use some notations here so'}
--------------------------------------------------
16 app 1.2213278741915971e-06 so mathematically this is as you say the predict the rating of this user on this app object 
{'5 - 13 - 4.7 Recommender Systems- Collaborative Filtering - Part 2 (00-12-09).srt : 00:03:01,830 --> 00:03:07,560': 'the predict the rating of this user on this app object'}
--------------------------------------------------
10 retreat 1.2213278741915971e-06 another measure is the cosine measure and this is the retreat the rating vectors as vectors in the vector space and then were going to measure the the angel and compute the cosign of the angle of the two vectors 
{'5 - 13 - 4.7 Recommender Systems- Collaborative Filtering - Part 2 (00-12-09).srt : 00:08:11,545 --> 00:08:17,806': 'another measure is the cosine measure and this is the retreat the rating vectors as'}
--------------------------------------------------
28 angel 1.2213278741915971e-06 another measure is the cosine measure and this is the retreat the rating vectors as vectors in the vector space and then were going to measure the the angel and compute the cosign of the angle of the two vectors 
{'5 - 13 - 4.7 Recommender Systems- Collaborative Filtering - Part 2 (00-12-09).srt : 00:08:17,806 --> 00:08:23,808': 'vectors in the vector space and then were going to measure the the angel and'}
--------------------------------------------------
32 cosign 1.2213278741915971e-06 another measure is the cosine measure and this is the retreat the rating vectors as vectors in the vector space and then were going to measure the the angel and compute the cosign of the angle of the two vectors 
{'5 - 13 - 4.7 Recommender Systems- Collaborative Filtering - Part 2 (00-12-09).srt : 00:08:23,808 --> 00:08:27,870': 'compute the cosign of the angle of the two vectors'}
--------------------------------------------------
10 studying 2.006143151929004e-06 and those are all interesting approaches that people are still studying 
{'5 - 13 - 4.7 Recommender Systems- Collaborative Filtering - Part 2 (00-12-09).srt : 00:09:30,100 --> 00:09:36,550': 'and those are all interesting approaches that people are still studying'}
--------------------------------------------------
12 strand 1.2213278741915971e-06 and practical applications could be a starting point to see if the strand here works well for your application 
{'5 - 13 - 4.7 Recommender Systems- Collaborative Filtering - Part 2 (00-12-09).srt : 00:09:49,918 --> 00:09:53,628': 'see if the strand here works well for your application'}
--------------------------------------------------
13 preliminary 1.7445380593498679e-06 so you can imagine you have iterative approach where you first do some preliminary prediction and then you can use the predictor values to further improve the similarity function 
{'5 - 13 - 4.7 Recommender Systems- Collaborative Filtering - Part 2 (00-12-09).srt : 00:10:39,785 --> 00:10:44,075': 'you have iterative approach where you first do some preliminary prediction and'}
--------------------------------------------------
26 iuf 2.2677482445081393e-06 another idea which is actually very similar to the idea of idf that we have seen in text research is called the inverse user frequency or iuf 
{'5 - 13 - 4.7 Recommender Systems- Collaborative Filtering - Part 2 (00-12-09).srt : 00:11:10,460 --> 00:11:15,180': 'have seen in text research is called the inverse user frequency or iuf'}
--------------------------------------------------
10 aah 1.2213278741915971e-06 if the item is a popular item that has been aah viewed by many people and seemingly leads to people interested in this item may not be so interesting 
{'5 - 13 - 4.7 Recommender Systems- Collaborative Filtering - Part 2 (00-12-09).srt : 00:11:24,550 --> 00:11:30,020': 'if the item is a popular item that has been aah viewed by many people and'}
--------------------------------------------------
16 seemingly 1.7445380593498679e-06 if the item is a popular item that has been aah viewed by many people and seemingly leads to people interested in this item may not be so interesting 
{'5 - 13 - 4.7 Recommender Systems- Collaborative Filtering - Part 2 (00-12-09).srt : 00:11:30,020 --> 00:11:35,770': 'seemingly leads to people interested in this item may not be so interesting'}
--------------------------------------------------
15 ascii 1.2213278741915971e-06 so as a string we are going to keep all of the spaces and these ascii symbols 
{'2 - 5 - 1.5 Text Representation- Part 1 (00-10-46).srt : 00:01:21,960 --> 00:01:29,480': 'so as a string we are going to keep all of the spaces and these ascii symbols'}
--------------------------------------------------
18 hadnt 1.2213278741915971e-06 but we cant really analyze semantics yet this is the most general way of representing text because we hadnt used this to represent any natural language or text 
{'2 - 5 - 1.5 Text Representation- Part 1 (00-10-46).srt : 00:01:48,800 --> 00:01:53,760': 'hadnt used this to represent any natural language or text'}
--------------------------------------------------
9 round 1.2213278741915971e-06 now if we go further to do in that round of processing we can add a part of these text 
{'2 - 5 - 1.5 Text Representation- Part 1 (00-10-46).srt : 00:04:01,681 --> 00:04:06,445': 'now if we go further to do in that round of processing we can add a part of'}
--------------------------------------------------
25 verbs 2.2677482445081393e-06 now once we do that we can count for example the most frequent nouns or what kind of nouns are associated with what kind of verbs etc 
{'2 - 5 - 1.5 Text Representation- Part 1 (00-10-46).srt : 00:04:13,840 --> 00:04:18,770': 'what kind of nouns are associated with what kind of verbs etc'}
--------------------------------------------------
7 pausing 2.4426557483831942e-06 if we go further then well be pausing the sentence to obtain a syntactic structure 
{'2 - 6 - 1.6 Vector Space Model- Simplest Instantiation (00-17-30).srt : 00:09:07,241 --> 00:09:10,190': 'and perhaps by pausing the lecture ', '2 - 5 - 1.5 Text Representation- Part 1 (00-10-46).srt : 00:05:02,433 --> 00:05:07,360': 'then well be pausing the sentence to obtain a syntactic structure'}
--------------------------------------------------
16 styles 1.7445380593498679e-06 now this of course will further open up more interesting analysis of for example the writing styles or correcting grammar mistakes 
{'2 - 5 - 1.5 Text Representation- Part 1 (00-10-46).srt : 00:05:17,874 --> 00:05:23,660': 'the writing styles or correcting grammar mistakes'}
--------------------------------------------------
18 correcting 1.2213278741915971e-06 now this of course will further open up more interesting analysis of for example the writing styles or correcting grammar mistakes 
{'2 - 5 - 1.5 Text Representation- Part 1 (00-10-46).srt : 00:05:17,874 --> 00:05:23,660': 'the writing styles or correcting grammar mistakes'}
--------------------------------------------------
4 analyse 1.2213278741915971e-06 and we can further analyse their relations 
{'2 - 5 - 1.5 Text Representation- Part 1 (00-10-46).srt : 00:05:38,750 --> 00:05:40,700': 'and we can further analyse their relations'}
--------------------------------------------------
10 recreation 1.4829329667707326e-06 this will add more entities and relations through entity relation recreation 
{'2 - 5 - 1.5 Text Representation- Part 1 (00-10-46).srt : 00:05:46,480 --> 00:05:52,040': 'this will add more entities and relations through entity relation recreation'}
--------------------------------------------------
11 mentioning 2.2677482445081393e-06 or whenever you mention this person you also tend to see mentioning of another person etc 
{'2 - 5 - 1.5 Text Representation- Part 1 (00-10-46).srt : 00:06:09,690 --> 00:06:12,880': 'you also tend to see mentioning of another person etc'}
--------------------------------------------------
14 heard 2.4426557483831942e-06 and it also related to the knowledge graph that some of you may have heard of that google is doing as a more semantic way of representing text data 
{'2 - 5 - 1.5 Text Representation- Part 1 (00-10-46).srt : 00:06:19,972 --> 00:06:25,510': 'and it also related to the knowledge graph that some of you may have heard of', '5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:10:24,130 --> 00:10:28,310': 'if you havent heard of it it is a good step toward this direction'}
--------------------------------------------------
2 intention 1.7445380593498679e-06 what the intention of saying that 
{'2 - 5 - 1.5 Text Representation- Part 1 (00-10-46).srt : 00:07:51,980 --> 00:07:54,020': 'what the intention of saying that '}
--------------------------------------------------
20 tolerate 1.7445380593498679e-06 so if we analyze our text at the levels that are representing deeper analysis of language then we have to tolerate errors 
{'2 - 5 - 1.5 Text Representation- Part 1 (00-10-46).srt : 00:08:33,040 --> 00:08:38,270': 'deeper analysis of language then we have to tolerate errors'}
--------------------------------------------------
2 shadow 2.006143151929004e-06 and doing shadow analysis which is more robust but wouldnt actually give us the necessary deeper representation of knowledge 
{'2 - 5 - 1.5 Text Representation- Part 1 (00-10-46).srt : 00:09:30,930 --> 00:09:35,785': 'and doing shadow analysis which is more robust but'}
--------------------------------------------------
13 annotating 1.7445380593498679e-06 and then humans can guide the computers to do more accurate analysis by annotating more data by providing features to guide machine learning programs to make them work more effectively 
{'2 - 5 - 1.5 Text Representation- Part 1 (00-10-46).srt : 00:10:22,423 --> 00:10:27,032': 'and then humans can guide the computers to do more accurate analysis by annotating'}
--------------------------------------------------
11 cutoffs 1.7445380593498679e-06 well naturally we have to look after the precisionrecall at different cutoffs 
{'3 - 6 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 1 (00-12-51).srt : 00:01:01,097 --> 00:01:07,180': 'well naturally we have to look after the precisionrecall at different cutoffs'}
--------------------------------------------------
5 securely 1.2213278741915971e-06 if we assume the user securely browses the list of results the user would stop at some point and that point would determine the set 
{'3 - 6 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 1 (00-12-51).srt : 00:01:17,640 --> 00:01:21,470': 'right if we assume the user securely browses'}
--------------------------------------------------
6 browses 1.7445380593498679e-06 if we assume the user securely browses the list of results the user would stop at some point and that point would determine the set 
{'3 - 6 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 1 (00-12-51).srt : 00:01:17,640 --> 00:01:21,470': 'right if we assume the user securely browses'}
--------------------------------------------------
15 othe 1.2213278741915971e-06 but as convenience we often assume that the precision is zero at all the the othe the precision are zero at all the other levels of recall that are beyond the search results 
{'3 - 6 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 1 (00-12-51).srt : 00:03:47,230 --> 00:03:51,890': 'at all the the othe the precision are zero at'}
--------------------------------------------------
18 deviates 1.7445380593498679e-06 and this is for the relative comparison so it okay if the actual measure or actual actual number deviates a little bit from the true number 
{'3 - 6 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 1 (00-12-51).srt : 00:04:34,870 --> 00:04:39,560': 'or actual actual number deviates a little bit from the true number'}
--------------------------------------------------
4 underestimated 1.2213278741915971e-06 because this would be underestimated for all the method 
{'3 - 6 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 1 (00-12-51).srt : 00:06:20,430 --> 00:06:24,600': 'because this would be underestimated for all the method'}
--------------------------------------------------
12 crosses 1.7445380593498679e-06 where system a is showing red system b is showing blue there crosses 
{'3 - 6 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 1 (00-12-51).srt : 00:06:40,880 --> 00:06:47,260': 'where system a is showing red system b is showing blue there crosses'}
--------------------------------------------------
4 replacement 1.7445380593498679e-06 if you make the replacement the search engine would behave like system a here whereas if you dont do that it will be like a system b 
{'3 - 6 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 1 (00-12-51).srt : 00:08:23,250 --> 00:08:29,430': 'if you make the replacement the search engine would behave like system a here'}
--------------------------------------------------
18 depict 1.7445380593498679e-06 so this means you may not necessarily be able to come up with one number that would accurately depict the performance 
{'3 - 6 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 1 (00-12-51).srt : 00:10:25,290 --> 00:10:28,810': 'that would accurately depict the performance'}
--------------------------------------------------
15 ours 1.2213278741915971e-06 yet as i said when you have a practical decision to make whether you replace ours with another then you may have to actually come up with a single number to quantify each method 
{'3 - 6 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 1 (00-12-51).srt : 00:10:35,620 --> 00:10:38,210': 'whether you replace ours with another '}
--------------------------------------------------
18 underneath 1.7445380593498679e-06 and one way to summarize this whole ranked list for this whole curve is look at the area underneath the curve 
{'3 - 6 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 1 (00-12-51).srt : 00:11:13,570 --> 00:11:18,090': 'this whole curve is look at the area underneath the curve'}
--------------------------------------------------
7 overlook 2.4426557483831942e-06 it a common mistake that people sometimes overlook 
{'3 - 6 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 1 (00-12-51).srt : 00:12:57,050 --> 00:13:01,300': 'it a common mistake that people sometimes overlook', '3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:01:11,480 --> 00:01:17,790': 'basically you overlook kinds of terms in a small set of documents and and'}
--------------------------------------------------
4 denomina 1.2213278741915971e-06 so note that this denomina denominator is ten the total number of relevant documents 
{'3 - 6 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 1 (00-12-51).srt : 00:13:22,115 --> 00:13:25,862': 'so note that this denomina denominator is ten'}
--------------------------------------------------
20 uhthe 1.2213278741915971e-06 whereas if i move any relevant document down let say i move this relevant document down then it would decrease uhthe average precision 
{'3 - 6 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 1 (00-12-51).srt : 00:14:17,630 --> 00:14:23,720': 'document down then it would decrease uhthe average precision'}
--------------------------------------------------
9 fearful 1.7445380593498679e-06 the six most frequently used categories are happy sad fearful angry surprised and disgusted 
{'5 - 2 - 4.2 Opinion Mining and Sentiment Analysis- Sentiment Classification (00-11-47).srt : 00:01:52,620 --> 00:01:57,150': 'sad fearful angry surprised and disgusted'}
--------------------------------------------------
10 angry 1.7445380593498679e-06 the six most frequently used categories are happy sad fearful angry surprised and disgusted 
{'5 - 2 - 4.2 Opinion Mining and Sentiment Analysis- Sentiment Classification (00-11-47).srt : 00:01:52,620 --> 00:01:57,150': 'sad fearful angry surprised and disgusted'}
--------------------------------------------------
11 surprised 1.7445380593498679e-06 the six most frequently used categories are happy sad fearful angry surprised and disgusted 
{'5 - 2 - 4.2 Opinion Mining and Sentiment Analysis- Sentiment Classification (00-11-47).srt : 00:01:52,620 --> 00:01:57,150': 'sad fearful angry surprised and disgusted'}
--------------------------------------------------
13 disgusted 1.7445380593498679e-06 the six most frequently used categories are happy sad fearful angry surprised and disgusted 
{'5 - 2 - 4.2 Opinion Mining and Sentiment Analysis- Sentiment Classification (00-11-47).srt : 00:01:52,620 --> 00:01:57,150': 'sad fearful angry surprised and disgusted'}
--------------------------------------------------
7 improvements 2.2677482445081393e-06 in particular it needs two kind of improvements 
{'5 - 2 - 4.2 Opinion Mining and Sentiment Analysis- Sentiment Classification (00-11-47).srt : 00:02:29,740 --> 00:02:33,220': 'in particular it needs two kind of improvements'}
--------------------------------------------------
6 spelling 1.7445380593498679e-06 and this is also robust to spelling errors or recognition errors right 
{'5 - 2 - 4.2 Opinion Mining and Sentiment Analysis- Sentiment Classification (00-11-47).srt : 00:03:42,260 --> 00:03:46,430': 'and this is also robust to spelling errors or recognition errors right'}
--------------------------------------------------
13 oriented 1.2213278741915971e-06 but it may cause overfitting because with such very unique features that machine oriented program can easily pick up such features from the training set and to rely on such unique features to distinguish the categories 
{'5 - 2 - 4.2 Opinion Mining and Sentiment Analysis- Sentiment Classification (00-11-47).srt : 00:05:16,770 --> 00:05:21,884': 'oriented program can easily pick up such features from the training set and'}
--------------------------------------------------
20 adjective 2.2677482445081393e-06 we can also consider part of speech tag ngrams if we can do part of speech tagging an for example adjective noun could form a pair 
{'5 - 2 - 4.2 Opinion Mining and Sentiment Analysis- Sentiment Classification (00-11-47).srt : 00:05:43,990 --> 00:05:49,310': 'speech tagging an for example adjective noun could form a pair'}
--------------------------------------------------
7 parodically 1.2213278741915971e-06 we can also learn word clusters and parodically for example weve talked about the mining associations of words 
{'5 - 2 - 4.2 Opinion Mining and Sentiment Analysis- Sentiment Classification (00-11-47).srt : 00:06:31,240 --> 00:06:35,884': 'we can also learn word clusters and parodically for example'}
--------------------------------------------------
11 syntaxmatically 1.2213278741915971e-06 and so we can have cluster of paradigmatically related words or syntaxmatically related words and these clusters can be features to supplement the word base representation 
{'5 - 2 - 4.2 Opinion Mining and Sentiment Analysis- Sentiment Classification (00-11-47).srt : 00:06:43,325 --> 00:06:45,090': 'syntaxmatically related words and '}
--------------------------------------------------
21 supplement 2.2677482445081393e-06 and so we can have cluster of paradigmatically related words or syntaxmatically related words and these clusters can be features to supplement the word base representation 
{'5 - 2 - 4.2 Opinion Mining and Sentiment Analysis- Sentiment Classification (00-11-47).srt : 00:06:45,090 --> 00:06:50,910': 'these clusters can be features to supplement the word base representation'}
--------------------------------------------------
15 overfeeding 1.2213278741915971e-06 so they have a lot of advantages but they might still face the problem of overfeeding as the features become more complex 
{'5 - 2 - 4.2 Opinion Mining and Sentiment Analysis- Sentiment Classification (00-11-47).srt : 00:07:27,244 --> 00:07:31,000': 'of overfeeding as the features become more complex'}
--------------------------------------------------
13 treebased 1.7445380593498679e-06 this is a problem in general and the same is true for parse treebased features when you can use a parse tree to derive features such as frequent subtrees or paths and those are even more discriminating but theyre also are more likely to cause over fitting 
{'5 - 2 - 4.2 Opinion Mining and Sentiment Analysis- Sentiment Classification (00-11-47).srt : 00:07:31,000 --> 00:07:37,500': 'this is a problem in general and the same is true for parse treebased features'}
--------------------------------------------------
28 subtrees 2.2677482445081393e-06 this is a problem in general and the same is true for parse treebased features when you can use a parse tree to derive features such as frequent subtrees or paths and those are even more discriminating but theyre also are more likely to cause over fitting 
{'5 - 2 - 4.2 Opinion Mining and Sentiment Analysis- Sentiment Classification (00-11-47).srt : 00:07:37,500 --> 00:07:42,610': 'when you can use a parse tree to derive features such as frequent subtrees or'}
--------------------------------------------------
10 revised 1.2213278741915971e-06 and so this can lead into feature validation that will revised the feature set and then you can iterate 
{'5 - 2 - 4.2 Opinion Mining and Sentiment Analysis- Sentiment Classification (00-11-47).srt : 00:09:58,177 --> 00:10:01,823': 'revised the feature set and then you can iterate'}
--------------------------------------------------
27 overflow 1.2213278741915971e-06 but be careful not to use a lot of category features because it can cause overfitting or otherwise you would have to training careful not to let overflow happen 
{'5 - 2 - 4.2 Opinion Mining and Sentiment Analysis- Sentiment Classification (00-11-47).srt : 00:10:33,464 --> 00:10:38,401': 'have to training careful not to let overflow happen'}
--------------------------------------------------
0 specifity 1.2213278741915971e-06 specifity requires the feature to be discriminative so naturally infrequent the features tend to be more discriminative 
{'5 - 2 - 4.2 Opinion Mining and Sentiment Analysis- Sentiment Classification (00-11-47).srt : 00:11:04,263 --> 00:11:08,086': 'specifity requires the feature to be discriminative so'}
--------------------------------------------------
19 categoration 1.2213278741915971e-06 and that probably the most important part in machine learning any problem in particularly in our case for text categoration or more specifically the senitment classification 
{'5 - 2 - 4.2 Opinion Mining and Sentiment Analysis- Sentiment Classification (00-11-47).srt : 00:11:27,693 --> 00:11:32,076': 'problem in particularly in our case for text categoration or'}
--------------------------------------------------
24 senitment 1.2213278741915971e-06 and that probably the most important part in machine learning any problem in particularly in our case for text categoration or more specifically the senitment classification 
{'5 - 2 - 4.2 Opinion Mining and Sentiment Analysis- Sentiment Classification (00-11-47).srt : 00:11:32,076 --> 00:11:35,723': 'more specifically the senitment classification'}
--------------------------------------------------
5 apprised 1.2213278741915971e-06 so the tokenizer will be apprised to query as well so that the text can be processed in the same way 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:01:02,092 --> 00:01:05,062': 'so the tokenizer will be apprised to query as well so'}
--------------------------------------------------
33 pixels 2.4426557483831942e-06 and then the user can look at the results and and provide some feedback that can be expressed judgements about which documents are good which documents are bad or implicit feedback such as pixels so the user doesnt have to any anything extra 
{'5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:03:36,837 --> 00:03:41,861': 'the opportunity of learning from a lot of pixels on the web', '3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:01:37,779 --> 00:01:43,140': 'or implicit feedback such as pixels so the user doesnt have to any anything extra'}
--------------------------------------------------
14 responds 1.2213278741915971e-06 the first part is the indexer and the second part is the scorer that responds to the user query 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:02:10,265 --> 00:02:13,105': 'that responds to the user query '}
--------------------------------------------------
21 inverter 1.2213278741915971e-06 now typically the indexer is done in the offline manner so you can preprocess the correct data and to build the inverter index which we will introduce in a moment 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:02:22,630 --> 00:02:28,140': 'the correct data and to build the inverter index which we will introduce in a moment'}
--------------------------------------------------
7 algorithmspecific 1.2213278741915971e-06 so that is usually done in a algorithmspecific way 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:03:01,516 --> 00:03:07,800': 'so that is usually done in a algorithmspecific way'}
--------------------------------------------------
16 inflectional 1.2213278741915971e-06 now in the language of english stemming is often used and this what map all the inflectional forms of words into the same root form 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:03:24,910 --> 00:03:29,710': 'this what map all the inflectional forms of words into the same root form'}
--------------------------------------------------
9 subtlest 1.2213278741915971e-06 but it also not always beneficial because sometimes the subtlest difference between computer and computation might still suggest the difference in the coverage of the content 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:03:56,730 --> 00:04:01,020': 'because sometimes the subtlest difference between computer and'}
--------------------------------------------------
3 ob 1.2213278741915971e-06 because it not ob obvious where the boundary is as there no space separating them 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:04:25,530 --> 00:04:27,680': 'because it not ob '}
--------------------------------------------------
5 precompute 1.2213278741915971e-06 the basic idea is to precompute as much as we can basically 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:04:52,940 --> 00:04:56,710': 'the basic idea is to precompute as much as we can basically'}
--------------------------------------------------
6 preconstruct 2.4426557483831942e-06 in this way you can basically preconstruct the answers 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:06:07,920 --> 00:06:11,960': 'in this way you can basically preconstruct the answers', '3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:06:30,680 --> 00:06:35,490': 'we can do preconstruct such a index '}
--------------------------------------------------
3 preconstruct 2.4426557483831942e-06 we can do preconstruct such a index 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:06:07,920 --> 00:06:11,960': 'in this way you can basically preconstruct the answers', '3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:06:30,680 --> 00:06:35,490': 'we can do preconstruct such a index '}
--------------------------------------------------
15 fre 1.2213278741915971e-06 for example the number of documents that match the term or the total number of fre total frequency of the term which means we would encounter duplicated occurrences of the term 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:07:05,370 --> 00:07:09,060': 'the total number of fre total frequency of the term'}
--------------------------------------------------
8 checking 1.2213278741915971e-06 now the position information is very useful for checking whether the matching of query terms is actually within a small window of let say five words or ten words or whether the matching of the two query terms is in fact a phrase of two words 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:10:40,990 --> 00:10:44,690': 'now the position information is very useful for checking whether'}
--------------------------------------------------
4 checked 1.7445380593498679e-06 this can all be checked quickly by using the position information 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:11:00,700 --> 00:11:04,430': 'this can all be checked quickly by using the position information'}
--------------------------------------------------
14 singleword 1.2213278741915971e-06 well we just talked about the possibility of using the two ends of a singleword query 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:11:13,180 --> 00:11:16,480': 'of using the two ends of a singleword query'}
--------------------------------------------------
3 multipleterm 1.2213278741915971e-06 what about a multipleterm queries 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:11:17,990 --> 00:11:19,910': 'what about a multipleterm queries '}
--------------------------------------------------
5 conjunctive 1.2213278741915971e-06 all right so that one conjunctive query 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:11:35,860 --> 00:11:38,770': 'all right so that one conjunctive query'}
--------------------------------------------------
2 disjunctive 2.4426557483831942e-06 that a disjunctive query 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:11:45,440 --> 00:11:46,540': 'that a disjunctive query ', '3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:12:38,770 --> 00:12:42,636': 'basically it similar to disjunctive boolean query'}
--------------------------------------------------
4 multiterm 1.2213278741915971e-06 now what about the multiterm keyword query 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:12:17,780 --> 00:12:20,840': 'now what about the multiterm keyword query'}
--------------------------------------------------
4 disjunctive 2.4426557483831942e-06 basically it similar to disjunctive boolean query 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:11:45,440 --> 00:11:46,540': 'that a disjunctive query ', '3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:12:38,770 --> 00:12:42,636': 'basically it similar to disjunctive boolean query'}
--------------------------------------------------
3 languagein 1.2213278741915971e-06 there are some languagein independent patterns that seem to be stable 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:13:54,010 --> 00:13:58,720': 'there are some languagein independent patterns that seem to be stable'}
--------------------------------------------------
7 tfi 2.4426557483831942e-06 and they also tend to have high tfi diff weights in these intermediate frequency words 
{'4 - 6 - 3.5 Smoothing Methods Part - 1 (00-09-54).srt : 00:01:11,218 --> 00:01:13,920': 'we have tfi tf weight here ', '3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:16:23,940 --> 00:16:28,842': 'and they also tend to have high tfi diff weights in these intermediate'}
--------------------------------------------------
8 diff 1.2213278741915971e-06 and they also tend to have high tfi diff weights in these intermediate frequency words 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:16:23,940 --> 00:16:28,842': 'and they also tend to have high tfi diff weights in these intermediate'}
--------------------------------------------------
3 stopper 1.2213278741915971e-06 they are usually stopper words the we of et cetera 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:16:39,634 --> 00:16:45,540': 'they are usually stopper words the we of et cetera'}
--------------------------------------------------
9 discriminated 1.2213278741915971e-06 they are in fact a too frequently to be discriminated 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:16:47,680 --> 00:16:50,810': 'they are in fact a too frequently to be discriminated'}
--------------------------------------------------
4 posting 2.4426557483831942e-06 you can imagine the posting entries for such a word would be very long 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:17:17,941 --> 00:17:22,365': 'you can imagine the posting entries for such a word would be very long', '3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:20:32,266 --> 00:20:38,935': 'and so by compressing the inverted index the posting files will become smaller'}
--------------------------------------------------
3 modest 2.4426557483831942e-06 the dictionary has modest size although for the web it still wouldnt be very large 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:18:17,020 --> 00:18:19,870': 'the dictionary has modest size although for', '3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:18:22,120 --> 00:18:24,690': 'but compared with postings it modest '}
--------------------------------------------------
5 modest 2.4426557483831942e-06 but compared with postings it modest 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:18:17,020 --> 00:18:19,870': 'the dictionary has modest size although for', '3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:18:22,120 --> 00:18:24,690': 'but compared with postings it modest '}
--------------------------------------------------
13 visible 2.4426557483831942e-06 or or or if the connection is not very large and this is visible 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:18:39,160 --> 00:18:43,650': 'or or or if the connection is not very large and this is visible', '2 - 13 - 1.13 Syntagmatic Relation Discovery- Mutual Information- Part 2 (00-09-42).srt : 00:07:16,235 --> 00:07:20,762': 'mainly showing that pure statistical approaches are visible'}
--------------------------------------------------
22 btree 1.2213278741915971e-06 so the data structures that we often use for storing dictionary would be direct access data structures like a hash table or btree if we cant store everything in memory of the newest disk 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:19:03,619 --> 00:19:08,348': 'btree if we cant store everything in memory of the newest disk'}
--------------------------------------------------
32 newest 1.2213278741915971e-06 so the data structures that we often use for storing dictionary would be direct access data structures like a hash table or btree if we cant store everything in memory of the newest disk 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:19:03,619 --> 00:19:08,348': 'btree if we cant store everything in memory of the newest disk'}
--------------------------------------------------
8 posting 2.4426557483831942e-06 and so by compressing the inverted index the posting files will become smaller 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:17:17,941 --> 00:17:22,365': 'you can imagine the posting entries for such a word would be very long', '3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:20:32,266 --> 00:20:38,935': 'and so by compressing the inverted index the posting files will become smaller'}
--------------------------------------------------
10 traffic 2.4426557483831942e-06 and then so we we can reduce the amount of traffic and io 
{'2 - 1 - 1.1 Overview Text Mining and Analytics- Part 1 (00-11-43).srt : 00:07:57,140 --> 00:08:02,580': 'a network sends over the monitor network traffic', '3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:20:47,495 --> 00:20:52,601': 'and then so we we can reduce the amount of traffic and io'}
--------------------------------------------------
14 loading 1.2213278741915971e-06 so compression here is both to save disk space and to speed up the loading of the inverted index 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:21:10,868 --> 00:21:14,976': 'to speed up the loading of the inverted index'}
--------------------------------------------------
6 exchanging 2.4426557483831942e-06 so now let talk about the exchanging of plsa to of lda and to motivate that we need to talk about some deficiencies of plsa 
{'5 - 8 - 4.8 Contextual Text Mining- Contextual Probabilistic Latent Semantic Analysis (00-17-59).srt : 00:00:28,990 --> 00:00:32,630': 'as exchanging of pos for doing contextual text mining', '3 - 16 - 2.16 Latent Dirichlet Allocation (LDA)- Part 2 (00-12-03).srt : 00:00:00,025 --> 00:00:05,631': 'sound so now let talk about the exchanging of'}
--------------------------------------------------
22 deficiencies 2.2677482445081393e-06 so now let talk about the exchanging of plsa to of lda and to motivate that we need to talk about some deficiencies of plsa 
{'3 - 16 - 2.16 Latent Dirichlet Allocation (LDA)- Part 2 (00-12-03).srt : 00:00:10,816 --> 00:00:17,145': 'we need to talk about some deficiencies of plsa'}
--------------------------------------------------
4 workaround 1.2213278741915971e-06 and there some heuristic workaround though 
{'3 - 16 - 2.16 Latent Dirichlet Allocation (LDA)- Part 2 (00-12-03).srt : 00:00:34,810 --> 00:00:39,030': 'and there some heuristic workaround though'}
--------------------------------------------------
12 prone 1.7445380593498679e-06 and this also means that there are many local maxima and it prone to overfitting 
{'3 - 16 - 2.16 Latent Dirichlet Allocation (LDA)- Part 2 (00-12-03).srt : 00:00:53,010 --> 00:00:55,090': 'it prone to overfitting '}
--------------------------------------------------
17 hitting 2.2677482445081393e-06 this however is not a necessary problem for text mining because here were often only interested in hitting the training documents that we have 
{'3 - 16 - 2.16 Latent Dirichlet Allocation (LDA)- Part 2 (00-12-03).srt : 00:01:28,530 --> 00:01:32,150': 'only interested in hitting the training documents that we have'}
--------------------------------------------------
3 proposing 1.2213278741915971e-06 so lda is proposing to improve that and basically to make plsa a generative model by imposing a dirichlet prior on the model parameters 
{'3 - 16 - 2.16 Latent Dirichlet Allocation (LDA)- Part 2 (00-12-03).srt : 00:01:42,330 --> 00:01:46,860': 'so lda is proposing to improve that and basically to make'}
--------------------------------------------------
14 elegant 2.4426557483831942e-06 so essentially they are doing something very similar but theoretically lda is a more elegant way of looking at the top and bottom problem 
{'3 - 16 - 2.16 Latent Dirichlet Allocation (LDA)- Part 2 (00-12-03).srt : 00:02:41,770 --> 00:02:48,110': 'lda is a more elegant way of looking at the top and bottom problem', '3 - 15 - 2.15 Latent Dirichlet Allocation (LDA)- Part 1 (00-10-20).srt : 00:02:35,835 --> 00:02:39,410': 'it turns out that there is a very elegant way of doing '}
--------------------------------------------------
3 dropped 1.2213278741915971e-06 only that i dropped the background for simplicity 
{'3 - 16 - 2.16 Latent Dirichlet Allocation (LDA)- Part 2 (00-12-03).srt : 00:06:22,760 --> 00:06:25,820': 'only that i dropped the background for simplicity'}
--------------------------------------------------
9 stressed 1.7445380593498679e-06 so this is a very important formula as ive stressed multiple times 
{'3 - 16 - 2.16 Latent Dirichlet Allocation (LDA)- Part 2 (00-12-03).srt : 00:06:55,180 --> 00:06:59,100': 'so this is a very important formula as ive stressed multiple times'}
--------------------------------------------------
9 integrals 1.7445380593498679e-06 so basically in the area were just adding this integrals to account for the uncertainties and we added of course the dirichlet distributions to cover the choice of this parameters pis and theta 
{'3 - 16 - 2.16 Latent Dirichlet Allocation (LDA)- Part 2 (00-12-03).srt : 00:07:58,760 --> 00:08:03,345': 'so basically in the area were just adding this integrals to account for'}
--------------------------------------------------
4 intractable 2.2677482445081393e-06 unfortunately this computation is intractable 
{'3 - 16 - 2.16 Latent Dirichlet Allocation (LDA)- Part 2 (00-12-03).srt : 00:09:09,700 --> 00:09:13,900': 'unfortunately this computation is intractable'}
--------------------------------------------------
20 kits 2.4426557483831942e-06 and there are many methods available for that and im sure you will see them when you use different tool kits for lda or when you read papers about these different extensions of lda 
{'5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:07:01,390 --> 00:07:07,182': 'more information about resources including readings and tool kits etc', '3 - 16 - 2.16 Latent Dirichlet Allocation (LDA)- Part 2 (00-12-03).srt : 00:09:24,220 --> 00:09:29,100': 'see them when you use different tool kits for lda or when you read papers about'}
--------------------------------------------------
8 instruction 1.4829329667707326e-06 now here we of course cant give indepth instruction to that but just know that they are computed based in inference by using the parameters alphas and betas 
{'3 - 16 - 2.16 Latent Dirichlet Allocation (LDA)- Part 2 (00-12-03).srt : 00:09:35,120 --> 00:09:39,210': 'now here we of course cant give indepth instruction to that but'}
--------------------------------------------------
8 assembly 1.2213278741915971e-06 and especially when we use algorithm called class assembly then the algorithm looks very similar to the algorithm 
{'3 - 16 - 2.16 Latent Dirichlet Allocation (LDA)- Part 2 (00-12-03).srt : 00:09:57,900 --> 00:10:02,720': 'and especially when we use algorithm called class assembly'}
--------------------------------------------------
6 proportions 1.2213278741915971e-06 and were going to also output proportions of these topics covered in each document 
{'3 - 16 - 2.16 Latent Dirichlet Allocation (LDA)- Part 2 (00-12-03).srt : 00:10:32,610 --> 00:10:36,999': 'and were going to also output proportions of these topics covered in each document'}
--------------------------------------------------
4 adequate 2.2677482445081393e-06 and this is often adequate for most applications 
{'3 - 16 - 2.16 Latent Dirichlet Allocation (LDA)- Part 2 (00-12-03).srt : 00:10:45,320 --> 00:10:48,310': 'and this is often adequate for most applications'}
--------------------------------------------------
13 fulfill 1.2213278741915971e-06 and in this case a user typically would use a search engine to fulfill the goal 
{'2 - 2 - 1.2 Text Access (00-09-24).srt : 00:01:47,700 --> 00:01:53,420': 'and in this case a user typically would use a search engine to fulfill the goal'}
--------------------------------------------------
6 satisfying 1.7445380593498679e-06 so this is usually appropriate for satisfying a user ad hoc information need 
{'2 - 2 - 1.2 Text Access (00-09-24).srt : 00:02:05,796 --> 00:02:09,995': 'satisfying a user ad hoc information need'}
--------------------------------------------------
8 purchased 1.7445380593498679e-06 but after you have collected information you have purchased your product you generally no longer need such information 
{'2 - 2 - 1.2 Text Access (00-09-24).srt : 00:02:24,650 --> 00:02:28,770': 'you have purchased your product you generally no longer need such information'}
--------------------------------------------------
1 hobby 1.7445380593498679e-06 your hobby is another example of a stable information need 
{'2 - 2 - 1.2 Text Access (00-09-24).srt : 00:03:26,880 --> 00:03:31,210': 'your hobby is another example of a stable information need'}
--------------------------------------------------
7 advertisement 1.2213278741915971e-06 so that should remind you for example advertisement placed on a search page 
{'2 - 2 - 1.2 Text Access (00-09-24).srt : 00:04:19,550 --> 00:04:24,880': 'so that should remind you for example advertisement placed on a search page'}
--------------------------------------------------
7 inconvenient 1.7445380593498679e-06 or simply because the user finds it inconvenient to type in the query 
{'2 - 2 - 1.2 Text Access (00-09-24).srt : 00:05:59,750 --> 00:06:05,080': 'or simply because the user finds it inconvenient to type in the query'}
--------------------------------------------------
15 sightseeing 1.7445380593498679e-06 the relationship between browsing and the query is best understood by making an analogy to sightseeing 
{'2 - 2 - 1.2 Text Access (00-09-24).srt : 00:06:21,410 --> 00:06:25,230': 'the query is best understood by making an analogy to sightseeing'}
--------------------------------------------------
4 touring 1.2213278741915971e-06 imagine if you are touring a city 
{'2 - 2 - 1.2 Text Access (00-09-24).srt : 00:06:25,230 --> 00:06:26,837': 'imagine if you are touring a city '}
--------------------------------------------------
22 nearby 1.2213278741915971e-06 but if you dont know the exact address you may need to walk around or you can take a taxi to a nearby place and then walk around 
{'2 - 2 - 1.2 Text Access (00-09-24).srt : 00:06:40,464 --> 00:06:43,579': 'you can take a taxi to a nearby place and then walk around'}
--------------------------------------------------
11 land 1.2213278741915971e-06 well your query probably wont work so well and you will land on some related pages and then you need to also walk around in the information space 
{'2 - 2 - 1.2 Text Access (00-09-24).srt : 00:07:02,160 --> 00:07:06,275': 'well your query probably wont work so well and you will land on some related'}
--------------------------------------------------
17 attractions 1.2213278741915971e-06 so just like you are looking around in some area and you want to see some interesting attractions in a related in the same region 
{'2 - 2 - 1.2 Text Access (00-09-24).srt : 00:07:28,050 --> 00:07:33,350': 'you want to see some interesting attractions in a related'}
--------------------------------------------------
24 chicago 2.4426557483831942e-06 and this is because in order to browse effectively we need a a map to guide us just like you need a map of chicago to tour the city of chicago 
{'2 - 2 - 1.2 Text Access (00-09-24).srt : 00:07:56,510 --> 00:08:00,220': 'just like you need a map of chicago to tour the city of chicago'}
--------------------------------------------------
26 tour 2.4426557483831942e-06 and this is because in order to browse effectively we need a a map to guide us just like you need a map of chicago to tour the city of chicago 
{'2 - 2 - 1.2 Text Access (00-09-24).srt : 00:07:56,510 --> 00:08:00,220': 'just like you need a map of chicago to tour the city of chicago', '2 - 2 - 1.2 Text Access (00-09-24).srt : 00:08:00,220 --> 00:08:04,060': 'you need a topical map to tour the information space'}
--------------------------------------------------
30 chicago 2.4426557483831942e-06 and this is because in order to browse effectively we need a a map to guide us just like you need a map of chicago to tour the city of chicago 
{'2 - 2 - 1.2 Text Access (00-09-24).srt : 00:07:56,510 --> 00:08:00,220': 'just like you need a map of chicago to tour the city of chicago'}
--------------------------------------------------
6 tour 2.4426557483831942e-06 you need a topical map to tour the information space 
{'2 - 2 - 1.2 Text Access (00-09-24).srt : 00:07:56,510 --> 00:08:00,220': 'just like you need a map of chicago to tour the city of chicago', '2 - 2 - 1.2 Text Access (00-09-24).srt : 00:08:00,220 --> 00:08:04,060': 'you need a topical map to tour the information space'}
--------------------------------------------------
12 replication 1.2213278741915971e-06 you should realize that nlp is always very important for any text replication because it enriches text representation 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:01:56,840 --> 00:02:01,510': 'any text replication because it enriches text representation'}
--------------------------------------------------
3 estate 1.2213278741915971e-06 however the current estate of art of natural energy processing is still not robust enough 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:02:12,950 --> 00:02:17,510': 'however the current estate of art of natural energy processing is'}
--------------------------------------------------
2 relied 1.2213278741915971e-06 and weve relied a lot on statistical techniques statistical learning techniques particularly 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:02:39,700 --> 00:02:42,478': 'and weve relied a lot on statistical techniques'}
--------------------------------------------------
1 wordassociation 1.2213278741915971e-06 in wordassociation mining and analysis the important points first we are introduced the two concepts for two basic and complementary relations of words paradigmatic and syntagmatic relations 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:02:47,790 --> 00:02:52,771': 'in wordassociation mining and analysis the important points first'}
--------------------------------------------------
15 paradynamic 1.2213278741915971e-06 we also talked a lot about test the similarity then we discuss how to discover paradynamic similarities compare the context of words discover words that share similar context 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:03:29,810 --> 00:03:34,350': 'discuss how to discover paradynamic similarities compare'}
--------------------------------------------------
4 wellconnected 1.2213278741915971e-06 and this part is wellconnected to text retrieval 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:03:55,193 --> 00:03:59,480': 'and this part is wellconnected to text retrieval'}
--------------------------------------------------
4 wilson 1.2213278741915971e-06 in this part of wilson videos is some general concepts that would be useful to know one is generative model and this is a general method for modeling text data and modeling other kinds of data as well 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:05:11,520 --> 00:05:15,930': 'in this part of wilson videos is some general concepts that would be useful to'}
--------------------------------------------------
7 erase 1.2213278741915971e-06 and we talked about the maximum life erase data the em algorithm for solving the problem of computing maximum estimator 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:05:24,740 --> 00:05:30,250': 'and we talked about the maximum life erase data the em algorithm for'}
--------------------------------------------------
14 cuss 1.2213278741915971e-06 and we then also prefer to view the similarity based approaches to test for cuss word 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:06:07,060 --> 00:06:10,000': 'approaches to test for cuss word '}
--------------------------------------------------
12 capitalization 2.4426557483831942e-06 this is the practical use for technique for a lot of text capitalization tasks 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:06:29,280 --> 00:06:36,160': 'this is the practical use for technique for a lot of text capitalization tasks', '5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:06:49,030 --> 00:06:50,490': 'text capitalization as well '}
--------------------------------------------------
14 sbn 1.2213278741915971e-06 we also introduce the some discriminative classifiers particularly logistical regression can nearest labor and sbn 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:06:41,010 --> 00:06:45,300': 'particularly logistical regression can nearest labor and sbn'}
--------------------------------------------------
14 capitalization 2.4426557483831942e-06 they also very important they are very popular they are very useful for text capitalization as well 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:06:29,280 --> 00:06:36,160': 'this is the practical use for technique for a lot of text capitalization tasks', '5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:06:49,030 --> 00:06:50,490': 'text capitalization as well '}
--------------------------------------------------
15 volatility 1.2213278741915971e-06 evaluation is quite important because if the matches that you use dont really reflect the volatility of the method then it would give you misleading results so its very important to get the variation right 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:07:03,110 --> 00:07:07,430': 'reflect the volatility of the method then it would give you misleading results so'}
--------------------------------------------------
24 misleading 2.4426557483831942e-06 evaluation is quite important because if the matches that you use dont really reflect the volatility of the method then it would give you misleading results so its very important to get the variation right 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:07:03,110 --> 00:07:07,430': 'reflect the volatility of the method then it would give you misleading results so', '4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:08:07,200 --> 00:08:10,120': 'if you dont get it right you might get misleading results'}
--------------------------------------------------
8 recalculation 2.4426557483831942e-06 and although it a special case of text recalculation but we talked about how to extend or improve the text recalculation method by using more sophisticated features that would be needed for sentiment analysis 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:07:25,053 --> 00:07:29,681': 'and although it a special case of text recalculation but', '5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:07:29,681 --> 00:07:34,932': 'we talked about how to extend or improve the text recalculation method'}
--------------------------------------------------
20 recalculation 2.4426557483831942e-06 and although it a special case of text recalculation but we talked about how to extend or improve the text recalculation method by using more sophisticated features that would be needed for sentiment analysis 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:07:25,053 --> 00:07:29,681': 'and although it a special case of text recalculation but', '5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:07:29,681 --> 00:07:34,932': 'we talked about how to extend or improve the text recalculation method'}
--------------------------------------------------
9 laying 1.2213278741915971e-06 and it also allows us to infer the viewers laying their weights on these aspects or which aspects are more important to a viewer can be revealed as well 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:08:26,781 --> 00:08:30,638': 'the viewers laying their weights on these aspects or'}
--------------------------------------------------
23 viewer 2.4426557483831942e-06 and it also allows us to infer the viewers laying their weights on these aspects or which aspects are more important to a viewer can be revealed as well 
{'5 - 1 - 4.1 Opinion Mining and Sentiment Analysis- Motivation (00-17-51).srt : 00:00:50,880 --> 00:00:56,670': 'happening in the real world objective to generate the viewer data for example', '5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:08:30,638 --> 00:08:35,740': 'which aspects are more important to a viewer can be revealed as well'}
--------------------------------------------------
26 revealed 1.7445380593498679e-06 and it also allows us to infer the viewers laying their weights on these aspects or which aspects are more important to a viewer can be revealed as well 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:08:30,638 --> 00:08:35,740': 'which aspects are more important to a viewer can be revealed as well'}
--------------------------------------------------
7 generalizing 2.006143151929004e-06 we introduced the contextual plsa as a generalizing or generalized model of plsa to allows us to incorporate the context of variables such as time and location 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:09:04,565 --> 00:09:08,921': 'we introduced the contextual plsa as a generalizing or generalized model of plsa'}
--------------------------------------------------
23 puppets 1.2213278741915971e-06 we also introduced the net plsa in this case we used social network or network in general of text data to help analyze puppets 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:09:24,750 --> 00:09:30,550': 'network in general of text data to help analyze puppets'}
--------------------------------------------------
14 lam 1.2213278741915971e-06 now in the other way of using text to help interpret patterns discovered from lam text data we did not really discuss anything in detail but just provide a reference but i should stress that that after a very important direction to know about if you want to build a practical text mining systems because understanding and interpreting patterns is quite important 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:09:47,990 --> 00:09:51,470': 'help interpret patterns discovered from lam text data'}
--------------------------------------------------
21 allowance 1.2213278741915971e-06 and this should provide a good basis for you to read from your research papers to know more about more of allowance for other organisms or to invent new hours in yourself 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:10:31,100 --> 00:10:33,580': 'to know more about more of allowance for '}
--------------------------------------------------
24 organisms 1.2213278741915971e-06 and this should provide a good basis for you to read from your research papers to know more about more of allowance for other organisms or to invent new hours in yourself 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:10:33,580 --> 00:10:37,320': 'other organisms or to invent new hours in yourself'}
--------------------------------------------------
27 invent 2.4426557483831942e-06 and this should provide a good basis for you to read from your research papers to know more about more of allowance for other organisms or to invent new hours in yourself 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:06:02,020 --> 00:06:05,340': 'there no need to invent a different word for different meanings', '5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:10:33,580 --> 00:10:37,320': 'other organisms or to invent new hours in yourself'}
--------------------------------------------------
6 backbone 2.4426557483831942e-06 and these techniques are now the backbone techniques for not just text analysis applications but also for nlp 
{'2 - 4 - 1.4 Natural Language Content Analysis- Part 2 (00-04-25).srt : 00:02:21,880 --> 00:02:29,150': 'with the general statistical and methods as a backbone as the basis', '5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:11:41,120 --> 00:11:45,090': 'and these techniques are now the backbone techniques for'}
--------------------------------------------------
6 nowadays 1.7445380593498679e-06 a lot of nlp techniques are nowadays actually based on supervised machinery 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:11:49,970 --> 00:11:55,310': 'a lot of nlp techniques are nowadays actually based on supervised machinery'}
--------------------------------------------------
9 werent 2.4426557483831942e-06 so that one example of inaudible techniques that we werent able to cover but that also very important 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:12:34,330 --> 00:12:38,320': 'so that one example of inaudible techniques that we werent able to cover', '5 - 8 - 4.4 Learning to Rank - Part 3 (00-04-58).srt : 00:03:01,330 --> 00:03:04,250': 'such data werent available before '}
--------------------------------------------------
6 emerged 1.2213278741915971e-06 and the other area that has emerged in status learning is the water and baring technique where they can learn better recognition of words 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:12:41,390 --> 00:12:45,400': 'and the other area that has emerged in status learning is the water and'}
--------------------------------------------------
14 baring 1.2213278741915971e-06 and the other area that has emerged in status learning is the water and baring technique where they can learn better recognition of words 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:12:45,400 --> 00:12:50,720': 'baring technique where they can learn better recognition of words'}
--------------------------------------------------
4 recognitions 1.2213278741915971e-06 and then these better recognitions will allow you confuse similarity of words 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:12:50,720 --> 00:12:55,210': 'and then these better recognitions will allow you confuse similarity of words'}
--------------------------------------------------
8 confuse 1.2213278741915971e-06 and then these better recognitions will allow you confuse similarity of words 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:12:50,720 --> 00:12:55,210': 'and then these better recognitions will allow you confuse similarity of words'}
--------------------------------------------------
10 impressive 1.2213278741915971e-06 and results that people have got so far are very impressive 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:13:01,230 --> 00:13:06,600': 'and results that people have got so far are very impressive'}
--------------------------------------------------
21 providence 2.4426557483831942e-06 so the other is to provide a way to annotate it to explain parents and this has to do with knowledge providence 
{'5 - 15 - 4.8 Course Summary (00-09-48).srt : 00:08:13,727 --> 00:08:18,338': 'provide providence and to help users interpret the inner', '5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:15:47,901 --> 00:15:51,521': 'and this has to do with knowledge providence'}
--------------------------------------------------
25 semester 1.2213278741915971e-06 so finally i want to remind you of this big picture for harnessing big text data that i showed you at your beginning of the semester 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:16:39,830 --> 00:16:43,900': 'harnessing big text data that i showed you at your beginning of the semester'}
--------------------------------------------------
25 channels 1.2213278741915971e-06 as you see from our discussions there are a lot of opportunities for this kind of techniques and there are also a lot of open channels 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:18:02,185 --> 00:18:06,235': 'this kind of techniques and there are also a lot of open channels'}
--------------------------------------------------
20 society 1.7445380593498679e-06 so i hope you can use what you have learned to build a lot of use for applications will benefit society and to also join the research community to discover new techniques for text mining and benefits 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:18:10,910 --> 00:18:15,550': 'applications will benefit society and to also join'}
--------------------------------------------------
8 faced 1.2213278741915971e-06 so once we estimated such a model we faced a problem of deciding which cluster document d should belong to 
{'4 - 9 - 3.9 Text Categorization- Generative Probabilistic Models (00-31-18).srt : 00:01:52,080 --> 00:01:58,380': 'we faced a problem of deciding which cluster document d should belong to'}
--------------------------------------------------
13 zeta 1.2213278741915971e-06 now how can you compute the probability that a particular topic word distribution zeta i has been used to generate this document 
{'4 - 9 - 3.9 Text Categorization- Generative Probabilistic Models (00-31-18).srt : 00:02:21,220 --> 00:02:25,960': 'topic word distribution zeta i has been used to generate this document'}
--------------------------------------------------
2 rigorously 1.7445380593498679e-06 so more rigorously this is what wed be doing 
{'4 - 9 - 3.9 Text Categorization- Generative Probabilistic Models (00-31-18).srt : 00:03:14,650 --> 00:03:18,740': 'so more rigorously this is what wed be doing'}
--------------------------------------------------
8 thru 1.2213278741915971e-06 so one way to solve the problem is thru take logarithm of this function which it doesnt changes all the often these categories 
{'4 - 9 - 3.9 Text Categorization- Generative Probabilistic Models (00-31-18).srt : 00:07:06,480 --> 00:07:10,269': 'so one way to solve the problem is thru take logarithm of this function'}
--------------------------------------------------
13 understandable 1.2213278741915971e-06 so this is called an naive bayes classifier now the keyword base is understandable because we are applying a base rule here when we go from the posterior probability of the topic to a product of the likelihood and the prior 
{'4 - 9 - 3.9 Text Categorization- Generative Probabilistic Models (00-31-18).srt : 00:07:34,870 --> 00:07:39,535': 'understandable because we are applying a base rule here when we go from'}
--------------------------------------------------
4 modal 1.2213278741915971e-06 it no longer mixed modal why 
{'4 - 9 - 3.9 Text Categorization- Generative Probabilistic Models (00-31-18).srt : 00:10:18,872 --> 00:10:20,452': 'it no longer mixed modal why '}
--------------------------------------------------
9 irrelevant 1.7445380593498679e-06 so once we know that it in some sense irrelevant of what other categories we are also dealing with 
{'4 - 9 - 3.9 Text Categorization- Generative Probabilistic Models (00-31-18).srt : 00:11:30,825 --> 00:11:35,660': 'it in some sense irrelevant of what other categories we are also dealing with'}
--------------------------------------------------
3 bases 1.2213278741915971e-06 first what the bases for estimating the prior or the probability of each category 
{'4 - 9 - 3.9 Text Categorization- Generative Probabilistic Models (00-31-18).srt : 00:13:29,400 --> 00:13:35,880': 'first what the bases for estimating the prior or the probability of each category'}
--------------------------------------------------
13 intercept 1.2213278741915971e-06 in other words we make this probability proportional to the size of training intercept in each category that a size of the set t sub i 
{'4 - 9 - 3.9 Text Categorization- Generative Probabilistic Models (00-31-18).srt : 00:14:46,929 --> 00:14:52,960': 'of training intercept in each category that a size of the set t sub i'}
--------------------------------------------------
9 probable 1.7445380593498679e-06 now you may notice that we often write down probable estimate in the form of being proportional for certain numbers 
{'4 - 9 - 3.9 Text Categorization- Generative Probabilistic Models (00-31-18).srt : 00:15:49,710 --> 00:15:55,110': 'now you may notice that we often write down probable'}
--------------------------------------------------
13 morals 2.4426557483831942e-06 in fact the smoothing is a general problem in older estimate of language morals 
{'4 - 11 - 3.11 Text Categorization- Discriminative Classifier Part 2 (00-31-46).srt : 00:24:44,820 --> 00:24:48,220': 'so talking morals like psa or ', '4 - 9 - 3.9 Text Categorization- Generative Probabilistic Models (00-31-18).srt : 00:16:35,940 --> 00:16:41,720': 'in fact the smoothing is a general problem in older estimate of language morals'}
--------------------------------------------------
9 outsmarts 1.2213278741915971e-06 so smoothing is an important technique to address that outsmarts this 
{'4 - 9 - 3.9 Text Categorization- Generative Probabilistic Models (00-31-18).srt : 00:16:47,540 --> 00:16:51,590': 'so smoothing is an important technique to address that outsmarts this'}
--------------------------------------------------
13 seudocount 1.2213278741915971e-06 but in this case sometimes we find it useful to use a nonuniform seudocount for the word 
{'4 - 9 - 3.9 Text Categorization- Generative Probabilistic Models (00-31-18).srt : 00:20:01,670 --> 00:20:05,750': 'to use a nonuniform seudocount for the word'}
--------------------------------------------------
13 mule 1.2213278741915971e-06 so here youll see well add a pseudocounts to each word and that mule multiplied by the probability of the word given by a background language model theta sub b 
{'4 - 9 - 3.9 Text Categorization- Generative Probabilistic Models (00-31-18).srt : 00:20:09,372 --> 00:20:12,143': 'that mule multiplied by the probability of'}
--------------------------------------------------
12 nave 2.4426557483831942e-06 and this is estimated as the log of probability ratio here in nave bayes 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:10:38,080 --> 00:10:43,470': 'one example is nave bayes classifier in this case', '4 - 9 - 3.9 Text Categorization- Generative Probabilistic Models (00-31-18).srt : 00:28:24,815 --> 00:28:30,465': 'and this is estimated as the log of probability ratio here in nave bayes'}
--------------------------------------------------
4 generalisation 1.2213278741915971e-06 now we do this generalisation what we see is that in general we can represent the document by feature vector fi here of course in this case fi is the count of a word 
{'4 - 9 - 3.9 Text Categorization- Generative Probabilistic Models (00-31-18).srt : 00:28:58,269 --> 00:29:03,154': 'now we do this generalisation what we see is that in'}
--------------------------------------------------
5 font 1.2213278741915971e-06 for example document length or font size or count of other patterns in the document 
{'4 - 9 - 3.9 Text Categorization- Generative Probabilistic Models (00-31-18).srt : 00:29:20,650 --> 00:29:25,720': 'font size or count of other patterns in the document'}
--------------------------------------------------
19 origin 1.7445380593498679e-06 so that would mean for example these words the is and we are known to be from this background origin distribution 
{'3 - 10 - 2.10 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1 (00-11-05).srt : 00:01:41,160 --> 00:01:44,700': 'we are known to be from this background origin distribution'}
--------------------------------------------------
11 mystery 2.4426557483831942e-06 and this is in fact making this model no longer a mystery model because we can already observe which of these distribution has been used to generate which part of the data 
{'3 - 10 - 2.10 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1 (00-11-05).srt : 00:02:36,470 --> 00:02:40,770': 'making this model no longer a mystery model because we can already observe which', '3 - 10 - 2.10 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1 (00-11-05).srt : 00:03:50,450 --> 00:03:53,280': 'so now all the parameters are known for this mystery model'}
--------------------------------------------------
25 mystery 2.4426557483831942e-06 so let assume that we actually know tentative probabilities for these words in theta sub d so now all the parameters are known for this mystery model 
{'3 - 10 - 2.10 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1 (00-11-05).srt : 00:02:36,470 --> 00:02:40,770': 'making this model no longer a mystery model because we can already observe which', '3 - 10 - 2.10 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1 (00-11-05).srt : 00:03:50,450 --> 00:03:53,280': 'so now all the parameters are known for this mystery model'}
--------------------------------------------------
8 basing 1.2213278741915971e-06 now this inference process is a typical of basing an inference situation where we have some prior about these two distributions 
{'3 - 10 - 2.10 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1 (00-11-05).srt : 00:04:15,050 --> 00:04:19,980': 'now this inference process is a typical of basing an inference situation'}
--------------------------------------------------
5 pry 2.4426557483831942e-06 so this is called a pry because this is our guess of which distribution has been used to generate the word 
{'3 - 10 - 2.10 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1 (00-11-05).srt : 00:04:48,370 --> 00:04:52,020': 'so this is called a pry because this is our guess', '3 - 10 - 2.10 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1 (00-11-05).srt : 00:04:57,950 --> 00:05:01,930': 'so that why we call it a pry '}
--------------------------------------------------
7 pry 2.4426557483831942e-06 so that why we call it a pry 
{'3 - 10 - 2.10 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1 (00-11-05).srt : 00:04:48,370 --> 00:04:52,020': 'so this is called a pry because this is our guess', '3 - 10 - 2.10 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1 (00-11-05).srt : 00:04:57,950 --> 00:05:01,930': 'so that why we call it a pry '}
--------------------------------------------------
13 csubd 2.4426557483831942e-06 now if youre like many others you would guess text is probably from csubd it more likely from csubd why 
{'3 - 10 - 2.10 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1 (00-11-05).srt : 00:06:17,460 --> 00:06:22,690': 'from csubd it more likely from csubd why'}
--------------------------------------------------
18 csubd 2.4426557483831942e-06 now if youre like many others you would guess text is probably from csubd it more likely from csubd why 
{'3 - 10 - 2.10 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1 (00-11-05).srt : 00:06:17,460 --> 00:06:22,690': 'from csubd it more likely from csubd why'}
--------------------------------------------------
23 distributing 2.4426557483831942e-06 and by this were going to say well text is more likely from theta sub d so you see our guess of which distributing has been used with the generated text would depend on how high the probability of the data the text is in each word distribution 
{'3 - 10 - 2.10 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1 (00-11-05).srt : 00:06:44,975 --> 00:06:49,497': 'so you see our guess of which distributing has been used with', '3 - 10 - 2.10 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1 (00-11-05).srt : 00:10:17,490 --> 00:10:20,260': 'all which distributing has been used to generate which word'}
--------------------------------------------------
14 gas 2.2677482445081393e-06 now if we have that kind of strong prior then that would affect your gas 
{'3 - 10 - 2.10 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1 (00-11-05).srt : 00:07:44,517 --> 00:07:49,500': 'now if we have that kind of strong prior then that would affect your gas'}
--------------------------------------------------
8 texter 1.2213278741915971e-06 you might think well wait a moment maybe texter could have been from the background as well 
{'3 - 10 - 2.10 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1 (00-11-05).srt : 00:07:50,290 --> 00:07:55,080': 'well wait a moment maybe texter could have been from the background as well'}
--------------------------------------------------
15 distributing 2.4426557483831942e-06 and then were going to take a guess of these z values and all which distributing has been used to generate which word 
{'3 - 10 - 2.10 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1 (00-11-05).srt : 00:06:44,975 --> 00:06:49,497': 'so you see our guess of which distributing has been used with', '3 - 10 - 2.10 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 1 (00-11-05).srt : 00:10:17,490 --> 00:10:20,260': 'all which distributing has been used to generate which word'}
--------------------------------------------------
6 rankings 2.2677482445081393e-06 that is they would give different rankings of those methods 
{'3 - 7 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01) .srt : 00:02:43,100 --> 00:02:45,920': 'that is they would give different rankings of those methods'}
--------------------------------------------------
4 becoming 1.2213278741915971e-06 this makes the question becoming even more important 
{'3 - 7 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01) .srt : 00:02:57,460 --> 00:03:00,379': 'this makes the question becoming even more important'}
--------------------------------------------------
8 dominating 2.4426557483831942e-06 youll realize in arithmetic mean the sum is dominating by large values 
{'2 - 9 - 1.9 Paradigmatic Relation Discovery Part 2 (00-17-53).srt : 00:04:42,500 --> 00:04:46,335': 'that kind of terms from dominating the scoring function', '3 - 7 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01) .srt : 00:03:12,750 --> 00:03:18,510': 'youll realize in arithmetic mean the sum is dominating by large values'}
--------------------------------------------------
13 pref 1.2213278741915971e-06 so again the answer depends on your users your users tasks and their pref their preferences 
{'3 - 7 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01) .srt : 00:04:05,150 --> 00:04:06,710': 'their pref their preferences '}
--------------------------------------------------
13 homepage 1.2213278741915971e-06 where you know a target page let say you have to find amazon homepage 
{'3 - 7 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01) .srt : 00:04:47,210 --> 00:04:52,670': 'where you know a target page let say you have to find amazon homepage'}
--------------------------------------------------
13 boil 1.7445380593498679e-06 so in this case you can easily verify the average position will basically boil down to reciprocal rank 
{'3 - 7 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01) .srt : 00:05:16,480 --> 00:05:21,710': 'will basically boil down to reciprocal rank'}
--------------------------------------------------
7 oath 1.2213278741915971e-06 and would would this difference change the oath of systems 
{'3 - 7 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 2 (00-10-01) .srt : 00:07:41,730 --> 00:07:46,050': 'and would would this difference change the oath of systems'}
--------------------------------------------------
6 entrophy 2.4426557483831942e-06 and if we frame this using entrophy that would mean we are interested in knowing whether knowing the presence of eats could reduce uncertainty about the meats 
{'2 - 11 - 1.11 Syntagmatic Relation Discovery- Conditional Entropy (00-11-57).srt : 00:01:02,020 --> 00:01:05,040': 'and if we frame this using entrophy ', '2 - 11 - 1.11 Syntagmatic Relation Discovery- Conditional Entropy (00-11-57).srt : 00:01:15,100 --> 00:01:18,800': 'or reduce the entrophy of the random variable'}
--------------------------------------------------
3 entrophy 2.4426557483831942e-06 or reduce the entrophy of the random variable corresponding to the presence or absence of meat 
{'2 - 11 - 1.11 Syntagmatic Relation Discovery- Conditional Entropy (00-11-57).srt : 00:01:02,020 --> 00:01:05,040': 'and if we frame this using entrophy ', '2 - 11 - 1.11 Syntagmatic Relation Discovery- Conditional Entropy (00-11-57).srt : 00:01:15,100 --> 00:01:18,800': 'or reduce the entrophy of the random variable'}
--------------------------------------------------
13 absents 1.2213278741915971e-06 we can also ask as a question what if we know of the absents of eats 
{'2 - 11 - 1.11 Syntagmatic Relation Discovery- Conditional Entropy (00-11-57).srt : 00:01:23,430 --> 00:01:27,950': 'we can also ask as a question what if we know of the absents of eats'}
--------------------------------------------------
11 conditioning 1.2213278741915971e-06 these questions can be addressed by using another concept called a conditioning entropy 
{'2 - 11 - 1.11 Syntagmatic Relation Discovery- Conditional Entropy (00-11-57).srt : 00:01:39,415 --> 00:01:43,120': 'concept called a conditioning entropy '}
--------------------------------------------------
8 incident 1.2213278741915971e-06 and of course this is  because there no incident anymore 
{'2 - 11 - 1.11 Syntagmatic Relation Discovery- Conditional Entropy (00-11-57).srt : 00:06:47,717 --> 00:06:52,518': 'and of course this is because there no incident anymore'}
--------------------------------------------------
9 anymore 1.2213278741915971e-06 and of course this is  because there no incident anymore 
{'2 - 11 - 1.11 Syntagmatic Relation Discovery- Conditional Entropy (00-11-57).srt : 00:06:47,717 --> 00:06:52,518': 'and of course this is because there no incident anymore'}
--------------------------------------------------
1 doubt 2.4426557483831942e-06 no doubt smaller entropy means easier for prediction 
{'4 - 8 - 3.6 Feedback in Text Retrieval (00-06-49).srt : 00:04:11,420 --> 00:04:18,378': 'so this is factored for improving the search doubt', '2 - 11 - 1.11 Syntagmatic Relation Discovery- Conditional Entropy (00-11-57).srt : 00:07:22,870 --> 00:07:27,763': 'no doubt smaller entropy means easier for prediction'}
--------------------------------------------------
2 stays 2.2677482445081393e-06 so it stays fairly close to the original entropy of meat 
{'2 - 11 - 1.11 Syntagmatic Relation Discovery- Conditional Entropy (00-11-57).srt : 00:07:51,520 --> 00:07:56,465': 'so it stays fairly close to the original entropy of meat'}
--------------------------------------------------
4 stuff 2.4426557483831942e-06 well that when this stuff is not really related to meat 
{'4 - 3 - 3.3 Query Likelihood Retrieval Function (00-12-07).srt : 00:11:45,670 --> 00:11:50,436': 'here are different ways to estimate this stuff in the language model', '2 - 11 - 1.11 Syntagmatic Relation Discovery- Conditional Entropy (00-11-57).srt : 00:08:45,300 --> 00:08:49,885': 'well that when this stuff is not really related to meat'}
--------------------------------------------------
7 ascending 1.7445380593498679e-06 we thought all the candidate was in ascending order of the conditional entropy because were out of favor a world that has a small entropy 
{'2 - 11 - 1.11 Syntagmatic Relation Discovery- Conditional Entropy (00-11-57).srt : 00:09:22,170 --> 00:09:26,630': 'we thought all the candidate was in ascending order of the conditional entropy'}
--------------------------------------------------
8 ring 1.2213278741915971e-06 and then were going to take the top ring of the candidate words as words that have potential syntagmatic relations with w 
{'2 - 11 - 1.11 Syntagmatic Relation Discovery- Conditional Entropy (00-11-57).srt : 00:09:34,637 --> 00:09:38,378': 'and then were going to take the top ring of the candidate words as words that have'}
--------------------------------------------------
1 stresser 1.2213278741915971e-06 the stresser can be the number of top candidates take or absolute value for the conditional entropy 
{'2 - 11 - 1.11 Syntagmatic Relation Discovery- Conditional Entropy (00-11-57).srt : 00:09:47,700 --> 00:09:51,474': 'the stresser can be the number of top candidates take or'}
--------------------------------------------------
8 outer 2.4426557483831942e-06 well that was because they have a different outer bounds 
{'2 - 11 - 1.11 Syntagmatic Relation Discovery- Conditional Entropy (00-11-57).srt : 00:11:19,870 --> 00:11:23,210': 'well that was because they have a different outer bounds', '2 - 11 - 1.11 Syntagmatic Relation Discovery- Conditional Entropy (00-11-57).srt : 00:11:23,210 --> 00:11:25,690': 'right so those outer bounds are precisely'}
--------------------------------------------------
2 outer 2.4426557483831942e-06 so those outer bounds are precisely the entropy of w and the entropy of w 
{'2 - 11 - 1.11 Syntagmatic Relation Discovery- Conditional Entropy (00-11-57).srt : 00:11:19,870 --> 00:11:23,210': 'well that was because they have a different outer bounds', '2 - 11 - 1.11 Syntagmatic Relation Discovery- Conditional Entropy (00-11-57).srt : 00:11:23,210 --> 00:11:25,690': 'right so those outer bounds are precisely'}
--------------------------------------------------
21 manipulate 1.7445380593498679e-06 this would give us a more robust way to rank the pages making it the harder for any spammer to just manipulate the one signal to improve the ranking of a page 
{'5 - 3 - 4.3 Link Analysis - Part 1 (00-09-16).srt : 00:02:03,570 --> 00:02:09,350': 'any spammer to just manipulate the one signal to improve the ranking of a page'}
--------------------------------------------------
19 queried 1.7445380593498679e-06 and many of them are based on the standard original models such as bm that we talked about or queried icode to score different parts of documents or to provide additional features based on content matching 
{'5 - 3 - 4.3 Link Analysis - Part 1 (00-09-16).srt : 00:02:57,152 --> 00:03:02,845': 'queried icode to score different parts of documents or'}
--------------------------------------------------
20 icode 1.2213278741915971e-06 and many of them are based on the standard original models such as bm that we talked about or queried icode to score different parts of documents or to provide additional features based on content matching 
{'5 - 3 - 4.3 Link Analysis - Part 1 (00-09-16).srt : 00:02:57,152 --> 00:03:02,845': 'queried icode to score different parts of documents or'}
--------------------------------------------------
4 snapshot 2.2677482445081393e-06 so this is a snapshot of some part of the web let say 
{'5 - 3 - 4.3 Link Analysis - Part 1 (00-09-16).srt : 00:03:21,080 --> 00:03:26,440': 'so this is a snapshot of some part of the web let say'}
--------------------------------------------------
7 bookmark 1.7445380593498679e-06 so for example if someone wants to bookmark amazoncom front page the person might say the big online bookstore and then with a link to amazon right 
{'5 - 3 - 4.3 Link Analysis - Part 1 (00-09-16).srt : 00:03:53,855 --> 00:03:59,695': 'so for example if someone wants to bookmark amazoncom front page'}
--------------------------------------------------
8 amazoncom 1.7445380593498679e-06 so for example if someone wants to bookmark amazoncom front page the person might say the big online bookstore and then with a link to amazon right 
{'5 - 3 - 4.3 Link Analysis - Part 1 (00-09-16).srt : 00:03:53,855 --> 00:03:59,695': 'so for example if someone wants to bookmark amazoncom front page'}
--------------------------------------------------
4 theater 1.2213278741915971e-06 so this is a theater page that would allow you to actually see a lot of other pages 
{'5 - 3 - 4.3 Link Analysis - Part 1 (00-09-16).srt : 00:05:25,920 --> 00:05:29,050': 'so this is a theater page that would allow you to'}
--------------------------------------------------
5 propose 1.2213278741915971e-06 so people then of course propose ideas to leverage this this link information 
{'5 - 3 - 4.3 Link Analysis - Part 1 (00-09-16).srt : 00:05:55,820 --> 00:06:02,610': 'so people then of course propose ideas to leverage this this link information'}
--------------------------------------------------
7 citing 1.7445380593498679e-06 this is very similar to one paper citing another paper 
{'5 - 3 - 4.3 Link Analysis - Part 1 (00-09-16).srt : 00:06:24,010 --> 00:06:27,440': 'this is very similar to one paper citing another paper'}
--------------------------------------------------
15 workshop 1.4829329667707326e-06 if you are cited by let say ten papers and those ten papers are just workshop papers and that or some papers that are not very influential right so although you got ten in links that not as good as if you have youre cited by ten papers that themselves have attracted a lot of other citations 
{'5 - 3 - 4.3 Link Analysis - Part 1 (00-09-16).srt : 00:07:42,860 --> 00:07:48,450': 'workshop papers and that or some papers that are not very influential right'}
--------------------------------------------------
26 influential 2.2677482445081393e-06 if you are cited by let say ten papers and those ten papers are just workshop papers and that or some papers that are not very influential right so although you got ten in links that not as good as if you have youre cited by ten papers that themselves have attracted a lot of other citations 
{'5 - 3 - 4.3 Link Analysis - Part 1 (00-09-16).srt : 00:07:42,860 --> 00:07:48,450': 'workshop papers and that or some papers that are not very influential right'}
--------------------------------------------------
18 elegantly 1.7445380593498679e-06 the the reason why they want to do that is this would allow them to solve the problem elegantly with linear algebra technique 
{'5 - 3 - 4.3 Link Analysis - Part 1 (00-09-16).srt : 00:08:39,470 --> 00:08:45,330': 'to solve the problem elegantly with linear algebra technique'}
--------------------------------------------------
6 mininal 1.2213278741915971e-06 or we can have any other mininal categories associate with text data as long as there is minimal connection between the entity and text data 
{'4 - 7 - 3.7 Text Categorization- Motivation (00-14-37).srt : 00:04:33,540 --> 00:04:38,048': 'or we can have any other mininal categories'}
--------------------------------------------------
3 restaurants 1.2213278741915971e-06 we can categorize restaurants or categorize products based on their corresponding reviews 
{'4 - 7 - 3.7 Text Categorization- Motivation (00-14-37).srt : 00:05:07,770 --> 00:05:09,921': 'we can categorize restaurants or '}
--------------------------------------------------
1 agencies 2.4426557483831942e-06 news agencies would like to assign predefined categories to categorize news generated everyday 
{'1 - 2 - Course Introduction (00-10-45).srt : 00:03:07,278 --> 00:03:11,550': 'news agencies might be interested in ', '4 - 7 - 3.7 Text Categorization- Motivation (00-14-37).srt : 00:05:25,110 --> 00:05:30,009': 'news agencies would like to assign predefined'}
--------------------------------------------------
4 biomedical 2.006143151929004e-06 for example in the biomedical domain there mesh annotations 
{'4 - 7 - 3.7 Text Categorization- Motivation (00-14-37).srt : 00:05:39,824 --> 00:05:43,650': 'for example in the biomedical domain there mesh annotations'}
--------------------------------------------------
19 helpdesk 1.2213278741915971e-06 the results are another important kind of applications of routing emails to the right person to handle so in helpdesk email messaging is generally routed to a particular person to handle 
{'4 - 7 - 3.7 Text Categorization- Motivation (00-14-37).srt : 00:06:52,580 --> 00:06:55,910': 'to the right person to handle so in helpdesk'}
--------------------------------------------------
21 messaging 1.2213278741915971e-06 the results are another important kind of applications of routing emails to the right person to handle so in helpdesk email messaging is generally routed to a particular person to handle 
{'4 - 7 - 3.7 Text Categorization- Motivation (00-14-37).srt : 00:06:55,910 --> 00:07:01,890': 'email messaging is generally routed to a particular person to handle'}
--------------------------------------------------
24 routed 1.2213278741915971e-06 the results are another important kind of applications of routing emails to the right person to handle so in helpdesk email messaging is generally routed to a particular person to handle 
{'4 - 7 - 3.7 Text Categorization- Motivation (00-14-37).srt : 00:06:55,910 --> 00:07:01,890': 'email messaging is generally routed to a particular person to handle'}
--------------------------------------------------
6 nonspams 1.2213278741915971e-06 spam filtering just distinguishing spams from nonspams so also two categories 
{'4 - 7 - 3.7 Text Categorization- Motivation (00-14-37).srt : 00:08:06,040 --> 00:08:12,330': 'spam filtering just distinguishing spams from nonspams so also two categories'}
--------------------------------------------------
1 classifications 1.2213278741915971e-06 sometimes classifications of opinions can be in two categories positive and a negative 
{'4 - 7 - 3.7 Text Categorization- Motivation (00-14-37).srt : 00:08:12,330 --> 00:08:16,800': 'sometimes classifications of opinions can be in two categories'}
--------------------------------------------------
6 kcategory 2.4426557483831942e-06 a more general case would be kcategory categorization and there are also many applications like that there could be more than two categories 
{'4 - 7 - 3.7 Text Categorization- Motivation (00-14-37).srt : 00:08:19,120 --> 00:08:22,650': 'a more general case would be kcategory categorization and there are also', '4 - 7 - 3.7 Text Categorization- Motivation (00-14-37).srt : 00:09:31,000 --> 00:09:34,839': 'for example a kcategory categorization task can be actually'}
--------------------------------------------------
15 route 1.2213278741915971e-06 email routing would be another example when you may have multiple folders or if you route the email to the right person to handle it then there are multiple people to classify 
{'4 - 7 - 3.7 Text Categorization- Motivation (00-14-37).srt : 00:08:36,205 --> 00:08:39,322': 'if you route the email to the right person to handle it'}
--------------------------------------------------
3 kcategory 2.4426557483831942e-06 for example a kcategory categorization task can be actually performed by using binary categorization 
{'4 - 7 - 3.7 Text Categorization- Motivation (00-14-37).srt : 00:08:19,120 --> 00:08:22,650': 'a more general case would be kcategory categorization and there are also', '4 - 7 - 3.7 Text Categorization- Motivation (00-14-37).srt : 00:09:31,000 --> 00:09:34,839': 'for example a kcategory categorization task can be actually'}
--------------------------------------------------
25 categorized 2.4426557483831942e-06 so we have first we categorize all the objects into let say a small number of highlevel categories and inside each category we have further categorized to subcategories etc 
{'4 - 7 - 3.7 Text Categorization- Motivation (00-14-37).srt : 00:10:09,140 --> 00:10:13,740': 'and inside each category we have further categorized to subcategories etc', '4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:05:53,300 --> 00:05:58,810': 'for example news articles can be tempted to be categorized by using a system and'}
--------------------------------------------------
27 subcategories 1.2213278741915971e-06 so we have first we categorize all the objects into let say a small number of highlevel categories and inside each category we have further categorized to subcategories etc 
{'4 - 7 - 3.7 Text Categorization- Motivation (00-14-37).srt : 00:10:09,140 --> 00:10:13,740': 'and inside each category we have further categorized to subcategories etc'}
--------------------------------------------------
14 coded 1.2213278741915971e-06 and sometimes you may see in some applications text with categorizations called a text coded encoded with some control of vocabulary 
{'4 - 7 - 3.7 Text Categorization- Motivation (00-14-37).srt : 00:12:13,640 --> 00:12:18,704': 'called a text coded encoded with some control of vocabulary'}
--------------------------------------------------
7 affiliations 1.2213278741915971e-06 but you can also imagine the author affiliations or the author age and other things can be actually connected to text data indirectly 
{'4 - 7 - 3.7 Text Categorization- Motivation (00-14-37).srt : 00:13:03,750 --> 00:13:08,340': 'but you can also imagine the author affiliations or the author age and'}
--------------------------------------------------
10 politician 1.2213278741915971e-06 another example is to predict the party affiliation of a politician based on the political speech 
{'4 - 7 - 3.7 Text Categorization- Motivation (00-14-37).srt : 00:14:00,680 --> 00:14:05,566': 'another example is to predict the party affiliation of a politician based'}
--------------------------------------------------
19 prerequires 1.2213278741915971e-06 so to summarize the main points made in this lecture are first the design of a good ranking function prerequires a computational definition of relevance and we achieve this goal by designing a proper retrieval model 
{'2 - 4 - 1.4 Overview of Text Retrieval Methods (00-10-10).srt : 00:08:36,770 --> 00:08:41,580': 'of a good ranking function prerequires a computational definition of relevance and'}
--------------------------------------------------
5 granularities 2.2677482445081393e-06 it can also have different granularities 
{'3 - 1 - 2.1 Topic Mining and Analysis- Motivation and Task Definition (00-07-36).srt : 00:01:25,860 --> 00:01:28,420': 'it can also have different granularities '}
--------------------------------------------------
4 aa 1.7445380593498679e-06 a topic of article aa topic of paragraph or the topic of all the research articles in the research library right so different grand narratives of topics obviously have different applications 
{'3 - 1 - 2.1 Topic Mining and Analysis- Motivation and Task Definition (00-07-36).srt : 00:01:31,240 --> 00:01:34,800': 'a topic of article aa topic of paragraph or'}
--------------------------------------------------
24 narratives 1.2213278741915971e-06 a topic of article aa topic of paragraph or the topic of all the research articles in the research library right so different grand narratives of topics obviously have different applications 
{'3 - 1 - 2.1 Topic Mining and Analysis- Motivation and Task Definition (00-07-36).srt : 00:01:40,540 --> 00:01:45,629': 'so different grand narratives of topics obviously have different applications'}
--------------------------------------------------
9 literatures 1.2213278741915971e-06 now this involves discovery of topics in data mining literatures and also we want to discover topics in today literature and those in the past 
{'3 - 1 - 2.1 Topic Mining and Analysis- Motivation and Task Definition (00-07-36).srt : 00:02:21,840 --> 00:02:26,820': 'now this involves discovery of topics in data mining literatures and'}
--------------------------------------------------
22 dislike 1.2213278741915971e-06 we might also be also interested in knowing what do people like about some products like the iphone  and what do they dislike 
{'3 - 1 - 2.1 Topic Mining and Analysis- Motivation and Task Definition (00-07-36).srt : 00:02:38,400 --> 00:02:43,710': 'some products like the iphone and what do they dislike'}
--------------------------------------------------
11 debated 1.7445380593498679e-06 or perhaps were interested in knowing what are the major topics debated in  presidential election 
{'3 - 1 - 2.1 Topic Mining and Analysis- Motivation and Task Definition (00-07-36).srt : 00:02:52,470 --> 00:02:56,810': 'or perhaps were interested in knowing what are the major topics debated in'}
--------------------------------------------------
16 trending 1.7445380593498679e-06 for example looking at topics over time we would be able to discover whether there a trending topic or some topics might be fading away 
{'3 - 1 - 2.1 Topic Mining and Analysis- Motivation and Task Definition (00-07-36).srt : 00:04:09,320 --> 00:04:14,290': 'whether there a trending topic or some topics might be fading away'}
--------------------------------------------------
23 fading 1.7445380593498679e-06 for example looking at topics over time we would be able to discover whether there a trending topic or some topics might be fading away 
{'3 - 1 - 2.1 Topic Mining and Analysis- Motivation and Task Definition (00-07-36).srt : 00:04:09,320 --> 00:04:14,290': 'whether there a trending topic or some topics might be fading away'}
--------------------------------------------------
25 laid 1.2213278741915971e-06 so now you can see there are generally two different tasks or subtasks the first is to discover k topics from a collection of text laid out 
{'3 - 1 - 2.1 Topic Mining and Analysis- Motivation and Task Definition (00-07-36).srt : 00:05:19,995 --> 00:05:25,760': 'subtasks the first is to discover k topics from a collection of text laid out'}
--------------------------------------------------
3 adult 1.2213278741915971e-06 when we take adult product we see the word frequency in the query to show up in such a sum 
{'4 - 5 - 3.4 Smoothing of Language Model - Part 2 (00-09-36).srt : 00:02:14,250 --> 00:02:16,240': 'when we take adult product '}
--------------------------------------------------
7 subseen 1.2213278741915971e-06 so have you noticed that this p subseen is related to the term frequency in the sense that if a word occurs very frequently in the document then the s probability here will tend to be larger 
{'4 - 5 - 3.4 Smoothing of Language Model - Part 2 (00-09-36).srt : 00:02:55,830 --> 00:03:02,640': 'so have you noticed that this p subseen is related to the term frequency'}
--------------------------------------------------
5 paralyze 2.4426557483831942e-06 so this term appears to paralyze long documents tend to be longer than larger than for long document 
{'4 - 5 - 3.4 Smoothing of Language Model - Part 2 (00-09-36).srt : 00:05:06,218 --> 00:05:12,049': 'so this term appears to paralyze long documents', '3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).srt : 00:10:28,310 --> 00:10:32,420': 'so it would paralyze a case where you have extremely high'}
--------------------------------------------------
8 penalizing 2.2677482445081393e-06 and so this may not actually be necessary penalizing long documents and in fact is not so clear here 
{'4 - 5 - 3.4 Smoothing of Language Model - Part 2 (00-09-36).srt : 00:05:26,950 --> 00:05:30,600': 'penalizing long documents and in fact is not so clear here'}
--------------------------------------------------
5 reform 1.2213278741915971e-06 and that basically a fixed reform of the formula that we did not really have to hueristically line 
{'4 - 5 - 3.4 Smoothing of Language Model - Part 2 (00-09-36).srt : 00:07:04,250 --> 00:07:09,820': 'a fixed reform of the formula that we did not really have to hueristically line'}
--------------------------------------------------
16 hueristically 2.4426557483831942e-06 and that basically a fixed reform of the formula that we did not really have to hueristically line 
{'4 - 5 - 3.4 Smoothing of Language Model - Part 2 (00-09-36).srt : 00:07:04,250 --> 00:07:09,820': 'a fixed reform of the formula that we did not really have to hueristically line', '4 - 5 - 3.4 Smoothing of Language Model - Part 2 (00-09-36).srt : 00:07:34,600 --> 00:07:36,740': 'and if we hueristically design the formula'}
--------------------------------------------------
3 hueristically 2.4426557483831942e-06 and if we hueristically design the formula we may not necessarily end up having such a specific form 
{'4 - 5 - 3.4 Smoothing of Language Model - Part 2 (00-09-36).srt : 00:07:04,250 --> 00:07:09,820': 'a fixed reform of the formula that we did not really have to hueristically line', '4 - 5 - 3.4 Smoothing of Language Model - Part 2 (00-09-36).srt : 00:07:34,600 --> 00:07:36,740': 'and if we hueristically design the formula'}
--------------------------------------------------
8 acc 1.2213278741915971e-06 it also necessary in general to improve the acc accuracy of estimating the model representing the topic of this document 
{'4 - 5 - 3.4 Smoothing of Language Model - Part 2 (00-09-36).srt : 00:07:59,370 --> 00:08:02,600': 'it also necessary in general to improve the acc'}
--------------------------------------------------
19 fuzzy 1.2213278741915971e-06 this is what allows you to distinguish subtle differences in topics and to introduce semantically related words in a fuzzy manner 
{'3 - 3 - 2.3 Topic Mining and Analysis- Probabilistic Topic Models (00-14-17).srt : 00:00:59,140 --> 00:01:04,600': 'to introduce semantically related words in a fuzzy manner'}
--------------------------------------------------
2 replanted 1.2213278741915971e-06 where we replanted each topic it was just one word or one term or one phrase 
{'3 - 3 - 2.3 Topic Mining and Analysis- Probabilistic Topic Models (00-14-17).srt : 00:01:35,650 --> 00:01:40,730': 'where we replanted each topic it was just one word or one term or one phrase'}
--------------------------------------------------
15 trouble 2.4426557483831942e-06 and of course it would also give a nonzero probability to some other word like trouble which might be related to sports in general not so much related to topic 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:04:46,940 --> 00:04:49,560': 'yet we humans have no trouble with understand that', '3 - 3 - 2.3 Topic Mining and Analysis- Probabilistic Topic Models (00-14-17).srt : 00:02:10,150 --> 00:02:15,430': 'like trouble which might be related to sports in general'}
--------------------------------------------------
25 dispose 1.2213278741915971e-06 now intuitively this distribution represents a topic in that if we assemble words from the distribution we tended to see words that are ready to dispose 
{'3 - 3 - 2.3 Topic Mining and Analysis- Probabilistic Topic Models (00-14-17).srt : 00:02:41,440 --> 00:02:46,780': 'words from the distribution we tended to see words that are ready to dispose'}
--------------------------------------------------
3 degenerates 2.2677482445081393e-06 and this basically degenerates to the symbol foundation of a topic was just one word 
{'3 - 3 - 2.3 Topic Mining and Analysis- Probabilistic Topic Models (00-14-17).srt : 00:02:57,387 --> 00:03:01,670': 'and this basically degenerates to the symbol foundation'}
--------------------------------------------------
12 flight 1.7445380593498679e-06 in the distribution for travel we see top words like attraction trip flight etc 
{'3 - 3 - 2.3 Topic Mining and Analysis- Probabilistic Topic Models (00-14-17).srt : 00:03:24,500 --> 00:03:30,120': 'in the distribution for travel we see top words like attraction trip flight etc'}
--------------------------------------------------
9 genomics 2.2677482445081393e-06 whereas in science we see scientist spaceship telescope or genomics and you know science related terms 
{'3 - 3 - 2.3 Topic Mining and Analysis- Probabilistic Topic Models (00-14-17).srt : 00:03:36,110 --> 00:03:39,820': 'genomics and you know science related terms'}
--------------------------------------------------
14 disintegrate 2.4426557483831942e-06 third because we have probabilities for the same word in different topics we can disintegrate the sense of word 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:07:49,700 --> 00:07:54,320': 'a lot of background knowledge to help us disintegrate the ambiguity', '3 - 3 - 2.3 Topic Mining and Analysis- Probabilistic Topic Models (00-14-17).srt : 00:05:07,930 --> 00:05:12,210': 'we can disintegrate the sense of word '}
--------------------------------------------------
14 refinement 1.2213278741915971e-06 the slight is very similar to what youve seen before except we have added refinement for what our topic is 
{'3 - 3 - 2.3 Topic Mining and Analysis- Probabilistic Topic Models (00-14-17).srt : 00:05:32,090 --> 00:05:34,920': 'added refinement for what our topic is '}
--------------------------------------------------
25 bind 1.2213278741915971e-06 c is our collection but we also generally assume we know the number of topics k or we hypothesize a number and then try to bind k topics even though we dont know the exact topics that exist in the collection 
{'3 - 3 - 2.3 Topic Mining and Analysis- Probabilistic Topic Models (00-14-17).srt : 00:06:18,620 --> 00:06:22,940': 'or we hypothesize a number and then try to bind k topics'}
--------------------------------------------------
4 ijs 1.2213278741915971e-06 that the same pi ijs that we have seen before 
{'3 - 3 - 2.3 Topic Mining and Analysis- Probabilistic Topic Models (00-14-17).srt : 00:07:03,520 --> 00:07:06,250': 'that the same pi ijs that we have seen before'}
--------------------------------------------------
3 dimmed 1.2213278741915971e-06 and here i dimmed the picture that you have seen before in order to show the generation process 
{'3 - 3 - 2.3 Topic Mining and Analysis- Probabilistic Topic Models (00-14-17).srt : 00:07:41,390 --> 00:07:46,190': 'and here i dimmed the picture that you have seen before'}
--------------------------------------------------
2 template 1.2213278741915971e-06 so this template of actually consists of all the parameters that were interested in 
{'3 - 3 - 2.3 Topic Mining and Analysis- Probabilistic Topic Models (00-14-17).srt : 00:08:18,840 --> 00:08:22,040': 'so this template of actually consists of '}
--------------------------------------------------
21 plans 1.2213278741915971e-06 now in this case of course for our text mining problem or more precisely topic mining problem we have the following plans 
{'3 - 3 - 2.3 Topic Mining and Analysis- Probabilistic Topic Models (00-14-17).srt : 00:08:39,910 --> 00:08:44,100': 'more precisely topic mining problem we have the following plans'}
--------------------------------------------------
12 snd 1.2213278741915971e-06 first of all we have theta i which is a word distribution snd then we have a set of pis for each document 
{'3 - 3 - 2.3 Topic Mining and Analysis- Probabilistic Topic Models (00-14-17).srt : 00:08:44,100 --> 00:08:49,450': 'first of all we have theta i which is a word distribution snd then we have'}
--------------------------------------------------
5 suffices 2.2677482445081393e-06 it oversimplification obviously but it suffices to show the idea 
{'3 - 3 - 2.3 Topic Mining and Analysis- Probabilistic Topic Models (00-14-17).srt : 00:10:44,260 --> 00:10:49,360': 'it oversimplification obviously but it suffices to show the idea'}
--------------------------------------------------
29 topicit 1.2213278741915971e-06 so to summarize we introduced a new way of representing topic namely representing as word distribution and this has the advantage of using multiple words to describe a complicated topicit also allow us to assign weights on words so we have more than several variations of semantics 
{'3 - 3 - 2.3 Topic Mining and Analysis- Probabilistic Topic Models (00-14-17).srt : 00:12:09,020 --> 00:12:14,039': 'multiple words to describe a complicated topicit also allow us to assign'}
--------------------------------------------------
2 importer 1.2213278741915971e-06 so the importer is a clashing of text articles and a number of topics and a vocabulary set and the output is a set of topics 
{'3 - 3 - 2.3 Topic Mining and Analysis- Probabilistic Topic Models (00-14-17).srt : 00:12:26,430 --> 00:12:30,140': 'so the importer is a clashing of text articles and a number of topics and'}
--------------------------------------------------
5 clashing 1.2213278741915971e-06 so the importer is a clashing of text articles and a number of topics and a vocabulary set and the output is a set of topics 
{'3 - 3 - 2.3 Topic Mining and Analysis- Probabilistic Topic Models (00-14-17).srt : 00:12:26,430 --> 00:12:30,140': 'so the importer is a clashing of text articles and a number of topics and'}
--------------------------------------------------
20 usercontrolled 1.2213278741915971e-06 one is to extend the plsa with prior knowledge and that would allow us to have in some sense a usercontrolled plsa so it doesnt apply to they just listen to data but also would listen to our needs 
{'3 - 15 - 2.15 Latent Dirichlet Allocation (LDA)- Part 1 (00-10-20).srt : 00:00:37,730 --> 00:00:41,795': 'would allow us to have in some sense a usercontrolled plsa '}
--------------------------------------------------
6 blindly 1.2213278741915971e-06 the standard plsa is going to blindly listen to the data by using maximum inaudible 
{'3 - 15 - 2.15 Latent Dirichlet Allocation (LDA)- Part 1 (00-10-20).srt : 00:01:14,665 --> 00:01:20,935': 'the standard plsa is going to blindly listen to the data by using maximum inaudible '}
--------------------------------------------------
12 expectations 2.2677482445081393e-06 this is also very useful but sometimes a user might have some expectations about which topics to analyze 
{'3 - 15 - 2.15 Latent Dirichlet Allocation (LDA)- Part 1 (00-10-20).srt : 00:01:27,155 --> 00:01:32,500': 'but sometimes a user might have some expectations about which topics to analyze '}
--------------------------------------------------
8 elegant 2.4426557483831942e-06 it turns out that there is a very elegant way of doing that and that would incorporate such knowledge as priors on the models 
{'3 - 16 - 2.16 Latent Dirichlet Allocation (LDA)- Part 2 (00-12-03).srt : 00:02:41,770 --> 00:02:48,110': 'lda is a more elegant way of looking at the top and bottom problem', '3 - 15 - 2.15 Latent Dirichlet Allocation (LDA)- Part 1 (00-10-20).srt : 00:02:35,835 --> 00:02:39,410': 'it turns out that there is a very elegant way of doing '}
--------------------------------------------------
14 listens 2.4426557483831942e-06 so what would happen is that we are going to have an estimate that listens to the data and also listens to our prior preferences 
{'3 - 15 - 2.15 Latent Dirichlet Allocation (LDA)- Part 1 (00-10-20).srt : 00:03:13,895 --> 00:03:19,400': 'that listens to the data and also listens to our prior preferences '}
--------------------------------------------------
20 listens 2.4426557483831942e-06 so what would happen is that we are going to have an estimate that listens to the data and also listens to our prior preferences 
{'3 - 15 - 2.15 Latent Dirichlet Allocation (LDA)- Part 1 (00-10-20).srt : 00:03:13,895 --> 00:03:19,400': 'that listens to the data and also listens to our prior preferences '}
--------------------------------------------------
4 artificially 1.2213278741915971e-06 so in fact we artificially inflated their probabilities 
{'3 - 15 - 2.15 Latent Dirichlet Allocation (LDA)- Part 1 (00-10-20).srt : 00:07:47,714 --> 00:07:53,180': 'so in fact we artificially inflated their probabilities '}
--------------------------------------------------
5 inflated 1.2213278741915971e-06 so in fact we artificially inflated their probabilities 
{'3 - 15 - 2.15 Latent Dirichlet Allocation (LDA)- Part 1 (00-10-20).srt : 00:07:47,714 --> 00:07:53,180': 'so in fact we artificially inflated their probabilities '}
--------------------------------------------------
3 infinitive 1.2213278741915971e-06 when mu is infinitive we basically let this one dominate 
{'3 - 15 - 2.15 Latent Dirichlet Allocation (LDA)- Part 1 (00-10-20).srt : 00:08:50,010 --> 00:08:57,360': 'when mu is infinitive we basically let this one dominate '}
--------------------------------------------------
10 hypothesizes 1.2213278741915971e-06 on the right side you see a list of some hypothesizes the data 
{'2 - 13 - 1.13 Syntagmatic Relation Discovery- Mutual Information- Part 2 (00-09-42).srt : 00:00:47,278 --> 00:00:52,970': 'on the right side you see a list of some hypothesizes the data'}
--------------------------------------------------
2 weighed 1.2213278741915971e-06 each is weighed at one quarter so the total of the sum is after the one 
{'2 - 13 - 1.13 Syntagmatic Relation Discovery- Mutual Information- Part 2 (00-09-42).srt : 00:04:17,520 --> 00:04:21,780': 'each is weighed at one quarter so the total of the sum is after the one'}
--------------------------------------------------
15 visible 2.4426557483831942e-06 we introduced multiple statistical approaches for discovering them mainly showing that pure statistical approaches are visible are variable for discovering both kind of relations 
{'3 - 1 - 2.1 Implementation of TR Systems (00-21-27).srt : 00:18:39,160 --> 00:18:43,650': 'or or or if the connection is not very large and this is visible', '2 - 13 - 1.13 Syntagmatic Relation Discovery- Mutual Information- Part 2 (00-09-42).srt : 00:07:16,235 --> 00:07:20,762': 'mainly showing that pure statistical approaches are visible'}
--------------------------------------------------
20 paragraphs 2.2677482445081393e-06 for example the context can be very narrow like a few words around a word or a sentence or maybe paragraphs as using differing contexts would allows to discover different flavors of paradigmatical relations 
{'2 - 13 - 1.13 Syntagmatic Relation Discovery- Mutual Information- Part 2 (00-09-42).srt : 00:07:56,190 --> 00:08:00,760': 'a sentence or maybe paragraphs as using differing contexts would'}
--------------------------------------------------
23 differing 1.7445380593498679e-06 for example the context can be very narrow like a few words around a word or a sentence or maybe paragraphs as using differing contexts would allows to discover different flavors of paradigmatical relations 
{'2 - 13 - 1.13 Syntagmatic Relation Discovery- Mutual Information- Part 2 (00-09-42).srt : 00:07:56,190 --> 00:08:00,760': 'a sentence or maybe paragraphs as using differing contexts would'}
--------------------------------------------------
30 flavors 2.2677482445081393e-06 for example the context can be very narrow like a few words around a word or a sentence or maybe paragraphs as using differing contexts would allows to discover different flavors of paradigmatical relations 
{'2 - 13 - 1.13 Syntagmatic Relation Discovery- Mutual Information- Part 2 (00-09-42).srt : 00:08:00,760 --> 00:08:05,330': 'allows to discover different flavors of paradigmatical relations'}
--------------------------------------------------
7 visual 2.2677482445081393e-06 and similarly counting cooccurrences using let say visual information to discover syntagmatical relations 
{'2 - 13 - 1.13 Syntagmatic Relation Discovery- Mutual Information- Part 2 (00-09-42).srt : 00:08:09,362 --> 00:08:13,380': 'visual information to discover syntagmatical relations'}
--------------------------------------------------
9 collocations 2.2677482445081393e-06 the first is a book with a chapter on collocations which is quite relevant to the topic of these lectures 
{'2 - 13 - 1.13 Syntagmatic Relation Discovery- Mutual Information- Part 2 (00-09-42).srt : 00:08:44,100 --> 00:08:46,880': 'the first is a book with a chapter on collocations'}
--------------------------------------------------
13 atoms 1.7445380593498679e-06 the second is an article about using various statistical measures to discover lexical atoms 
{'2 - 13 - 1.13 Syntagmatic Relation Discovery- Mutual Information- Part 2 (00-09-42).srt : 00:08:55,120 --> 00:08:58,160': 'statistical measures to discover lexical atoms'}
--------------------------------------------------
6 claim 1.4829329667707326e-06 or in facebook as people might claim friends of others etc 
{'5 - 9 - 4.9 Contextual Text Mining- Mining Topics with Social Network Context (00-14-43).srt : 00:00:51,910 --> 00:01:00,570': 'or in facebook as people might claim friends of others etc'}
--------------------------------------------------
7 friends 1.7445380593498679e-06 or in facebook as people might claim friends of others etc 
{'5 - 9 - 4.9 Contextual Text Mining- Mining Topics with Social Network Context (00-14-43).srt : 00:00:51,910 --> 00:01:00,570': 'or in facebook as people might claim friends of others etc'}
--------------------------------------------------
17 frontier 1.7445380593498679e-06 but in general in this part of the course we dont have enough time to cover these frontier topics in detail 
{'5 - 9 - 4.9 Contextual Text Mining- Mining Topics with Social Network Context (00-14-43).srt : 00:02:53,930 --> 00:02:56,940': 'these frontier topics in detail '}
--------------------------------------------------
7 lea 1.2213278741915971e-06 it doesnt have to be plsa or lea or the current topic models 
{'5 - 9 - 4.9 Contextual Text Mining- Mining Topics with Social Network Context (00-14-43).srt : 00:06:07,350 --> 00:06:11,370': 'it doesnt have to be plsa or lea or the current topic models'}
--------------------------------------------------
1 offers 2.2677482445081393e-06 it offers a general approach to combining these different types of data in single optimization framework 
{'5 - 9 - 4.9 Contextual Text Mining- Mining Topics with Social Network Context (00-14-43).srt : 00:06:42,490 --> 00:06:47,530': 'it offers a general approach to combining these different'}
--------------------------------------------------
14 familiarly 1.2213278741915971e-06 and if you look at this formula you can actually recognize some part fairly familiarly 
{'5 - 9 - 4.9 Contextual Text Mining- Mining Topics with Social Network Context (00-14-43).srt : 00:07:36,050 --> 00:07:38,489': 'you can actually recognize some part fairly familiarly'}
--------------------------------------------------
13 collaborators 1.2213278741915971e-06 if you have a weight that says well these two nodes are strong collaborators of researchers 
{'5 - 9 - 4.9 Contextual Text Mining- Mining Topics with Social Network Context (00-14-43).srt : 00:09:34,495 --> 00:09:38,470': 'these two nodes are strong collaborators of researchers'}
--------------------------------------------------
5 dblp 1.7445380593498679e-06 and the data here is dblp data bibliographic data about research articles 
{'5 - 9 - 4.9 Contextual Text Mining- Mining Topics with Social Network Context (00-14-43).srt : 00:10:41,440 --> 00:10:45,917': 'and the data here is dblp data bibliographic data'}
--------------------------------------------------
9 hoping 1.2213278741915971e-06 there are four communities of articles and we were hoping to see that the topic mining can help us uncover these four communities 
{'5 - 9 - 4.9 Contextual Text Mining- Mining Topics with Social Network Context (00-14-43).srt : 00:11:00,860 --> 00:11:05,240': 'there are four communities of articles and we were hoping'}
--------------------------------------------------
1 ned 1.2213278741915971e-06 but ned pierre said we gave much more meaningful topics 
{'5 - 9 - 4.9 Contextual Text Mining- Mining Topics with Social Network Context (00-14-43).srt : 00:11:57,210 --> 00:12:01,780': 'but ned pierre said we gave much more meaningful topics'}
--------------------------------------------------
2 pierre 1.2213278741915971e-06 but ned pierre said we gave much more meaningful topics 
{'5 - 9 - 4.9 Contextual Text Mining- Mining Topics with Social Network Context (00-14-43).srt : 00:11:57,210 --> 00:12:01,780': 'but ned pierre said we gave much more meaningful topics'}
--------------------------------------------------
19 collaborations 1.2213278741915971e-06 now a similar model could have been also useful to to characterize the content associated with each subnetwork of collaborations 
{'5 - 9 - 4.9 Contextual Text Mining- Mining Topics with Social Network Context (00-14-43).srt : 00:13:13,143 --> 00:13:16,270': 'associated with each subnetwork of collaborations'}
--------------------------------------------------
17 living 1.7445380593498679e-06 so a more general view of text mining in context of network is you treat text as living in a rich information network environment 
{'5 - 9 - 4.9 Contextual Text Mining- Mining Topics with Social Network Context (00-14-43).srt : 00:13:24,585 --> 00:13:29,870': 'treat text as living in a rich information network environment'}
--------------------------------------------------
5 behaviour 1.2213278741915971e-06 now lets look at another behaviour of the mixed model and in this case lets look at the response to data frequencies 
{'3 - 9 - 2.9 Probabilistic Topic Models- Mixture Model Estimation- Part 2 (00-08-15).srt : 00:00:00,025 --> 00:00:07,001': 'sound now lets look at another behaviour of the mixed model and'}
--------------------------------------------------
19 lock 1.2213278741915971e-06 so now as you can imagine it would make sense to actually assign a smaller probability for text and lock it 
{'3 - 9 - 2.9 Probabilistic Topic Models- Mixture Model Estimation- Part 2 (00-08-15).srt : 00:02:53,270 --> 00:02:57,850': 'assign a smaller probability for text and lock it'}
--------------------------------------------------
4 surprise 1.2213278741915971e-06 and this is no surprise at all because after all we are maximizing the likelihood of the data 
{'3 - 9 - 2.9 Probabilistic Topic Models- Mixture Model Estimation- Part 2 (00-08-15).srt : 00:03:31,310 --> 00:03:33,470': 'and this is no surprise at all '}
--------------------------------------------------
12 regulated 1.7445380593498679e-06 so the impact here of increasing the probability of the is somewhat regulated by this coefficient the point of i 
{'3 - 9 - 2.9 Probabilistic Topic Models- Mixture Model Estimation- Part 2 (00-08-15).srt : 00:05:07,622 --> 00:05:10,900': 'regulated by this coefficient the point of i'}
--------------------------------------------------
18 effected 1.2213278741915971e-06 so this means the behavior here which is high frequency words tend to get the high probabilities are effected or regularized somewhat by the probability of choosing each component 
{'3 - 9 - 2.9 Probabilistic Topic Models- Mixture Model Estimation- Part 2 (00-08-15).srt : 00:05:20,395 --> 00:05:25,345': 'which is high frequency words tend to get the high probabilities are effected or'}
--------------------------------------------------
12 incentive 2.4426557483831942e-06 if you have a various small probability of being chosen then the incentive is less 
{'4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:10:20,350 --> 00:10:25,280': 'there will be no incentive to assign a non probability using this approach', '3 - 9 - 2.9 Probabilistic Topic Models- Mixture Model Estimation- Part 2 (00-08-15).srt : 00:05:37,910 --> 00:05:44,100': 'if you have a various small probability of being chosen then the incentive is less'}
--------------------------------------------------
33 infusions 1.2213278741915971e-06 and we discussed that the estimation problem of the mixture model and particular with this discussed some general behavior of the estimator and that means we can expect our estimator to capture these infusions 
{'3 - 9 - 2.9 Probabilistic Topic Models- Mixture Model Estimation- Part 2 (00-08-15).srt : 00:06:01,420 --> 00:06:07,070': 'that means we can expect our estimator to capture these infusions'}
--------------------------------------------------
4 collaboratively 1.7445380593498679e-06 and this is to collaboratively maximize likelihood 
{'3 - 9 - 2.9 Probabilistic Topic Models- Mixture Model Estimation- Part 2 (00-08-15).srt : 00:06:14,330 --> 00:06:18,090': 'and this is to collaboratively maximize likelihood'}
--------------------------------------------------
8 waste 2.2677482445081393e-06 and this is to avoid a competition or waste of probability 
{'3 - 9 - 2.9 Probabilistic Topic Models- Mixture Model Estimation- Part 2 (00-08-15).srt : 00:06:23,520 --> 00:06:28,130': 'and this is to avoid a competition or waste of probability'}
--------------------------------------------------
7 regulates 1.7445380593498679e-06 so the probability of choosing each component regulates the collaboration and the competition between component models 
{'3 - 9 - 2.9 Probabilistic Topic Models- Mixture Model Estimation- Part 2 (00-08-15).srt : 00:06:33,580 --> 00:06:39,490': 'so the probability of choosing each component regulates the collaboration and'}
--------------------------------------------------
8 fixing 1.7445380593498679e-06 we also talked about the special case of fixing one component to a background word distribution right 
{'3 - 9 - 2.9 Probabilistic Topic Models- Mixture Model Estimation- Part 2 (00-08-15).srt : 00:06:53,160 --> 00:06:56,600': 'we also talked about the special case of fixing one component to a background'}
--------------------------------------------------
2 excludes 1.2213278741915971e-06 that effectively excludes such a scenario 
{'3 - 9 - 2.9 Probabilistic Topic Models- Mixture Model Estimation- Part 2 (00-08-15).srt : 00:07:56,000 --> 00:07:59,790': 'that effectively excludes such a scenario '}
--------------------------------------------------
12 definite 1.2213278741915971e-06 in particular were going to talk about using machine running to combine definite features to improve ranking function 
{'5 - 6 - 4.4 Learning to Rank Part 1 (00-13-09).srt : 00:00:14,731 --> 00:00:19,478': 'in particular were going to talk about using machine running to combine definite'}
--------------------------------------------------
6 linkbased 2.2677482445081393e-06 and we also talked about the linkbased approaches like page rank that can give additional scores to help us improve ranking 
{'5 - 6 - 4.4 Learning to Rank Part 1 (00-13-09).srt : 00:00:54,502 --> 00:00:58,029': 'and we also talked about the linkbased approaches'}
--------------------------------------------------
18 respected 1.2213278741915971e-06 and these features can vary from content based features such as a score of the document it was respected to the query according to a retrieval function such as bm or query light or pivot commands from a machine or pl et cetera 
{'5 - 6 - 4.4 Learning to Rank Part 1 (00-13-09).srt : 00:01:54,905 --> 00:01:59,735': 'a score of the document it was respected to the query'}
--------------------------------------------------
2 hypothesizing 1.2213278741915971e-06 but by hypothesizing that the relevance is related to those features in the particular way we can then combine these futures to generate the potentially more powerful ranking function a more robust ranking function 
{'5 - 6 - 4.4 Learning to Rank Part 1 (00-13-09).srt : 00:03:44,092 --> 00:03:49,099': 'but by hypothesizing that the relevance is related to those features'}
--------------------------------------------------
20 futures 1.2213278741915971e-06 but by hypothesizing that the relevance is related to those features in the particular way we can then combine these futures to generate the potentially more powerful ranking function a more robust ranking function 
{'5 - 6 - 4.4 Learning to Rank Part 1 (00-13-09).srt : 00:03:49,099 --> 00:03:54,106': 'in the particular way we can then combine these futures to generate'}
--------------------------------------------------
7 ranging 2.4426557483831942e-06 so in general the fit such hypothesize ranging function to the training day meaning that we will try to optimize its retrieval accuracy on the training data 
{'5 - 5 - 4.3 Link Analysis - Part 3 (00-05-59).srt : 00:05:03,920 --> 00:05:08,637': 'and these scores can then be used in ranging to start the pagerank scores', '5 - 6 - 4.4 Learning to Rank Part 1 (00-13-09).srt : 00:04:53,272 --> 00:05:00,022': 'so in general the fit such hypothesize ranging function to the training day'}
--------------------------------------------------
0 htuple 1.2213278741915971e-06 htuple it has three elements the query the document and the judgment 
{'5 - 6 - 4.4 Learning to Rank Part 1 (00-13-09).srt : 00:05:25,187 --> 00:05:32,605': 'htuple it has three elements the query the document and the judgment'}
--------------------------------------------------
2 rep 2.2677482445081393e-06 they must rep represent the real queries and real documents that the users handle 
{'3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).srt : 00:00:43,240 --> 00:00:47,260': 'they must rep represent the real queries and real documents that the users handle'}
--------------------------------------------------
3 intensive 2.4426557483831942e-06 it very labor intensive 
{'4 - 8 - 3.8 Text Categorization- Methods (00-11-50).srt : 00:02:22,790 --> 00:02:27,010': 'first off because it label intensive it requires a lot of manual work', '3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).srt : 00:01:44,980 --> 00:01:47,690': 'it very labor intensive '}
--------------------------------------------------
15 reflected 1.7445380593498679e-06 for measures it also challenging because what we want with measures is that with accuracy reflected the perceived utility of users 
{'3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).srt : 00:02:06,760 --> 00:02:11,670': 'with accuracy reflected the perceived utility of users'}
--------------------------------------------------
17 perceived 1.7445380593498679e-06 for measures it also challenging because what we want with measures is that with accuracy reflected the perceived utility of users 
{'3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).srt : 00:02:06,760 --> 00:02:11,670': 'with accuracy reflected the perceived utility of users'}
--------------------------------------------------
16 misled 2.4426557483831942e-06 if we your measure is not measuring the right thing then your conclusion would would be misled 
{'3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).srt : 00:02:21,370 --> 00:02:23,810': 'then your conclusion would would be misled', '4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:08:10,120 --> 00:08:14,160': 'and you might be misled to believe one method is better than the other'}
--------------------------------------------------
12 fluctuate 2.2677482445081393e-06 if there a a big variance that means that the results could fluctuate a lot according to different queries 
{'3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).srt : 00:05:21,770 --> 00:05:25,880': 'that the results could fluctuate a lot according to different queries'}
--------------------------------------------------
5 concur 1.2213278741915971e-06 you know we cant reliably concur that 
{'3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).srt : 00:06:39,510 --> 00:06:41,330': 'you know we cant reliably concur that '}
--------------------------------------------------
7 parametrical 1.2213278741915971e-06 now in wilcoxon test it a non parametrical test and we would be not only looking at the signs well be also looking at the magnitude of the difference 
{'3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).srt : 00:07:01,310 --> 00:07:06,470': 'now in wilcoxon test it a non parametrical test'}
--------------------------------------------------
3 interval 2.2677482445081393e-06 and in this interval then the observed values may still be from random fluctuation 
{'3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).srt : 00:08:21,890 --> 00:08:25,990': 'and in this interval then the observed values'}
--------------------------------------------------
8 nominate 1.7445380593498679e-06 and we hope these methods can help us nominate likely relevance in the documents 
{'3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).srt : 00:09:57,120 --> 00:10:00,702': 'and we hope these methods can help us nominate'}
--------------------------------------------------
3 unjudged 1.7445380593498679e-06 and the other unjudged documents are usually just a assumed to be nonrelevant 
{'3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).srt : 00:11:22,410 --> 00:11:26,710': 'and the other unjudged documents are usually just a assumed to be nonrelevant'}
--------------------------------------------------
14 reconsidered 1.2213278741915971e-06 but the if the pool is not very large this actually has to be reconsidered and we might use other strategies to deal with them and there are indeed other methods to handle such cases 
{'3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).srt : 00:11:32,234 --> 00:11:37,904': 'but the if the pool is not very large this actually has to be reconsidered'}
--------------------------------------------------
3 problematic 1.7445380593498679e-06 however this is problematic for even evaluating a new system that may not have contributed to the pool 
{'3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).srt : 00:12:04,300 --> 00:12:06,530': 'however this is problematic for '}
--------------------------------------------------
15 nominated 1.2213278741915971e-06 in this case you know a new system might be penalized because it might have nominated some relevant documents that have not been judged 
{'3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).srt : 00:12:16,010 --> 00:12:20,800': 'nominated some relevant documents that have not been judged'}
--------------------------------------------------
3 unfair 1.7445380593498679e-06 and that that unfair 
{'3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).srt : 00:12:24,370 --> 00:12:26,150': 'and that that unfair '}
--------------------------------------------------
3 inappropriate 1.7445380593498679e-06 if we have inappropriate experiment design we might misguide our research or applications 
{'3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).srt : 00:12:43,580 --> 00:12:46,630': 'if we have inappropriate experiment design'}
--------------------------------------------------
8 misguide 1.7445380593498679e-06 if we have inappropriate experiment design we might misguide our research or applications 
{'3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).srt : 00:12:46,630 --> 00:12:49,710': 'we might misguide our research or applications'}
--------------------------------------------------
0 perceiving 1.2213278741915971e-06 perceiving up to ten documents is easier to interpret from users perspective 
{'3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).srt : 00:13:22,965 --> 00:13:27,060': 'perceiving up to ten documents is easier to interpret from users perspective'}
--------------------------------------------------
4 leverages 1.2213278741915971e-06 so this is what leverages a real users of a search engine to do evaluation 
{'3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).srt : 00:14:17,730 --> 00:14:21,268': 'so this is what leverages a real users of a search engine to do evaluation'}
--------------------------------------------------
17 commercial 1.7445380593498679e-06 it called ab test and it a strategy that often used by the modern search engines the commercial search engines 
{'3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).srt : 00:14:25,950 --> 00:14:28,380': 'the modern search engines the commercial search engines'}
--------------------------------------------------
10 mini 1.2213278741915971e-06 so there are three additional readings here these are three mini books about evaluation 
{'3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).srt : 00:14:44,170 --> 00:14:47,980': 'these are three mini books about evaluation'}
--------------------------------------------------
5 paradigmatics 1.2213278741915971e-06 this lecture is about the paradigmatics relation discovery 
{'2 - 8 - 1.8 Paradigmatic Relation Discovery Part 1 (00-14-31).srt : 00:00:07,935 --> 00:00:14,253': 'lecture is about the paradigmatics relation discovery'}
--------------------------------------------------
17 nonadjacent 2.2677482445081393e-06 so in general context may contain adjacent words like eats and my that you see here or nonadjacent words like saturday tuesday or some other words in the context 
{'2 - 8 - 1.8 Paradigmatic Relation Discovery Part 1 (00-14-31).srt : 00:03:54,744 --> 00:03:59,575': 'my that you see here or nonadjacent words like saturday'}
--------------------------------------------------
4 loosely 1.7445380593498679e-06 that would give us loosely related paradigmatical relations 
{'2 - 8 - 1.8 Paradigmatic Relation Discovery Part 1 (00-14-31).srt : 00:04:19,130 --> 00:04:25,270': 'that would give us loosely related paradigmatical relations'}
--------------------------------------------------
15 pardigmatically 1.7445380593498679e-06 and this would be naturally application specific but again here the main idea for discovering pardigmatically related words is to computer the similarity of their context 
{'2 - 8 - 1.8 Paradigmatic Relation Discovery Part 1 (00-14-31).srt : 00:05:24,395 --> 00:05:28,935': 'here the main idea for discovering pardigmatically related words is'}
--------------------------------------------------
7 plausible 1.7445380593498679e-06 so let first look at the one plausible approach where we try to match the similarity of context based on the expected overlap of words and we call this eowc 
{'2 - 8 - 1.8 Paradigmatic Relation Discovery Part 1 (00-14-31).srt : 00:08:01,378 --> 00:08:05,829': 'so let first look at the one plausible approach'}
--------------------------------------------------
29 eowc 2.4426557483831942e-06 so let first look at the one plausible approach where we try to match the similarity of context based on the expected overlap of words and we call this eowc 
{'2 - 8 - 1.8 Paradigmatic Relation Discovery Part 1 (00-14-31).srt : 00:08:10,481 --> 00:08:15,150': 'the expected overlap of words and we call this eowc', '2 - 8 - 1.8 Paradigmatic Relation Discovery Part 1 (00-14-31).srt : 00:11:34,920 --> 00:11:42,380': 'that one possible approach eowc extracted overlap of words in context'}
--------------------------------------------------
4 stare 1.2213278741915971e-06 so if you just stare at the formula to check what inside this sum then you will see basically in each case it gives us the probability that we will see an overlap on a particular word wi 
{'2 - 8 - 1.8 Paradigmatic Relation Discovery Part 1 (00-14-31).srt : 00:10:57,440 --> 00:11:04,550': 'so if you just stare at the formula to check what inside this sum'}
--------------------------------------------------
4 eowc 2.4426557483831942e-06 that one possible approach eowc extracted overlap of words in context 
{'2 - 8 - 1.8 Paradigmatic Relation Discovery Part 1 (00-14-31).srt : 00:08:10,481 --> 00:08:15,150': 'the expected overlap of words and we call this eowc', '2 - 8 - 1.8 Paradigmatic Relation Discovery Part 1 (00-14-31).srt : 00:11:34,920 --> 00:11:42,380': 'that one possible approach eowc extracted overlap of words in context'}
--------------------------------------------------
3 ultimately 1.7445380593498679e-06 now of course ultimately we have to test the approach with real data and see if it gives us really semantically related words 
{'2 - 8 - 1.8 Paradigmatic Relation Discovery Part 1 (00-14-31).srt : 00:11:49,440 --> 00:11:52,880': 'now of course ultimately we have to test the approach with real data and'}
--------------------------------------------------
46 ser 1.2213278741915971e-06 these vertical search engines can be expected to be more effective than the current general search engines because they could assume that users are a special group of users that might have a common information need and then the search engine can be customized with this ser so such users 
{'5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:01:02,070 --> 00:01:06,420': 'and then the search engine can be customized with this ser so such users'}
--------------------------------------------------
4 customization 1.7445380593498679e-06 and because of the customization it also possible to do personalization 
{'5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:01:07,970 --> 00:01:12,150': 'and because of the customization it also possible to do personalization'}
--------------------------------------------------
3 restrictions 2.4426557483831942e-06 because of the restrictions with domain we also have some advantages in handling the documents because we can have better understanding of documents 
{'2 - 1 - 1.1 Natural Language Content Analysis (00-21-05).srt : 00:12:16,540 --> 00:12:21,670': 'restrictions on the world of users you may be to may be able to perform', '5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:01:20,330 --> 00:01:25,550': 'because of the restrictions with domain we also have some advantages'}
--------------------------------------------------
3 lifetime 1.7445380593498679e-06 it like a lifetime learning or lifelong learning and this is of course very attractive because that means the search engine will selfimprove itself 
{'5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:01:45,600 --> 00:01:52,430': 'it like a lifetime learning or lifelong learning and this is of course'}
--------------------------------------------------
6 lifelong 2.006143151929004e-06 it like a lifetime learning or lifelong learning and this is of course very attractive because that means the search engine will selfimprove itself 
{'5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:01:45,600 --> 00:01:52,430': 'it like a lifetime learning or lifelong learning and this is of course'}
--------------------------------------------------
14 attractive 1.7445380593498679e-06 it like a lifetime learning or lifelong learning and this is of course very attractive because that means the search engine will selfimprove itself 
{'5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:01:52,430 --> 00:01:57,800': 'very attractive because that means the search engine will selfimprove itself'}
--------------------------------------------------
22 selfimprove 1.7445380593498679e-06 it like a lifetime learning or lifelong learning and this is of course very attractive because that means the search engine will selfimprove itself 
{'5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:01:52,430 --> 00:01:57,800': 'very attractive because that means the search engine will selfimprove itself'}
--------------------------------------------------
9 bottles 1.2213278741915971e-06 the third trend might be to the integration of bottles of information access 
{'5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:02:24,600 --> 00:02:27,190': 'bottles of information access '}
--------------------------------------------------
13 fullfledged 2.2677482445081393e-06 so search navigation and recommendation or filtering might be combined to form a fullfledged information management system 
{'5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:02:32,050 --> 00:02:37,480': 'combined to form a fullfledged information management system'}
--------------------------------------------------
12 endings 1.2213278741915971e-06 and in fact were doing that basically today is the inaudible search endings 
{'5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:02:53,660 --> 00:02:58,390': 'and in fact were doing that basically today is the inaudible search endings'}
--------------------------------------------------
5 clicking 1.2213278741915971e-06 we are querying sometimes browsing clicking on links 
{'5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:02:58,390 --> 00:03:02,000': 'we are querying sometimes browsing clicking on links'}
--------------------------------------------------
7 seamlessly 2.2677482445081393e-06 but in the future you can imagine seamlessly integrate the system with multimode for information access and that would be convenient for people 
{'5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:03:11,466 --> 00:03:16,305': 'but in the future you can imagine seamlessly integrate the system with'}
--------------------------------------------------
12 purchase 2.2677482445081393e-06 for example consumers might search for opinions about products in order to purchase a product choose a good product by so in this case it would be beneficial to support the whole workflow of purchasing a product or choosing a product 
{'5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:03:42,380 --> 00:03:45,385': 'opinions about products in order to purchase a product'}
--------------------------------------------------
34 purchasing 1.7445380593498679e-06 for example consumers might search for opinions about products in order to purchase a product choose a good product by so in this case it would be beneficial to support the whole workflow of purchasing a product or choosing a product 
{'5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:03:50,160 --> 00:03:55,330': 'support the whole workflow of purchasing a product or choosing a product'}
--------------------------------------------------
2 era 2.2677482445081393e-06 in this era after the common search engines already provide a good support 
{'5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:03:56,732 --> 00:04:00,300': 'in this era after the common search engines already provide a good support'}
--------------------------------------------------
23 button 1.7445380593498679e-06 for example you can sometimes look at the reviews and then if you want to buy it you can just click on the button to go the shopping site and directly get it done 
{'5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:04:04,190 --> 00:04:09,040': 'you can just click on the button to go the shopping site and directly get it done'}
--------------------------------------------------
10 realm 1.2213278741915971e-06 for example for researchers you might want to find the realm in the literature or site of the literature 
{'5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:04:14,720 --> 00:04:18,800': 'you might want to find the realm in the literature or site of the literature'}
--------------------------------------------------
25 imagining 1.2213278741915971e-06 so in the following few slides ill be talking a little bit more about some specific ideas or thoughts that hopefully can help you in imagining new application possibilities 
{'5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:04:39,900 --> 00:04:43,720': 'can help you in imagining new application possibilities'}
--------------------------------------------------
18 alert 2.2677482445081393e-06 if you connect the scientist with literature information to provide all kinds of service including search browsing or alert of new random documents or mining analyzing research trends or provide the task with support or decision support 
{'5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:06:22,050 --> 00:06:28,310': 'alert of new random documents or mining analyzing research trends'}
--------------------------------------------------
1 intelligently 1.7445380593498679e-06 maybe intelligently attach also a promotion message if appropriate if they detect that that a positive message not a complaint and then you might take this opportunity to attach some promotion information 
{'5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:07:39,630 --> 00:07:45,720': 'maybe intelligently attach also a promotion message'}
--------------------------------------------------
6 imagination 1.2213278741915971e-06 it just only restricted by our imagination 
{'5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:08:19,850 --> 00:08:22,090': 'it just only restricted by our imagination'}
--------------------------------------------------
3 heard 2.4426557483831942e-06 if you havent heard of it it is a good step toward this direction 
{'2 - 5 - 1.5 Text Representation- Part 1 (00-10-46).srt : 00:06:19,972 --> 00:06:25,510': 'and it also related to the knowledge graph that some of you may have heard of', '5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:10:24,130 --> 00:10:28,310': 'if you havent heard of it it is a good step toward this direction'}
--------------------------------------------------
9 initiating 1.2213278741915971e-06 and once we can get to that level without initiating robust manner at larger scale it can enable the search engine to provide a much better service 
{'5 - 9 - 4.5 Future of Web Search (00-13-09).srt : 00:10:28,310 --> 00:10:33,820': 'and once we can get to that level without initiating robust manner at larger scale'}
--------------------------------------------------
11 implicitly 2.2677482445081393e-06 now this is in contrast with a generative model where we implicitly define the clustering bias by using a particular object to function like a inaudible function 
{'4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:00:42,980 --> 00:00:48,010': 'a generative model where we implicitly define the clustering bias'}
--------------------------------------------------
10 aim 1.7445380593498679e-06 so once we have a similarity function we can then aim at optimally partitioning to partitioning the data into clusters or into different groups 
{'4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:01:12,272 --> 00:01:18,229': 'so once we have a similarity function we can then aim at optimally partitioning'}
--------------------------------------------------
8 topdown 2.2677482445081393e-06 until we group everything together the other is topdown or divisive in this case we gradually partition the whole data set into smaller and smaller clusters 
{'4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:02:31,440 --> 00:02:36,351': 'until we group everything together the other is topdown or divisive in this'}
--------------------------------------------------
21 hac 1.2213278741915971e-06 but here we are going to talk about the two representative methods in some detail one is hierarchical agglomerative clustering or hac the other is kmeans 
{'4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:03:14,340 --> 00:03:20,350': 'one is hierarchical agglomerative clustering or hac the other is kmeans'}
--------------------------------------------------
12 fashion 1.7445380593498679e-06 and then we can gradually group similar objects together in a bottomup fashion to form larger and larger groups 
{'4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:03:30,850 --> 00:03:36,110': 'and then we can gradually group similar objects together in a bottomup fashion to'}
--------------------------------------------------
14 criterion 2.2677482445081393e-06 and they always form a hierarchy and then we can stop when some stopping criterion is met 
{'4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:03:40,550 --> 00:03:44,190': 'then we can stop when some stopping criterion is met'}
--------------------------------------------------
11 induced 1.2213278741915971e-06 based on the individual objects similarity so let illustrate how again induced a structure based on just similarity 
{'4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:04:01,470 --> 00:04:07,410': 'let illustrate how again induced a structure based on just similarity'}
--------------------------------------------------
13 farthest 2.4426557483831942e-06 completelink defines the similarity of the two groups as the similarity of the farthest system pair 
{'4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:05:56,010 --> 00:05:59,755': 'as the similarity of the farthest system pair', '4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:07:04,400 --> 00:07:10,956': 'taking the similarity of the two farthest pair as the similarity for the two groups'}
--------------------------------------------------
9 illustrating 1.2213278741915971e-06 so it much easier to understand the methods by illustrating them so here are two groups g and g with some objects in each group 
{'4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:06:06,490 --> 00:06:12,220': 'so it much easier to understand the methods by illustrating them'}
--------------------------------------------------
19 paired 1.2213278741915971e-06 so in terms of singlelink and were just looking at the closest pair so in this case these two paired objects will defined the similarities of the two groups 
{'4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:06:40,975 --> 00:06:45,640': 'these two paired objects will defined the similarities of the two groups'}
--------------------------------------------------
21 optimistic 1.7445380593498679e-06 as long as they are very close were going to say the two groups are very close so it is an optimistic view of similarity 
{'4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:06:51,710 --> 00:06:56,090': 'close so it is an optimistic view of similarity'}
--------------------------------------------------
20 farthest 2.4426557483831942e-06 the complete link on the other hand were in some sense pessimistic and by taking the similarity of the two farthest pair as the similarity for the two groups 
{'4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:05:56,010 --> 00:05:59,755': 'as the similarity of the farthest system pair', '4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:07:04,400 --> 00:07:10,956': 'taking the similarity of the two farthest pair as the similarity for the two groups'}
--------------------------------------------------
9 parties 1.2213278741915971e-06 if you think about this as similar to having parties with people then it just means two groups of people would be partying together 
{'4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:08:09,240 --> 00:08:15,170': 'if you think about this as similar to having parties with people'}
--------------------------------------------------
22 partying 1.2213278741915971e-06 if you think about this as similar to having parties with people then it just means two groups of people would be partying together 
{'4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:08:15,170 --> 00:08:22,010': 'then it just means two groups of people would be partying together'}
--------------------------------------------------
3 leaders 1.2213278741915971e-06 so the two leaders of the two groups can have a good relationship with each other and then they will bring together the two groups 
{'4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:08:29,780 --> 00:08:35,100': 'so the two leaders of the two groups can have a good'}
--------------------------------------------------
16 insensitive 1.2213278741915971e-06 the average link of clusters in between and as group decision so it going to be insensitive to outliers now in practice which one is the best 
{'4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:09:37,510 --> 00:09:42,920': 'going to be insensitive to outliers now in practice which one is the best'}
--------------------------------------------------
1 aggressively 1.2213278741915971e-06 and aggressively cluster objects together that maybe singlelink is good 
{'4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:09:48,270 --> 00:09:53,050': 'and aggressively cluster objects together that maybe singlelink is good'}
--------------------------------------------------
11 centers 1.2213278741915971e-06 selected vectors as centroids of k clusters and treat them as centers as if they represent they each represent a cluster 
{'4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:10:37,170 --> 00:10:43,380': 'treat them as centers as if they represent they each represent a cluster'}
--------------------------------------------------
4 recompute 1.7445380593498679e-06 then we can do recompute the centroid based on the locate the object in each cluster 
{'4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:11:28,740 --> 00:11:32,570': 'then we can do recompute the centroid based on'}
--------------------------------------------------
13 brother 1.4829329667707326e-06 we dont make a probabilistic allocation as in the case of estep the brother will make a choice 
{'4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:13:20,170 --> 00:13:22,515': 'the brother will make a choice '}
--------------------------------------------------
11 countspull 1.2213278741915971e-06 so this is also similar to the mstep where we do countspull together counts and then normalize them 
{'4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:14:23,170 --> 00:14:27,731': 'so this is also similar to the mstep where we do countspull together counts'}
--------------------------------------------------
24 allocations 1.2213278741915971e-06 and this is only a subset of data points but in the algorithm we in principle consider all the data points based on probabilistic allocations 
{'4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:14:50,760 --> 00:14:55,375': 'we in principle consider all the data points based on probabilistic allocations'}
--------------------------------------------------
7 customize 1.2213278741915971e-06 we can also use prior to further customize the clustering algorithm to for example control the topic of one cluster or multiple clusters 
{'4 - 5 - 3.5 Text Clustering- Similarity-based Approaches (00-17-48).srt : 00:15:54,820 --> 00:15:59,760': 'we can also use prior to further customize the clustering algorithm to for'}
--------------------------------------------------
13 capacities 1.2213278741915971e-06 and that would open up a more interesting representation opportunities and also analysis capacities 
{'2 - 6 - 1.6 Text Representation- Part 2 (00-09-29).srt : 00:00:29,520 --> 00:00:33,780': 'opportunities and also analysis capacities'}
--------------------------------------------------
2 visualizes 1.7445380593498679e-06 the second visualizes the generality of such a representation 
{'2 - 6 - 1.6 Text Representation- Part 2 (00-09-29).srt : 00:00:39,800 --> 00:00:44,820': 'the second visualizes the generality of such a representation'}
--------------------------------------------------
2 repetition 1.2213278741915971e-06 word base repetition is a very important level of representation 
{'2 - 6 - 1.6 Text Representation- Part 2 (00-09-29).srt : 00:01:28,540 --> 00:01:32,470': 'word base repetition is a very important level of representation'}
--------------------------------------------------
7 abounded 1.2213278741915971e-06 and topic and opinion related applications are abounded 
{'2 - 6 - 1.6 Text Representation- Part 2 (00-09-29).srt : 00:01:54,930 --> 00:02:00,550': 'and topic and opinion related applications are abounded'}
--------------------------------------------------
13 scaled 2.2677482445081393e-06 you can also use this level representation to integrate everything about anything from scaled resources 
{'2 - 6 - 1.6 Text Representation- Part 2 (00-09-29).srt : 00:04:31,825 --> 00:04:35,820': 'to integrate everything about anything from scaled resources'}
--------------------------------------------------
7 integrating 1.7445380593498679e-06 and this can be very useful for integrating analysis of scattered knowledge 
{'2 - 6 - 1.6 Text Representation- Part 2 (00-09-29).srt : 00:04:46,190 --> 00:04:48,780': 'integrating analysis of scattered knowledge'}
--------------------------------------------------
24 compiling 1.2213278741915971e-06 for example whether a gene has a certain function and then the intelligent program can read the literature to extract the relevant facts doing compiling and information extracting 
{'2 - 6 - 1.6 Text Representation- Part 2 (00-09-29).srt : 00:05:42,135 --> 00:05:45,250': 'doing compiling and information extracting'}
--------------------------------------------------
14 questioning 1.2213278741915971e-06 and then using a logic system to actually track that the answers to researchers questioning about what genes are related to what functions 
{'2 - 6 - 1.6 Text Representation- Part 2 (00-09-29).srt : 00:05:50,891 --> 00:05:56,060': 'to researchers questioning about what genes are related to what functions'}
--------------------------------------------------
11 richer 1.7445380593498679e-06 and when different levels are combined together we can enable a richer analysis more powerful analysis 
{'2 - 6 - 1.6 Text Representation- Part 2 (00-09-29).srt : 00:07:37,210 --> 00:07:41,520': 'we can enable a richer analysis more powerful analysis'}
--------------------------------------------------
11 fragile 1.7445380593498679e-06 that a big advantage over other approaches that rely on more fragile natural language processing techniques 
{'2 - 6 - 1.6 Text Representation- Part 2 (00-09-29).srt : 00:07:59,780 --> 00:08:03,510': 'more fragile natural language processing techniques'}
--------------------------------------------------
5 surprisingly 2.4426557483831942e-06 third these techniques are actually surprisingly powerful and effective form in implications 
{'4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:14:13,220 --> 00:14:19,440': 'well not surprisingly we see these common words on top as we always do', '2 - 6 - 1.6 Text Representation- Part 2 (00-09-29).srt : 00:08:20,910 --> 00:08:25,373': 'third these techniques are actually surprisingly powerful and'}
--------------------------------------------------
11 implications 1.2213278741915971e-06 third these techniques are actually surprisingly powerful and effective form in implications 
{'2 - 6 - 1.6 Text Representation- Part 2 (00-09-29).srt : 00:08:25,373 --> 00:08:27,690': 'effective form in implications '}
--------------------------------------------------
11 provision 1.2213278741915971e-06 earlier we have introduced measures that can be used with computer provision and recall 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:00:12,439 --> 00:00:18,302': 'earlier we have introduced measures that can be used with computer provision and'}
--------------------------------------------------
3 fore 1.2213278741915971e-06 so the difference fore example between arithmetically and geometrically is that the arithmetically would be dominated by high values whereas geometrically would be more affected by low values 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:02:00,860 --> 00:02:03,770': 'so the difference fore example between arithmetically and'}
--------------------------------------------------
6 arithmetically 2.4426557483831942e-06 so the difference fore example between arithmetically and geometrically is that the arithmetically would be dominated by high values whereas geometrically would be more affected by low values 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:02:00,860 --> 00:02:03,770': 'so the difference fore example between arithmetically and', '4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:02:03,770 --> 00:02:08,360': 'geometrically is that the arithmetically would be dominated by high'}
--------------------------------------------------
12 arithmetically 2.4426557483831942e-06 so the difference fore example between arithmetically and geometrically is that the arithmetically would be dominated by high values whereas geometrically would be more affected by low values 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:02:00,860 --> 00:02:03,770': 'so the difference fore example between arithmetically and', '4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:02:03,770 --> 00:02:08,360': 'geometrically is that the arithmetically would be dominated by high'}
--------------------------------------------------
29 recal 1.2213278741915971e-06 base and so whether you are want to emphasis low values or high values would be a question relate with all you and similar we can do that for recal and f score 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:02:22,040 --> 00:02:24,720': 'similar we can do that for recal and f score'}
--------------------------------------------------
27 insightful 1.4829329667707326e-06 and especially if you compare different methods in different dimensions it might reveal which method is better in which measure or in what situations and this provides insightful 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:03:16,370 --> 00:03:19,830': 'in what situations and this provides insightful'}
--------------------------------------------------
8 weakness 1.7445380593498679e-06 understanding the strands of a method or a weakness and this provides further insight for improving them 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:03:19,830 --> 00:03:23,070': 'understanding the strands of a method or a weakness and'}
--------------------------------------------------
7 microaverage 1.2213278741915971e-06 so as i mentioned there is also microaverage in contrast to the macro average that we talked about earlier 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:03:28,260 --> 00:03:32,180': 'so as i mentioned there is also microaverage'}
--------------------------------------------------
32 contingency 1.2213278741915971e-06 so we can compute the overall precision and recall by just counting how many cases are in true positive how many cases in false positive etc it computing the values in the contingency table and then we can compute the precision and recall just once 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:03:55,832 --> 00:04:01,660': 'etc it computing the values in the contingency table'}
--------------------------------------------------
3 macroaveraging 1.2213278741915971e-06 in contrast in macroaveraging were going to do that for each category first 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:04:06,060 --> 00:04:10,296': 'in contrast in macroaveraging were going to do that for each category first'}
--------------------------------------------------
22 pooled 1.2213278741915971e-06 and then aggregate over these categories or we do that for each document and then aggregate all the documents but here we pooled them together 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:04:16,070 --> 00:04:19,950': 'then aggregate all the documents but here we pooled them together'}
--------------------------------------------------
2 averaging 2.4426557483831942e-06 but macro averaging and micro averaging they are both very common and you might see both reported in research papers on categorization 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:05:20,620 --> 00:05:27,210': 'but macro averaging and micro averaging they are both very common'}
--------------------------------------------------
5 averaging 2.4426557483831942e-06 but macro averaging and micro averaging they are both very common and you might see both reported in research papers on categorization 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:05:20,620 --> 00:05:27,210': 'but macro averaging and micro averaging they are both very common'}
--------------------------------------------------
10 prospective 1.2213278741915971e-06 also sometimes categorization results might actually be evaluated from ranking prospective 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:05:36,750 --> 00:05:39,290': 'be evaluated from ranking prospective '}
--------------------------------------------------
10 editing 1.2213278741915971e-06 for example it might be passed to humans for further editing 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:05:49,610 --> 00:05:53,300': 'for example it might be passed to humans for further editing'}
--------------------------------------------------
6 tempted 1.2213278741915971e-06 for example news articles can be tempted to be categorized by using a system and then human editors would then correct them 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:05:53,300 --> 00:05:58,810': 'for example news articles can be tempted to be categorized by using a system and'}
--------------------------------------------------
9 categorized 2.4426557483831942e-06 for example news articles can be tempted to be categorized by using a system and then human editors would then correct them 
{'4 - 7 - 3.7 Text Categorization- Motivation (00-14-37).srt : 00:10:09,140 --> 00:10:13,740': 'and inside each category we have further categorized to subcategories etc', '4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:05:53,300 --> 00:05:58,810': 'for example news articles can be tempted to be categorized by using a system and'}
--------------------------------------------------
17 editors 2.2677482445081393e-06 for example news articles can be tempted to be categorized by using a system and then human editors would then correct them 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:05:58,810 --> 00:06:01,040': 'then human editors would then correct them'}
--------------------------------------------------
17 desk 1.2213278741915971e-06 and all the email messages might be throughout to the right person for handling in the help desk 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:06:07,500 --> 00:06:09,890': 'handling in the help desk '}
--------------------------------------------------
9 prioritizing 1.2213278741915971e-06 and in such a case the categorizations will help prioritizing the task for particular customer service person 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:06:09,890 --> 00:06:14,090': 'and in such a case the categorizations will help prioritizing'}
--------------------------------------------------
9 prioritized 1.2213278741915971e-06 so in this case the results have to be prioritized and if the system cant give a score to the categorization decision for confidence then we can use the scores to rank these decisions and then evaluate the results as a rank list just as in a search engine 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:06:19,690 --> 00:06:25,360': 'so in this case the results have to be prioritized'}
--------------------------------------------------
7 responsible 1.7445380593498679e-06 evaluation where you rank the documents in responsible query 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:06:44,660 --> 00:06:47,990': 'evaluation where you rank the documents in responsible query'}
--------------------------------------------------
16 chris 1.2213278741915971e-06 so to reflect the utility for humans in such a task it better to evaluate ranking chris and this is basically similar to a search again 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:07:19,180 --> 00:07:23,860': 'better to evaluate ranking chris and this is basically similar to a search again'}
--------------------------------------------------
9 misleading 2.4426557483831942e-06 if you dont get it right you might get misleading results 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:07:03,110 --> 00:07:07,430': 'reflect the volatility of the method then it would give you misleading results so', '4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:08:07,200 --> 00:08:10,120': 'if you dont get it right you might get misleading results'}
--------------------------------------------------
4 misled 2.4426557483831942e-06 and you might be misled to believe one method is better than the other which is in fact not true 
{'3 - 9 - 2.8 Evaluation of TR Systems- Practical Issues (00-15-14).srt : 00:02:21,370 --> 00:02:23,810': 'then your conclusion would would be misled', '4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:08:10,120 --> 00:08:14,160': 'and you might be misled to believe one method is better than the other'}
--------------------------------------------------
4 offs 2.4426557483831942e-06 sometimes there are trade offs between multiple aspects like precision and recall and so we need to know for this application is high recall more important or high precision is more important 
{'2 - 4 - 1.4 Natural Language Content Analysis- Part 2 (00-04-25).srt : 00:00:27,938 --> 00:00:33,342': 'the two offs actually have somewhat a differentness in their active', '4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:08:49,240 --> 00:08:52,440': 'sometimes there are trade offs between multiple aspects like precision and'}
--------------------------------------------------
1 preceding 1.2213278741915971e-06 inaudible preceding inaudible scores are common and report characterizing performances given angles and give us some inaudible like a inaudible per document basis inaudible and then take a average of all of them different ways micro versus macro inaudible 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:09:17,268 --> 00:09:22,230': 'inaudible preceding inaudible scores are common and'}
--------------------------------------------------
9 performances 1.2213278741915971e-06 inaudible preceding inaudible scores are common and report characterizing performances given angles and give us some inaudible like a inaudible per document basis inaudible and then take a average of all of them different ways micro versus macro inaudible 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:09:22,230 --> 00:09:27,266': 'report characterizing performances given angles and give us some'}
--------------------------------------------------
25 diagnoses 1.2213278741915971e-06 in general you want to look at the results from multiple perspectives and for particular applications some perspectives would be more important than others but diagnoses and analysis of categorization methods 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:09:46,970 --> 00:09:50,120': 'diagnoses and analysis of categorization methods'}
--------------------------------------------------
18 tow 1.2213278741915971e-06 it generally useful to look at as many perspectives as possible to see subtle differences between methods or tow see where a method might be weak from which you can obtain sight for improving a method 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:09:54,920 --> 00:10:00,220': 'to see subtle differences between methods or tow see where a method might be weak'}
--------------------------------------------------
25 weak 2.4426557483831942e-06 it generally useful to look at as many perspectives as possible to see subtle differences between methods or tow see where a method might be weak from which you can obtain sight for improving a method 
{'3 - 8 - 2.8 Probabilistic Topic Models- Mixture Model Estimation- Part 1 (00-10-16).srt : 00:08:56,690 --> 00:08:59,850': 'because the background part is weak for text', '4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:09:54,920 --> 00:10:00,220': 'to see subtle differences between methods or tow see where a method might be weak'}
--------------------------------------------------
31 sight 1.2213278741915971e-06 it generally useful to look at as many perspectives as possible to see subtle differences between methods or tow see where a method might be weak from which you can obtain sight for improving a method 
{'4 - 13 - 3.13 Text Categorization- Evaluation Part 2 (00-10-51).srt : 00:10:00,220 --> 00:10:03,100': 'from which you can obtain sight for improving a method'}
--------------------------------------------------
8 costs 1.2213278741915971e-06 in general different categorization mistakes however have different costs for specific applications 
{'4 - 12 - 3.12 Text Categorization- Evaluation Part 1 (00-14-12).srt : 00:02:58,890 --> 00:03:03,570': 'however have different costs for specific applications'}
--------------------------------------------------
12 ck 1.2213278741915971e-06 so here you see that there are categories denoted by c through ck and there are n documents denoted by d through d n and for each pair of category and the document we can then look at the situation 
{'4 - 12 - 3.12 Text Categorization- Evaluation Part 1 (00-14-12).srt : 00:04:04,990 --> 00:04:10,620': 'through ck and there are n documents denoted by d through d n'}
--------------------------------------------------
7 ns 2.4426557483831942e-06 so well see all combinations of this ns yes and nos minus and pluses 
{'3 - 6 - 2.6 Probabilistic Topic Models- Mining One Topic (00-12-21).srt : 00:09:37,280 --> 00:09:43,089': 'because this is just the normalized count of these words by the document ns', '4 - 12 - 3.12 Text Categorization- Evaluation Part 1 (00-14-12).srt : 00:04:46,360 --> 00:04:53,708': 'so well see all combinations of this ns yes and nos minus and pluses'}
--------------------------------------------------
3 plusses 1.2213278741915971e-06 one is y plusses 
{'4 - 12 - 3.12 Text Categorization- Evaluation Part 1 (00-14-12).srt : 00:05:25,090 --> 00:05:26,580': 'one is y plusses '}
--------------------------------------------------
1 letting 1.7445380593498679e-06 but letting spam to come into your folder is another type of error 
{'4 - 12 - 3.12 Text Categorization- Evaluation Part 1 (00-14-12).srt : 00:06:42,360 --> 00:06:47,050': 'but letting spam to come into your folder is another type of error'}
--------------------------------------------------
12 inbox 2.4426557483831942e-06 it okay to occasionally let a spam email to come into your inbox 
{'1 - 2 - Course Introduction (00-10-45).srt : 00:02:27,800 --> 00:02:33,600': 'this is actually to filter out the spams from your inbox all right', '4 - 12 - 3.12 Text Categorization- Evaluation Part 1 (00-14-12).srt : 00:06:54,930 --> 00:06:59,930': 'it okay to occasionally let a spam email to come into your inbox'}
--------------------------------------------------
5 imbalance 1.2213278741915971e-06 there also another problem with imbalance to test set 
{'4 - 12 - 3.12 Text Categorization- Evaluation Part 1 (00-14-12).srt : 00:07:14,340 --> 00:07:16,950': 'there also another problem with imbalance to test set'}
--------------------------------------------------
3 skew 1.2213278741915971e-06 imagine there a skew to test set where most instances are category one and  of instances are category one 
{'4 - 12 - 3.12 Text Categorization- Evaluation Part 1 (00-14-12).srt : 00:07:16,950 --> 00:07:22,883': 'imagine there a skew to test set where most instances are category one and'}
--------------------------------------------------
4 appearing 1.7445380593498679e-06 it going to be appearing to be very effective but in reality this is obviously not a good result 
{'4 - 12 - 3.12 Text Categorization- Evaluation Part 1 (00-14-12).srt : 00:07:39,450 --> 00:07:43,760': 'it going to be appearing to be very effective but in reality'}
--------------------------------------------------
13 minority 1.2213278741915971e-06 and one above equal number of instances for example in each class the minority categories or causes tend to be overlooked in the evaluation of classification accuracy 
{'4 - 12 - 3.12 Text Categorization- Evaluation Part 1 (00-14-12).srt : 00:07:57,340 --> 00:08:02,860': 'example in each class the minority categories or causes tend to be'}
--------------------------------------------------
20 overlooked 1.2213278741915971e-06 and one above equal number of instances for example in each class the minority categories or causes tend to be overlooked in the evaluation of classification accuracy 
{'4 - 12 - 3.12 Text Categorization- Evaluation Part 1 (00-14-12).srt : 00:08:02,860 --> 00:08:07,290': 'overlooked in the evaluation of classification accuracy'}
--------------------------------------------------
4 confirm 2.2677482445081393e-06 but when the human confirm that it is indeed correct that becomes a true positive 
{'4 - 12 - 3.12 Text Categorization- Evaluation Part 1 (00-14-12).srt : 00:09:02,210 --> 00:09:05,420': 'but when the human confirm that it is indeed correct'}
--------------------------------------------------
17 fp 1.2213278741915971e-06 when the system says yes but the human says no that incorrect that a false positive have fp 
{'4 - 12 - 3.12 Text Categorization- Evaluation Part 1 (00-14-12).srt : 00:09:10,710 --> 00:09:13,950': 'that incorrect that a false positive have fp'}
--------------------------------------------------
5 shock 1.2213278741915971e-06 it doesnt as well for shock documents 
{'4 - 12 - 3.12 Text Categorization- Evaluation Part 1 (00-14-12).srt : 00:11:14,370 --> 00:11:17,350': 'it doesnt as well for shock documents '}
--------------------------------------------------
7 inputting 1.2213278741915971e-06 and this gives you some insight for inputting the method 
{'4 - 12 - 3.12 Text Categorization- Evaluation Part 1 (00-14-12).srt : 00:11:18,900 --> 00:11:22,760': 'and this gives you some insight for inputting the method'}
--------------------------------------------------
6 percategory 1.2213278741915971e-06 similarly we can look at the percategory evaluation 
{'4 - 12 - 3.12 Text Categorization- Evaluation Part 1 (00-14-12).srt : 00:11:22,760 --> 00:11:25,830': 'similarly we can look at the percategory evaluation'}
--------------------------------------------------
15 undesirable 1.2213278741915971e-06 and we think about that youll see that there is indeed some difference and some undesirable property of this arithmatic 
{'4 - 12 - 3.12 Text Categorization- Evaluation Part 1 (00-14-12).srt : 00:13:08,610 --> 00:13:13,480': 'some undesirable property of this arithmatic'}
--------------------------------------------------
19 arithmatic 1.2213278741915971e-06 and we think about that youll see that there is indeed some difference and some undesirable property of this arithmatic 
{'4 - 12 - 3.12 Text Categorization- Evaluation Part 1 (00-14-12).srt : 00:13:08,610 --> 00:13:13,480': 'some undesirable property of this arithmatic'}
--------------------------------------------------
12 awkward 1.2213278741915971e-06 but if we think about the feedback information it a little bit awkward to use query likelihood to perform feedback because a lot of times the feedback information is additional information about the query 
{'4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt : 00:00:35,860 --> 00:00:39,910': 'but if we think about the feedback information it a little bit awkward to'}
--------------------------------------------------
3 unnatural 1.2213278741915971e-06 it kind of unnatural to sample words that form feedback documents 
{'4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt : 00:00:56,850 --> 00:01:03,170': 'it kind of unnatural to sample words that form feedback documents'}
--------------------------------------------------
20 layered 1.2213278741915971e-06 and then kldivergence or also called cross entropy retrieval model is basically to generalize the frequency part here into a layered model 
{'4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt : 00:02:16,808 --> 00:02:21,150': 'here into a layered model '}
--------------------------------------------------
6 plotting 2.2677482445081393e-06 and this difference allows us to plotting various different ways to estimate this 
{'4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt : 00:02:35,810 --> 00:02:42,610': 'and this difference allows us to plotting various different ways to estimate this'}
--------------------------------------------------
5 equivalence 1.7445380593498679e-06 so you can see the equivalence 
{'4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt : 00:04:33,830 --> 00:04:36,200': 'so you can see the equivalence '}
--------------------------------------------------
5 selective 2.2677482445081393e-06 let assume they are more selective sorry mostly positive documents 
{'4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt : 00:05:31,680 --> 00:05:37,420': 'let assume they are more selective sorry mostly positive documents'}
--------------------------------------------------
4 controller 1.2213278741915971e-06 we have a source controller here 
{'4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt : 00:09:50,810 --> 00:09:53,620': 'we have a source controller here '}
--------------------------------------------------
9 removing 2.2677482445081393e-06 this would allow us to achieve the effect of removing these top words that are meaningless in the feedback 
{'4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt : 00:12:49,520 --> 00:12:53,800': 'this would allow us to achieve the effect of removing these top words'}
--------------------------------------------------
15 meaningless 2.4426557483831942e-06 this would allow us to achieve the effect of removing these top words that are meaningless in the feedback 
{'2 - 7 - 1.7 Word Association Mining and Analysis (00-15-39).srt : 00:02:59,590 --> 00:03:03,950': 'meaning that if we do that the sentence will become somewhat meaningless', '4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt : 00:12:53,800 --> 00:12:56,790': 'that are meaningless in the feedback '}
--------------------------------------------------
9 guy 1.2213278741915971e-06 all the other variables are known except for this guy 
{'4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt : 00:14:10,510 --> 00:14:13,910': 'all the other variables are known except for this guy'}
--------------------------------------------------
14 interpreter 1.2213278741915971e-06 once we have done that we obtain this theta f that can be the interpreter with the original query model to do feedback 
{'4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt : 00:14:43,490 --> 00:14:47,860': 'that can be the interpreter with the original query model to do feedback'}
--------------------------------------------------
8 picks 1.2213278741915971e-06 it shows that this model really works and picks up mm some related words to the query 
{'4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt : 00:16:22,930 --> 00:16:26,790': 'picks up mm some related words to the query'}
--------------------------------------------------
10 mm 2.4426557483831942e-06 it shows that this model really works and picks up mm some related words to the query 
{'3 - 3 - 2.3 System Implementation- Fast Search (00-17-11).srt : 00:16:13,390 --> 00:16:18,490': 'so these basic techniques have mm have great potential for further scanning', '4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt : 00:16:22,930 --> 00:16:26,790': 'picks up mm some related words to the query'}
--------------------------------------------------
14 topped 1.2213278741915971e-06 if we dont rely much on background model we still have to use this topped model to account for the common words 
{'4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt : 00:16:53,245 --> 00:16:58,100': 'we still have to use this topped model to account for the common words'}
--------------------------------------------------
26 burden 2.2677482445081393e-06 whereas if we set lambda to a very high value we would use the background model very often to explain these words then there is no burden on expanding those common words in the feedback documents by the topping model 
{'4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt : 00:17:03,062 --> 00:17:06,980': 'model very often to explain these words then there is no burden on'}
--------------------------------------------------
28 expanding 1.7445380593498679e-06 whereas if we set lambda to a very high value we would use the background model very often to explain these words then there is no burden on expanding those common words in the feedback documents by the topping model 
{'4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt : 00:17:06,980 --> 00:17:11,800': 'expanding those common words in the feedback documents by the topping model'}
--------------------------------------------------
38 topping 1.2213278741915971e-06 whereas if we set lambda to a very high value we would use the background model very often to explain these words then there is no burden on expanding those common words in the feedback documents by the topping model 
{'4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt : 00:17:06,980 --> 00:17:11,800': 'expanding those common words in the feedback documents by the topping model'}
--------------------------------------------------
8 pseudoexamples 1.2213278741915971e-06 these examples can be assumed examples can be pseudoexamples like assume the the top ten documents are assumed to be random 
{'4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt : 00:17:38,290 --> 00:17:42,914': 'these examples can be assumed examples can be pseudoexamples'}
--------------------------------------------------
6 fractions 1.2213278741915971e-06 they could be based on using fractions like feedback based on quick sorts or implicit feedback 
{'4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt : 00:17:48,268 --> 00:17:52,277': 'they could be based on using fractions like feedback'}
--------------------------------------------------
12 sorts 1.2213278741915971e-06 they could be based on using fractions like feedback based on quick sorts or implicit feedback 
{'4 - 10 - 3.8 Feedback in Text Retrieval- Feedback in LM (00-19-11).srt : 00:17:52,277 --> 00:17:55,260': 'based on quick sorts or implicit feedback '}
--------------------------------------------------
9 inequalities 1.2213278741915971e-06 this required more knowledge about that some of that inequalities that we havent really covered yet 
{'3 - 12 - 2.12 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3 (00-06-25).srt : 00:00:29,613 --> 00:00:36,910': 'some of that inequalities that we havent really covered yet'}
--------------------------------------------------
5 mitsumoto 1.2213278741915971e-06 but in the case of mitsumoto we can not easily find an analytic solution to the problem 
{'3 - 12 - 2.12 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3 (00-06-25).srt : 00:01:06,630 --> 00:01:11,480': 'but in the case of mitsumoto we can not easily find an analytic solution'}
--------------------------------------------------
12 analytic 1.2213278741915971e-06 but in the case of mitsumoto we can not easily find an analytic solution to the problem 
{'3 - 12 - 2.12 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3 (00-06-25).srt : 00:01:06,630 --> 00:01:11,480': 'but in the case of mitsumoto we can not easily find an analytic solution'}
--------------------------------------------------
2 hillclimb 1.2213278741915971e-06 it a hillclimb algorithm 
{'3 - 12 - 2.12 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3 (00-06-25).srt : 00:01:16,457 --> 00:01:17,850': 'it a hillclimb algorithm '}
--------------------------------------------------
5 climbing 1.2213278741915971e-06 so that the ideal hill climbing 
{'3 - 12 - 2.12 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3 (00-06-25).srt : 00:01:35,420 --> 00:01:37,630': 'so that the ideal hill climbing '}
--------------------------------------------------
10 stuck 2.2677482445081393e-06 unless it has reached the maximum where it will be stuck there 
{'3 - 12 - 2.12 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3 (00-06-25).srt : 00:03:03,620 --> 00:03:06,930': 'unless it has reached the maximum where it will be stuck there'}
--------------------------------------------------
10 numeral 1.2213278741915971e-06 and this actually in general is a difficult problem in numeral optimization 
{'3 - 12 - 2.12 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3 (00-06-25).srt : 00:03:54,340 --> 00:03:59,070': 'and this actually in general is a difficult problem in numeral optimization'}
--------------------------------------------------
24 gear 1.2213278741915971e-06 so that not optimal and wed like to climb up all the way to here so the only way to climb up to this gear is to start from somewhere here or here 
{'3 - 12 - 2.12 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3 (00-06-25).srt : 00:04:11,227 --> 00:04:16,575': 'so the only way to climb up to this gear is to start from somewhere here or here'}
--------------------------------------------------
8 augmented 2.4426557483831942e-06 in the mstep then we would exploit such augmented data which would make it easier to estimate the distribution to improve the estimate of parameters 
{'3 - 14 - 2.14 Probabilistic Latent Semantic Analysis (PLSA)- Part 2 (00-10-15).srt : 00:00:36,750 --> 00:00:40,740': 'so now the e step as you can recall is your augmented data and', '3 - 12 - 2.12 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3 (00-06-25).srt : 00:05:10,056 --> 00:05:15,750': 'in the mstep then we would exploit such augmented data which would make'}
--------------------------------------------------
3 augmentation 1.2213278741915971e-06 now here data augmentation is done probabilistically 
{'3 - 12 - 2.12 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 3 (00-06-25).srt : 00:05:47,500 --> 00:05:50,790': 'now here data augmentation is done probabilistically'}
--------------------------------------------------
9 scalable 2.006143151929004e-06 such detail in understanding techniques however are generally not scalable and they tend to require a lot of human effort 
{'1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:02:05,220 --> 00:02:11,360': 'are generally not scalable and they tend to require a lot of human effort'}
--------------------------------------------------
20 datasets 2.2677482445081393e-06 second you will have opportunity to experiment with some algorithms for text mining and analytics to try them on some datasets and to understand how to do experiments 
{'1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:02:57,037 --> 00:03:01,309': 'text mining and analytics to try them on some datasets and'}
--------------------------------------------------
8 suspect 1.2213278741915971e-06 not because it not important but because we suspect that the not all of you will have the need for computing resources to do the program assignment 
{'1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:04:40,053 --> 00:04:46,019': 'because we suspect that the not all of you will have the need for'}
--------------------------------------------------
31 teach 2.4426557483831942e-06 so naturally we would encourage all of you to try to do the program assignments if possible as that will be a great way to learn about the knowledge that we teach in this course 
{'1 - 1 - Text Mining and Analytics (00-08-15).srt : 00:00:44,770 --> 00:00:49,370': 'a capstone project course that all of us will teach together', '1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:05:01,657 --> 00:05:06,860': 'to learn about the knowledge that we teach in this course'}
--------------------------------------------------
28 quiz 2.4426557483831942e-06 so we expect you to be able to understand all the essential materials by just watching the actual videos and you should be able to answer all the quiz questions by just watching the videos 
{'1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:05:22,680 --> 00:05:27,810': 'watching the actual videos and you should be able to answer all the quiz', '1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:07:28,357 --> 00:07:35,034': 'it does mean every quiz has to be or better'}
--------------------------------------------------
8 silly 1.2213278741915971e-06 the third book is actually a collection of silly articles and it has broadly covered all the aspects of mining text data 
{'1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:06:10,460 --> 00:06:16,020': 'the third book is actually a collection of silly articles and'}
--------------------------------------------------
11 cutting 2.4426557483831942e-06 in these chapters you can find some in depth discussion of cutting edge research on the topics that we discussed in this course 
{'5 - 5 - 4.5 Opinion Mining and Sentiment Analysis- Latent Aspect Rating Analysis Part 2 (00-14-43).srt : 00:01:46,150 --> 00:01:51,990': 'many other cases in this part of the cause where we discuss the cutting edge topics', '1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:06:26,048 --> 00:06:31,474': 'in these chapters you can find some in depth discussion of cutting'}
--------------------------------------------------
14 earning 1.2213278741915971e-06 so i just briefly go over it and you can complete the course by earning one of the following badges 
{'1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:07:09,755 --> 00:07:15,552': 'you can complete the course by earning one of the following badges'}
--------------------------------------------------
19 badges 2.4426557483831942e-06 so i just briefly go over it and you can complete the course by earning one of the following badges 
{'1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:07:09,755 --> 00:07:15,552': 'you can complete the course by earning one of the following badges', '1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:07:48,731 --> 00:07:52,190': 'there are also three optional programming badges'}
--------------------------------------------------
3 achievement 2.4426557483831942e-06 one is course achievement badge 
{'1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:07:15,552 --> 00:07:18,014': 'one is course achievement badge ', '1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:07:59,925 --> 00:08:02,188': 'the first is programming achievement badge'}
--------------------------------------------------
1 earn 1.2213278741915971e-06 to earn that you have to have at least a  average score on all the quizzes combined 
{'1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:07:18,014 --> 00:07:23,318': 'to earn that you have to have at least a'}
--------------------------------------------------
4 quiz 2.4426557483831942e-06 it does mean every quiz has to be  or better 
{'1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:05:22,680 --> 00:05:27,810': 'watching the actual videos and you should be able to answer all the quiz', '1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:07:28,357 --> 00:07:35,034': 'it does mean every quiz has to be or better'}
--------------------------------------------------
2 batch 2.4426557483831942e-06 the second batch here this is a course mastery badge and this just requires a higher score  average score for the quizzes 
{'1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:07:35,034 --> 00:07:39,294': 'the second batch here this is a course mastery badge and', '3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:04:18,860 --> 00:04:24,030': 'and that would allow us to use the memory to process the next batch of documents'}
--------------------------------------------------
8 mastery 2.4426557483831942e-06 the second batch here this is a course mastery badge and this just requires a higher score  average score for the quizzes 
{'1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:07:35,034 --> 00:07:39,294': 'the second batch here this is a course mastery badge and', '1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:08:12,649 --> 00:08:17,696': 'and similarly the mastery badge '}
--------------------------------------------------
6 badges 2.4426557483831942e-06 there are also three optional programming badges 
{'1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:07:09,755 --> 00:07:15,552': 'you can complete the course by earning one of the following badges', '1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:07:48,731 --> 00:07:52,190': 'there are also three optional programming badges'}
--------------------------------------------------
4 achievement 2.4426557483831942e-06 the first is programming achievement badge 
{'1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:07:15,552 --> 00:07:18,014': 'one is course achievement badge ', '1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:07:59,925 --> 00:08:02,188': 'the first is programming achievement badge'}
--------------------------------------------------
6 switching 1.2213278741915971e-06 this is similar to the call switching from the badge 
{'1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:08:02,188 --> 00:08:05,648': 'this is similar to the call switching from the badge'}
--------------------------------------------------
3 mastery 2.4426557483831942e-06 and similarly the mastery badge is given to those who can score  average score or better 
{'1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:07:35,034 --> 00:07:39,294': 'the second batch here this is a course mastery badge and', '1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:08:12,649 --> 00:08:17,696': 'and similarly the mastery badge '}
--------------------------------------------------
8 leader 1.2213278741915971e-06 the last badge is a text mining competition leader badge and this is given to those of you who do well in the competition task 
{'1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:08:26,928 --> 00:08:33,095': 'the last badge is a text mining competition leader badge and'}
--------------------------------------------------
3 planning 1.2213278741915971e-06 and specifically were planning to give the badge to the top  in the leaderboard 
{'1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:08:39,768 --> 00:08:44,530': 'and specifically were planning to give '}
--------------------------------------------------
13 leaderboard 1.2213278741915971e-06 and specifically were planning to give the badge to the top  in the leaderboard 
{'1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:08:44,530 --> 00:08:49,684': 'the badge to the top in the leaderboard'}
--------------------------------------------------
12 classroom 1.2213278741915971e-06 this lecture is a continuing discussion of generative probabilistic models for tax classroom 
{'4 - 4 - 3.4 Text Clustering- Generative Probabilistic Models Part 3 (00-14-55).srt : 00:00:08,289 --> 00:00:12,379': 'is a continuing discussion of generative probabilistic models for tax classroom'}
--------------------------------------------------
16 crossing 2.4426557483831942e-06 in this lecture were going to do a finishing discussion of generative probabilistic models for text crossing 
{'4 - 4 - 3.4 Text Clustering- Generative Probabilistic Models Part 3 (00-14-55).srt : 00:00:17,210 --> 00:00:19,630': 'generative probabilistic models for text crossing', '4 - 4 - 3.4 Text Clustering- Generative Probabilistic Models Part 3 (00-14-55).srt : 00:00:26,635 --> 00:00:32,371': 'the mixture model for text crossing and what the likelihood function looks like'}
--------------------------------------------------
22 crossing 2.4426557483831942e-06 so this is a slide that you have seen before and here we show how we define the mixture model for text crossing and what the likelihood function looks like 
{'4 - 4 - 3.4 Text Clustering- Generative Probabilistic Models Part 3 (00-14-55).srt : 00:00:17,210 --> 00:00:19,630': 'generative probabilistic models for text crossing', '4 - 4 - 3.4 Text Clustering- Generative Probabilistic Models Part 3 (00-14-55).srt : 00:00:26,635 --> 00:00:32,371': 'the mixture model for text crossing and what the likelihood function looks like'}
--------------------------------------------------
12 trsa 1.2213278741915971e-06 now if you have understood how algorithm works for topic models like trsa and i think here it would be very similar 
{'4 - 4 - 3.4 Text Clustering- Generative Probabilistic Models Part 3 (00-14-55).srt : 00:01:04,450 --> 00:01:09,490': 'topic models like trsa and i think here it would be very similar'}
--------------------------------------------------
8 zd 1.7445380593498679e-06 so i have to introduce a hidden variable zd for each document and this variable could take a value from the range of  through k representing k different distributions 
{'4 - 4 - 3.4 Text Clustering- Generative Probabilistic Models Part 3 (00-14-55).srt : 00:01:44,040 --> 00:01:48,540': 'so i have to introduce a hidden variable zd for'}
--------------------------------------------------
2 reestimation 2.2677482445081393e-06 so the reestimation involves two kinds of parameters  is p of theta and this is the probability of selecting a particular distribution 
{'4 - 4 - 3.4 Text Clustering- Generative Probabilistic Models Part 3 (00-14-55).srt : 00:03:57,089 --> 00:04:02,522': 'so the reestimation involves two kinds of parameters is p of theta and'}
--------------------------------------------------
11 crack 1.2213278741915971e-06 but after we have observed that these documents then we can crack the evidence to infer which cluster is more likely 
{'4 - 4 - 3.4 Text Clustering- Generative Probabilistic Models Part 3 (00-14-55).srt : 00:04:17,558 --> 00:04:23,544': 'then we can crack the evidence to infer which cluster is more likely'}
--------------------------------------------------
8 piz 2.4426557483831942e-06 and this is very similar to the case piz and here we just report the kinds of words that are in documents that are inferred to have been generated from a particular topic of theta i here 
{'4 - 4 - 3.4 Text Clustering- Generative Probabilistic Models Part 3 (00-14-55).srt : 00:05:01,144 --> 00:05:05,384': 'and this is very similar to the case piz and', '4 - 4 - 3.4 Text Clustering- Generative Probabilistic Models Part 3 (00-14-55).srt : 00:06:54,970 --> 00:06:58,907': 'and before in piz we used one hidden variable for'}
--------------------------------------------------
3 piz 2.4426557483831942e-06 and before in piz we used one hidden variable for each work because that the output from one mixture model 
{'4 - 4 - 3.4 Text Clustering- Generative Probabilistic Models Part 3 (00-14-55).srt : 00:05:01,144 --> 00:05:05,384': 'and this is very similar to the case piz and', '4 - 4 - 3.4 Text Clustering- Generative Probabilistic Models Part 3 (00-14-55).srt : 00:06:54,970 --> 00:06:58,907': 'and before in piz we used one hidden variable for'}
--------------------------------------------------
24 bar 1.7445380593498679e-06 so here you see that we take a average of all these two math solutions to compute average at the screen called a theta bar 
{'4 - 4 - 3.4 Text Clustering- Generative Probabilistic Models Part 3 (00-14-55).srt : 00:09:18,537 --> 00:09:23,340': 'solutions to compute average at the screen called a theta bar'}
--------------------------------------------------
21 numerators 1.2213278741915971e-06 the whole value of this expression is not changed but by doing this normalization you can see we can make the numerators and the denominators more manageable in that the overall value is not going to be very small for each 
{'4 - 4 - 3.4 Text Clustering- Generative Probabilistic Models Part 3 (00-14-55).srt : 00:10:07,940 --> 00:10:14,480': 'you can see we can make the numerators and the denominators more manageable'}
--------------------------------------------------
24 denominators 2.4426557483831942e-06 the whole value of this expression is not changed but by doing this normalization you can see we can make the numerators and the denominators more manageable in that the overall value is not going to be very small for each 
{'4 - 4 - 3.4 Text Clustering- Generative Probabilistic Models Part 3 (00-14-55).srt : 00:10:07,940 --> 00:10:14,480': 'you can see we can make the numerators and the denominators more manageable', '3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).srt : 00:05:19,683 --> 00:05:22,449': 'were going to use different denominators '}
--------------------------------------------------
26 manageable 1.2213278741915971e-06 the whole value of this expression is not changed but by doing this normalization you can see we can make the numerators and the denominators more manageable in that the overall value is not going to be very small for each 
{'4 - 4 - 3.4 Text Clustering- Generative Probabilistic Models Part 3 (00-14-55).srt : 00:10:07,940 --> 00:10:14,480': 'you can see we can make the numerators and the denominators more manageable'}
--------------------------------------------------
5 fractional 1.2213278741915971e-06 so these gives us some fractional accounts 
{'4 - 4 - 3.4 Text Clustering- Generative Probabilistic Models Part 3 (00-14-55).srt : 00:12:43,567 --> 00:12:47,490': 'so these gives us some fractional accounts'}
--------------------------------------------------
7 copy 2.2677482445081393e-06 and this is very different from again copy model where we can generate the words in the document by using multiple unigram language models 
{'4 - 4 - 3.4 Text Clustering- Generative Probabilistic Models Part 3 (00-14-55).srt : 00:13:45,960 --> 00:13:49,830': 'and this is very different from again copy model where we can generate'}
--------------------------------------------------
13 disjointed 1.2213278741915971e-06 but if we want to achieve harder clusters mainly to partition documents into disjointed clusters 
{'4 - 4 - 3.4 Text Clustering- Generative Probabilistic Models Part 3 (00-14-55).srt : 00:14:16,800 --> 00:14:20,160': 'partition documents into disjointed clusters'}
--------------------------------------------------
6 petitions 1.2213278741915971e-06 and unfortunately in most retrieval a petitions the data set would be large and they generally cannot be loaded into the memory at once 
{'3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:00:46,444 --> 00:00:52,125': 'and unfortunately in most retrieval a petitions the data set would be large and'}
--------------------------------------------------
10 sortingbased 1.7445380593498679e-06 and there are many approaches to solving that problem and sortingbased method is quite common and works in four steps as shown here 
{'3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:01:00,725 --> 00:01:06,710': 'sortingbased method is quite common and works in four steps as shown here'}
--------------------------------------------------
6 termid 1.7445380593498679e-06 first we collect the the local termid document id and frequency tuples 
{'3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:01:06,710 --> 00:01:11,480': 'first we collect the the local termid document id and frequency tuples'}
--------------------------------------------------
2 overlook 2.4426557483831942e-06 basically you overlook kinds of terms in a small set of documents and and then once you collect those counts you can sort those counts based on terms so that you build a local a partial inverted index 
{'3 - 6 - 2.6 Evaluation of TR Systems- Evaluating Ranked Lists Part 1 (00-12-51).srt : 00:12:57,050 --> 00:13:01,300': 'it a common mistake that people sometimes overlook', '3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:01:11,480 --> 00:01:17,790': 'basically you overlook kinds of terms in a small set of documents and and'}
--------------------------------------------------
13 batch 2.4426557483831942e-06 and that would allow us to use the memory to process the next batch of documents and were going to do that for all the documents 
{'1 - 2 - Course Prerequisites & Completion (00-09-02).srt : 00:07:35,034 --> 00:07:39,294': 'the second batch here this is a course mastery badge and', '3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:04:18,860 --> 00:04:24,030': 'and that would allow us to use the memory to process the next batch of documents'}
--------------------------------------------------
7 po 1.2213278741915971e-06 now we mentioned earlier that because the po postings are very large it desirable to compress them 
{'3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:05:11,245 --> 00:05:16,245': 'because the po postings are very large it desirable to compress them'}
--------------------------------------------------
21 defaulting 1.2213278741915971e-06 and we generally have to use variable lengths in coding instead of the fixed lengths in coding as we using by defaulting a program language like c 
{'3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:05:32,100 --> 00:05:38,745': 'lengths in coding as we using by defaulting a program language like c'}
--------------------------------------------------
5 uniformly 1.7445380593498679e-06 if the values are distributed uniformly and this wont save us any spacing 
{'3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:06:59,865 --> 00:07:05,580': 'if the values are distributed uniformly and this wont save us any spacing'}
--------------------------------------------------
12 spacing 1.2213278741915971e-06 if the values are distributed uniformly and this wont save us any spacing 
{'3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:06:59,865 --> 00:07:05,580': 'if the values are distributed uniformly and this wont save us any spacing'}
--------------------------------------------------
11 dgap 2.4426557483831942e-06 well it turns out you can use a trick called the dgap and that that is to store the difference of these term ids 
{'3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:07:31,860 --> 00:07:34,616': 'well it turns out you can use a trick called the dgap', '3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:17:11,070 --> 00:17:15,720': 'what about the document ids that might be compressed using dgap'}
--------------------------------------------------
32 restore 1.2213278741915971e-06 and in order to recover the the exact document id we have to first recover the previous document id and then we can add the difference to the previous document id to restore the the current document id 
{'3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:08:30,560 --> 00:08:35,830': 'the difference to the previous document id to restore the the current document id'}
--------------------------------------------------
4 equallength 1.7445380593498679e-06 binary code is really equallength in coding 
{'3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:09:14,130 --> 00:09:16,830': 'binary code is really equallength in coding'}
--------------------------------------------------
4 inefficient 1.7445380593498679e-06 so this is very inefficient 
{'3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:10:07,160 --> 00:10:08,980': 'so this is very inefficient '}
--------------------------------------------------
10 aggressive 2.4426557483831942e-06 now which is to start at unary code is to aggressive in rewarding small numbers 
{'3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:11:07,810 --> 00:11:15,287': 'now which is to start at unary code is to aggressive in rewarding small numbers', '3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:11:20,430 --> 00:11:24,900': 'so what about some other less aggressive method'}
--------------------------------------------------
12 rewarding 2.2677482445081393e-06 now which is to start at unary code is to aggressive in rewarding small numbers 
{'3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:11:07,810 --> 00:11:15,287': 'now which is to start at unary code is to aggressive in rewarding small numbers'}
--------------------------------------------------
6 aggressive 2.4426557483831942e-06 so what about some other less aggressive method 
{'3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:11:07,810 --> 00:11:15,287': 'now which is to start at unary code is to aggressive in rewarding small numbers', '3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:11:20,430 --> 00:11:24,900': 'so what about some other less aggressive method'}
--------------------------------------------------
7 urinary 2.4426557483831942e-06 so that why we have four using urinary code for that so and so we first we have the urinary code for coding this log of s and this will be followed by a uniform code or binary code and this is basically the same uniform code and binary code are the same 
{'3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:11:47,910 --> 00:11:53,140': 'so that why we have four using urinary code for that so', '3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:11:54,220 --> 00:11:58,920': 'and so we first we have the urinary code for coding this log of s'}
--------------------------------------------------
19 urinary 2.4426557483831942e-06 so that why we have four using urinary code for that so and so we first we have the urinary code for coding this log of s and this will be followed by a uniform code or binary code and this is basically the same uniform code and binary code are the same 
{'3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:11:47,910 --> 00:11:53,140': 'so that why we have four using urinary code for that so', '3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:11:54,220 --> 00:11:58,920': 'and so we first we have the urinary code for coding this log of s'}
--------------------------------------------------
3 digits 1.2213278741915971e-06 the first two digits are the unary code 
{'3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:13:19,000 --> 00:13:21,560': 'the first two digits are the unary code '}
--------------------------------------------------
8 floor 2.4426557483831942e-06 since this is  then we know that the floor of log of x is actually  
{'3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:13:45,620 --> 00:13:50,050': 'since this is then we know that the floor of log of x is actually', '3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:14:18,980 --> 00:14:26,590': 'so this is the unary code for and so the floor of log of x is'}
--------------------------------------------------
10 floor 2.4426557483831942e-06 so this is the unary code for  and so the floor of log of x is  
{'3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:13:45,620 --> 00:13:50,050': 'since this is then we know that the floor of log of x is actually', '3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:14:18,980 --> 00:14:26,590': 'so this is the unary code for and so the floor of log of x is'}
--------------------------------------------------
12 operating 2.006143151929004e-06 it really a big loss for unary code and they are all operating of course at different degrees of favoring short favoring small integers 
{'3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:16:18,274 --> 00:16:24,464': 'it really a big loss for unary code and they are all operating'}
--------------------------------------------------
17 degrees 1.4829329667707326e-06 it really a big loss for unary code and they are all operating of course at different degrees of favoring short favoring small integers 
{'3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:16:24,464 --> 00:16:32,078': 'of course at different degrees of favoring short favoring small integers'}
--------------------------------------------------
10 dgap 2.4426557483831942e-06 what about the document ids that might be compressed using dgap 
{'3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:07:31,860 --> 00:07:34,616': 'well it turns out you can use a trick called the dgap', '3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:17:11,070 --> 00:17:15,720': 'what about the document ids that might be compressed using dgap'}
--------------------------------------------------
4 idealist 2.4426557483831942e-06 so suppose the encoded idealist is x x x et cetera 
{'3 - 8 - 2.7 Evaluation of TR Systems- Multi-Level Judgements (00-10-48).srt : 00:08:01,650 --> 00:08:06,270': 'that when youre relevance is in fact the idealist', '3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:17:18,330 --> 00:17:23,800': 'so suppose the encoded idealist is x x x et cetera'}
--------------------------------------------------
6 decoded 1.2213278741915971e-06 so we have to add the decoded value of x to id to recover the value of the the id at this secondary position right 
{'3 - 2 - 2.2 System Implementation- Inverted Index Construction (00-18-21).srt : 00:17:34,610 --> 00:17:41,330': 'so we have to add the decoded value of x to id to recover the value'}
--------------------------------------------------
13 analyses 2.006143151929004e-06 and this is very important because this allows us to do interesting comparative analyses 
{'5 - 7 - 4.7 Contextual Text Mining- Motivation (00-06-47).srt : 00:02:14,180 --> 00:02:18,060': 'us to do interesting comparative analyses '}
--------------------------------------------------
9 menus 1.2213278741915971e-06 similarly we can partition the data based on the menus 
{'5 - 7 - 4.7 Contextual Text Mining- Motivation (00-06-47).srt : 00:03:39,840 --> 00:03:42,745': 'similarly we can partition the data based on the menus'}
--------------------------------------------------
4 partitionings 1.2213278741915971e-06 and note that these partitionings can be also intersected with each other to generate even more complicated partitions 
{'5 - 7 - 4.7 Contextual Text Mining- Motivation (00-06-47).srt : 00:04:21,890 --> 00:04:25,310': 'and note that these partitionings can be also'}
--------------------------------------------------
8 intersected 1.7445380593498679e-06 and note that these partitionings can be also intersected with each other to generate even more complicated partitions 
{'5 - 7 - 4.7 Contextual Text Mining- Motivation (00-06-47).srt : 00:04:25,310 --> 00:04:28,860': 'intersected with each other to generate even more complicated partitions'}
--------------------------------------------------
6 responses 2.2677482445081393e-06 is there any difference in the responses of people in different regions to the event to any event 
{'5 - 7 - 4.7 Contextual Text Mining- Motivation (00-06-47).srt : 00:05:17,455 --> 00:05:20,675': 'is there any difference in the responses of people in different regions'}
--------------------------------------------------
11 regions 2.2677482445081393e-06 is there any difference in the responses of people in different regions to the event to any event 
{'5 - 7 - 4.7 Contextual Text Mining- Motivation (00-06-47).srt : 00:05:17,455 --> 00:05:20,675': 'is there any difference in the responses of people in different regions'}
--------------------------------------------------
13 usa 1.4829329667707326e-06 is there any difference in the research topics published by authors in the usa and those outside 
{'5 - 7 - 4.7 Contextual Text Mining- Motivation (00-06-47).srt : 00:05:34,110 --> 00:05:38,250': 'is there any difference in the research topics published by authors in the usa and'}
--------------------------------------------------
9 herself 2.4426557483831942e-06 so this goes beyond just the author himself or herself 
{'2 - 2 - 1.2 Overview Text Mining and Analytics- Part 2 (00-11-44).srt : 00:04:50,060 --> 00:04:54,710': 'we can mine knowledge about this observer himself or herself', '5 - 7 - 4.7 Contextual Text Mining- Motivation (00-06-47).srt : 00:05:47,810 --> 00:05:51,700': 'so this goes beyond just the author himself or herself'}
--------------------------------------------------
2 mattered 2.2677482445081393e-06 what issues mattered in the  presidential campaign or presidential election 
{'5 - 7 - 4.7 Contextual Text Mining- Motivation (00-06-47).srt : 00:06:17,230 --> 00:06:20,780': 'what issues mattered in the presidential campaign or'}
--------------------------------------------------
6 masses 1.2213278741915971e-06 so let plug in these model masses into the ranking function to see what we will get okay 
{'4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:00:13,295 --> 00:00:15,326': 'so let plug in these model masses '}
--------------------------------------------------
8 subtraction 1.2213278741915971e-06 so a general ranking function for smoothing with subtraction and you have seen this before 
{'4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:00:20,780 --> 00:00:24,570': 'so a general ranking function for smoothing with subtraction and'}
--------------------------------------------------
4 rewritten 2.4426557483831942e-06 this can be then rewritten as this 
{'3 - 6 - 2.6 Probabilistic Topic Models- Mining One Topic (00-12-21).srt : 00:03:52,580 --> 00:03:58,550': 'so in this line we have rewritten the formula into a product', '4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:01:18,530 --> 00:01:21,681': 'this can be then rewritten as this '}
--------------------------------------------------
5 lan 2.4426557483831942e-06 and we see docking the lan relationship here 
{'4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:04:05,720 --> 00:04:07,910': 'called a lan unigram language model ', '4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:02:58,720 --> 00:03:00,996': 'and we see docking the lan relationship here'}
--------------------------------------------------
27 clutch 1.2213278741915971e-06 the actual count of the word in the document with expected count given by this product if the word is in fact following the distribution in the clutch this 
{'4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:04:21,280 --> 00:04:29,570': 'product if the word is in fact following the distribution in the clutch this'}
--------------------------------------------------
18 documenting 1.2213278741915971e-06 and we also have a formula that intuitively makes a lot of sense and does tfidf weighting and documenting and some others 
{'4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:05:04,290 --> 00:05:07,190': 'does tfidf weighting and documenting and some others'}
--------------------------------------------------
11 meal 1.2213278741915971e-06 which is the expected account of the world if we sampled meal worlds according to the collection world probability 
{'4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:05:54,440 --> 00:05:59,400': 'which is the expected account of the world if we sampled meal worlds according to'}
--------------------------------------------------
12 worlds 1.2213278741915971e-06 which is the expected account of the world if we sampled meal worlds according to the collection world probability 
{'4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:05:54,440 --> 00:05:59,400': 'which is the expected account of the world if we sampled meal worlds according to'}
--------------------------------------------------
14 lighter 1.2213278741915971e-06 so note that it interesting we dont even see docking the lens here and lighter in the jms model 
{'4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:06:07,266 --> 00:06:08,910': 'lighter in the jms model '}
--------------------------------------------------
17 jms 2.4426557483831942e-06 so note that it interesting we dont even see docking the lens here and lighter in the jms model 
{'4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:06:07,266 --> 00:06:08,910': 'lighter in the jms model ', '4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:07:32,038 --> 00:07:34,580': 'in jms one '}
--------------------------------------------------
4 queer 1.2213278741915971e-06 and were against the queer the query time frequency here 
{'4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:06:36,290 --> 00:06:40,050': 'and were against the queer the query time frequency here'}
--------------------------------------------------
11 modulation 1.2213278741915971e-06 but it still it still clear that it does documents lens modulation because this lens is in the denominator so a longer document will have a lower weight here 
{'4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:07:15,974 --> 00:07:19,765': 'modulation because this lens is in the denominator so'}
--------------------------------------------------
16 jms 2.4426557483831942e-06 only that this time the form of the formula is different from the previous one in jms one 
{'4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:06:07,266 --> 00:06:08,910': 'lighter in the jms model ', '4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:07:32,038 --> 00:07:34,580': 'in jms one '}
--------------------------------------------------
11 intentionally 2.4426557483831942e-06 so we do have the sublinear transformation but we do not intentionally do that 
{'3 - 14 - 2.14 Probabilistic Latent Semantic Analysis (PLSA)- Part 2 (00-10-15).srt : 00:07:35,340 --> 00:07:38,550': 'and i intentionally leave this as an exercise for you', '4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:08:20,986 --> 00:08:23,320': 'we do not intentionally do that '}
--------------------------------------------------
26 motion 1.7445380593498679e-06 so that an example of the gap between a formal model like this and the relevance that we have to model which is really a subject motion that is tied to users 
{'4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:08:43,080 --> 00:08:48,720': 'which is really a subject motion that is tied to users'}
--------------------------------------------------
10 predictable 1.2213278741915971e-06 so the consequence of the modification is no longer as predictable as what we have been doing now 
{'4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:09:10,780 --> 00:09:14,670': 'longer as predictable as what we have been doing now'}
--------------------------------------------------
13 channel 1.2213278741915971e-06 so that also why for example pm remains very competitive and still open channel how to use public risk models as they arrive better model than the pm 
{'4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:09:21,410 --> 00:09:26,720': 'still open channel how to use public risk models as they arrive'}
--------------------------------------------------
17 public 1.7445380593498679e-06 so that also why for example pm remains very competitive and still open channel how to use public risk models as they arrive better model than the pm 
{'4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:09:21,410 --> 00:09:26,720': 'still open channel how to use public risk models as they arrive'}
--------------------------------------------------
22 arrive 1.2213278741915971e-06 so that also why for example pm remains very competitive and still open channel how to use public risk models as they arrive better model than the pm 
{'4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:09:21,410 --> 00:09:26,720': 'still open channel how to use public risk models as they arrive'}
--------------------------------------------------
25 articulate 1.2213278741915971e-06 in most cases we can see by using these smoothing methods we will be able to reach a retrieval function where the assumptions are clearly articulate 
{'4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:10:10,890 --> 00:10:16,670': 'reach a retrieval function where the assumptions are clearly articulate'}
--------------------------------------------------
13 adultation 1.2213278741915971e-06 also are very effective and they are comparable to bm  or pm lens adultation 
{'4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:10:23,810 --> 00:10:31,036': 'also are very effective and they are comparable to bm or pm lens adultation'}
--------------------------------------------------
7 uhwith 1.2213278741915971e-06 yet in the end we end up uhwith some retrievable functions that look very similar to the vector space model 
{'4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:11:08,900 --> 00:11:12,980': 'yet in the end we end up uhwith some retrievable functions that'}
--------------------------------------------------
9 retrievable 1.2213278741915971e-06 yet in the end we end up uhwith some retrievable functions that look very similar to the vector space model 
{'4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:11:08,900 --> 00:11:12,980': 'yet in the end we end up uhwith some retrievable functions that'}
--------------------------------------------------
5 med 1.2213278741915971e-06 and the second assumption with med is are query words are generated independently that allows us to decompose the probability of the whole query into a product of probabilities of old words in the query 
{'4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:11:49,470 --> 00:11:53,450': 'and the second assumption with med is are query words are generated independently'}
--------------------------------------------------
6 ama 1.2213278741915971e-06 that a smoothing with a collection ama model 
{'4 - 7 - 3.5 Smoothing Methods Part - 2 (00-13-17).srt : 00:12:14,870 --> 00:12:17,290': 'that a smoothing with a collection ama model'}
--------------------------------------------------
8 da 1.2213278741915971e-06 we can then run two systems on these da data sets to quantitatively evaluate your performance 
{'3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).srt : 00:00:39,440 --> 00:00:44,299': 'we can then run two systems on these da data sets to'}
--------------------------------------------------
8 settles 1.2213278741915971e-06 and we raised to the question about which settles results is better is system a better or system b better 
{'3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).srt : 00:00:52,197 --> 00:00:57,140': 'sound which settles results is better is system a better or system b better'}
--------------------------------------------------
6 rendered 1.2213278741915971e-06 and we have only seen three rendered documents there but we can imagine there are other random documents in judging for this query 
{'3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).srt : 00:01:15,410 --> 00:01:20,580': 'and we have only seen three rendered documents there but'}
--------------------------------------------------
31 denominators 2.4426557483831942e-06 all right so we going to see precision and recall is all focused on looking at the a that the number of retrieval relevant documents but were going to use different denominators 
{'4 - 4 - 3.4 Text Clustering- Generative Probabilistic Models Part 3 (00-14-55).srt : 00:10:07,940 --> 00:10:14,480': 'you can see we can make the numerators and the denominators more manageable', '3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).srt : 00:05:19,683 --> 00:05:22,449': 'were going to use different denominators '}
--------------------------------------------------
5 distant 1.7445380593498679e-06 as you go down the distant to try to get as many relevant actions as possible 
{'3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).srt : 00:05:56,210 --> 00:06:00,790': 'as you go down the distant to try to get as many relevant actions as possible'}
--------------------------------------------------
6 tension 1.2213278741915971e-06 they are the fundamental measures in tension retrieval and many other tasks 
{'3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).srt : 00:06:19,905 --> 00:06:24,270': 'they are the fundamental measures in tension retrieval and many other tasks'}
--------------------------------------------------
23 co 1.4829329667707326e-06 so you can see it first computed inverse of r and p here and then it would be interpreted to by using a co coefficients 
{'3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).srt : 00:07:31,732 --> 00:07:38,166': 'interpreted to by using a co coefficients'}
--------------------------------------------------
3 paralyze 2.4426557483831942e-06 so it would paralyze a case where you have extremely high matter for one of them 
{'4 - 5 - 3.4 Smoothing of Language Model - Part 2 (00-09-36).srt : 00:05:06,218 --> 00:05:12,049': 'so this term appears to paralyze long documents', '3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).srt : 00:10:28,310 --> 00:10:32,420': 'so it would paralyze a case where you have extremely high'}
--------------------------------------------------
6 settle 1.7445380593498679e-06 but it important that not to settle on this solution 
{'3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).srt : 00:10:53,790 --> 00:10:57,160': 'but it important that not to settle on this solution'}
--------------------------------------------------
23 ahead 1.2213278741915971e-06 but if you dont pay attention to these subtle differences you might just take an easy way to combine them and then go ahead with it 
{'3 - 5 - 2.5 Evaluation of TR Systems- Basic Measures (00-12-54).srt : 00:11:29,172 --> 00:11:33,388': 'you might just take an easy way to combine them and then go ahead with it'}
--------------------------------------------------
7 complicate 1.2213278741915971e-06 that of course an oversimplification of the complicate it well 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:00:25,215 --> 00:00:29,640': 'that of course an oversimplification of the complicate it well'}
--------------------------------------------------
9 walker 1.7445380593498679e-06 and let assume that a random surfer or random walker can be any of these pages 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:00:34,280 --> 00:00:40,310': 'and let assume that a random surfer or random walker can be any of these pages'}
--------------------------------------------------
3 outlinks 1.7445380593498679e-06 now there two outlinks here 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:01:06,280 --> 00:01:08,090': 'now there two outlinks here '}
--------------------------------------------------
5 eh 1.2213278741915971e-06 so if it does that eh it would be able to reach any of the other pages even though there is no link directly from to that page 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:01:33,356 --> 00:01:38,950': 'so if it does that eh it would be able to reach'}
--------------------------------------------------
6 randoms 1.2213278741915971e-06 so this is the assume the randoms of 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:01:46,170 --> 00:01:49,430': 'so this is the assume the randoms of '}
--------------------------------------------------
15 visits 1.7445380593498679e-06 so the page rank score of the document is the average probability that the surfer visits a particular page 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:02:17,990 --> 00:02:20,850': 'that the surfer visits a particular page '}
--------------------------------------------------
10 sphere 1.2213278741915971e-06 so first let take a look at the transition matching sphere 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:03:22,128 --> 00:03:25,171': 'so first let take a look at the transition matching sphere'}
--------------------------------------------------
12 rand 1.2213278741915971e-06 and this is just a matrix with values indicating how likely a rand the random surfer will go from one page to another 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:03:25,171 --> 00:03:29,543': 'and this is just a matrix with values indicating how likely a rand'}
--------------------------------------------------
24 cont 1.2213278741915971e-06 so here on the lefthand side you see it the probability of visiting page dj at time t plus  because it the next time cont 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:05:02,170 --> 00:05:07,570': 'visiting page dj at time t plus because it the next time cont'}
--------------------------------------------------
16 ei 1.7445380593498679e-06 on the right hand side you can see the question involves the probability of at page ei at time t so you can see the subsequent index t here 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:05:14,740 --> 00:05:20,000': 'of at page ei at time t '}
--------------------------------------------------
25 subsequent 2.2677482445081393e-06 on the right hand side you can see the question involves the probability of at page ei at time t so you can see the subsequent index t here 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:05:21,330 --> 00:05:24,200': 'so you can see the subsequent index t here'}
--------------------------------------------------
8 chooses 1.7445380593498679e-06 and you can see and the random surfer chooses this strategy was probably the inaudible as we assumed 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:06:01,670 --> 00:06:05,720': 'and you can see and the random surfer chooses this'}
--------------------------------------------------
10 transports 1.2213278741915971e-06 it the vector p here equals a metrics or the transports of the metrics here 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:09:20,950 --> 00:09:29,740': 'it the vector p here equals a metrics or the transports of the metrics here'}
--------------------------------------------------
8 ball 2.4426557483831942e-06 so is it because she here on the ball easily taken from the previous slide 
{'4 - 2 - 3.2 Statistical Language Models (00-17-53) .srt : 00:16:30,780 --> 00:16:35,670': 'by taking the same ball of text that contains the computer we dont', '5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:09:54,700 --> 00:10:01,940': 'so is it because she here on the ball easily taken from the previous slide'}
--------------------------------------------------
5 pvector 1.2213278741915971e-06 the metrics here by this pvector 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:10:26,860 --> 00:10:30,850': 'the metrics here by this pvector '}
--------------------------------------------------
8 guys 1.2213278741915971e-06 we started with some initial values for these guys 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:12:18,420 --> 00:12:20,939': 'we started with some initial values for these guys'}
--------------------------------------------------
7 revise 2.4426557483831942e-06 for for this and then we just revise the scores which generate a new set of scores 
{'3 - 11 - 2.11 Probabilistic Topic Models- Expectation-Maximization Algorithm- Part 2 (00-10-39).srt : 00:02:32,865 --> 00:02:35,479': 'to revise our estimate of the parameters ', '5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:12:24,000 --> 00:12:29,250': 'we just revise the scores which generate a new set of scores'}
--------------------------------------------------
7 propagated 1.7445380593498679e-06 and then combine their scores and the propagated score the sum of the scores to this document d 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:13:54,560 --> 00:14:00,400': 'the propagated score the sum of the scores to this document d'}
--------------------------------------------------
4 composition 2.006143151929004e-06 so in practice the composition of pagerank score is actually efficient because the metrices are sparse and there are some ways to transform the equation so you avoid actually literally computing the values of all of those elements 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:14:38,180 --> 00:14:44,070': 'so in practice the composition of pagerank score is actually efficient because'}
--------------------------------------------------
13 metrices 1.2213278741915971e-06 so in practice the composition of pagerank score is actually efficient because the metrices are sparse and there are some ways to transform the equation so you avoid actually literally computing the values of all of those elements 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:14:44,070 --> 00:14:49,280': 'the metrices are sparse and there are some ways to transform the equation so'}
--------------------------------------------------
10 outlook 1.2213278741915971e-06 in that case if the page does not have any outlook then the probability of these pages will will not sum to  
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:15:10,730 --> 00:15:17,540': 'in that case if the page does not have any outlook then the probability of'}
--------------------------------------------------
10 damping 1.2213278741915971e-06 and one possible solution is simply to use page specific damping factor and that that could easily fix this 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:15:37,160 --> 00:15:43,250': 'and one possible solution is simply to use page specific damping factor and'}
--------------------------------------------------
16 outlink 1.2213278741915971e-06 basically that to say how far do we want from zero for a page with no outlink 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:15:46,740 --> 00:15:50,740': 'basically that to say how far do we want from zero for a page with no outlink'}
--------------------------------------------------
9 render 1.2213278741915971e-06 in that case the server would just have to render them inaudible to another page instead of trying to follow the link 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:15:54,140 --> 00:15:57,370': 'render them inaudible to another page instead of trying to follow the link'}
--------------------------------------------------
5 topspecific 1.2213278741915971e-06 one extension is to do topspecific page rank 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:16:01,540 --> 00:16:05,060': 'one extension is to do topspecific page rank'}
--------------------------------------------------
5 canbuy 1.2213278741915971e-06 by doing this then we canbuy a pagerank to topic align with sports 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:16:40,630 --> 00:16:45,350': 'by doing this then we canbuy a pagerank to topic align with sports'}
--------------------------------------------------
17 friendship 1.7445380593498679e-06 we can imagine if you compute their pagerank scores for social network where a link might indicate friendship relation youll get some meaningful scores for people 
{'5 - 4 - 4.3 Link Analysis - Part 2 (00-17-30).srt : 00:17:10,493 --> 00:17:13,776': 'where a link might indicate friendship relation'}
--------------------------------------------------
14 synagmatic 1.2213278741915971e-06 it is not really comparable so that makes it harder with this cover strong synagmatic relations globally from corpus 
{'2 - 12 - 1.12 Syntagmatic Relation Discovery- Mutual Information- Part 1 (00-13-55).srt : 00:00:42,600 --> 00:00:48,360': 'strong synagmatic relations globally from corpus'}
--------------------------------------------------
16 globally 1.2213278741915971e-06 it is not really comparable so that makes it harder with this cover strong synagmatic relations globally from corpus 
{'2 - 12 - 1.12 Syntagmatic Relation Discovery- Mutual Information- Part 1 (00-13-55).srt : 00:00:42,600 --> 00:00:48,360': 'strong synagmatic relations globally from corpus'}
--------------------------------------------------
8 ixy 1.2213278741915971e-06 in particular mutual information in order to find ixy matches the entropy reduction of x obtained from knowing y 
{'2 - 12 - 1.12 Syntagmatic Relation Discovery- Mutual Information- Part 1 (00-13-55).srt : 00:01:04,930 --> 00:01:10,090': 'in particular mutual information in order to find ixy'}
--------------------------------------------------
20 verified 1.4829329667707326e-06 that means knowing one of them does not tell us anything about the other and this last property can be verified by simply looking at the equation above and it reaches  if and only the conditional entropy of x inaudible y is exactly the same as original entropy of x 
{'2 - 12 - 1.12 Syntagmatic Relation Discovery- Mutual Information- Part 1 (00-13-55).srt : 00:03:07,949 --> 00:03:14,626': 'this last property can be verified by simply looking at the equation above and'}
--------------------------------------------------
8 ys 1.2213278741915971e-06 now when we fix x to rank different ys using conditional entropy would give the same order as ranking based on mutual information because in the function here hx is fixed because x is fixed 
{'2 - 12 - 1.12 Syntagmatic Relation Discovery- Mutual Information- Part 1 (00-13-55).srt : 00:03:32,120 --> 00:03:37,880': 'now when we fix x to rank different ys using conditional entropy'}
--------------------------------------------------
28 hx 2.2677482445081393e-06 now when we fix x to rank different ys using conditional entropy would give the same order as ranking based on mutual information because in the function here hx is fixed because x is fixed 
{'2 - 12 - 1.12 Syntagmatic Relation Discovery- Mutual Information- Part 1 (00-13-55).srt : 00:03:44,180 --> 00:03:49,940': 'because in the function here hx is fixed because x is fixed'}
--------------------------------------------------
5 forcing 1.7445380593498679e-06 now the question we ask forcing that relation mining is whenever eats occurs what other words also tend to occur 
{'2 - 12 - 1.12 Syntagmatic Relation Discovery- Mutual Information- Part 1 (00-13-55).srt : 00:04:17,150 --> 00:04:20,430': 'now the question we ask forcing that relation mining is'}
--------------------------------------------------
12 volume 1.7445380593498679e-06 it is going to be larger then are equal to the machine volume eats in other words 
{'2 - 12 - 1.12 Syntagmatic Relation Discovery- Mutual Information- Part 1 (00-13-55).srt : 00:05:54,472 --> 00:06:02,520': 'it is going to be larger then are equal to the machine volume eats in other words'}
--------------------------------------------------
10 mute 1.2213278741915971e-06 so now let us look at how to compute the mute information 
{'2 - 12 - 1.12 Syntagmatic Relation Discovery- Mutual Information- Part 1 (00-13-55).srt : 00:06:16,386 --> 00:06:21,390': 'so now let us look at how to compute the mute information'}
--------------------------------------------------
17 absences 1.2213278741915971e-06 in the segment and similarly for the second word we also have two probabilities representing presence or absences of this word and there is some to y as well 
{'2 - 12 - 1.12 Syntagmatic Relation Discovery- Mutual Information- Part 1 (00-13-55).srt : 00:09:18,230 --> 00:09:20,920': 'absences of this word and there is some to y as well'}
--------------------------------------------------
36 cooccurrency 1.2213278741915971e-06 so in the previous slide that you have seen that the marginal probabilities of these words sum to one and we also have seen this constraint that says the two words have these four scenarios of cooccurrency but we also have some additional constraints listed in the bottom 
{'2 - 12 - 1.12 Syntagmatic Relation Discovery- Mutual Information- Part 1 (00-13-55).srt : 00:10:46,114 --> 00:10:53,190': 'that says the two words have these four scenarios of cooccurrency'}
--------------------------------------------------
22 naming 1.2213278741915971e-06 so this slide shows that we only need to know how to compute these three probabilities that are shown in the boxes naming the presence of each word and the cooccurence of both words in a segment 
{'2 - 12 - 1.12 Syntagmatic Relation Discovery- Mutual Information- Part 1 (00-13-55).srt : 00:13:35,800 --> 00:13:43,092': 'naming the presence of each word and the cooccurence of both words in a segment'}
--------------------------------------------------
30 cooccurence 1.2213278741915971e-06 so this slide shows that we only need to know how to compute these three probabilities that are shown in the boxes naming the presence of each word and the cooccurence of both words in a segment 
{'2 - 12 - 1.12 Syntagmatic Relation Discovery- Mutual Information- Part 1 (00-13-55).srt : 00:13:35,800 --> 00:13:43,092': 'naming the presence of each word and the cooccurence of both words in a segment'}
--------------------------------------------------
11 reproaches 1.7445380593498679e-06 there are many more advanced learning algorithms than the regression based reproaches 
{'5 - 8 - 4.4 Learning to Rank - Part 3 (00-04-58).srt : 00:00:10,014 --> 00:00:11,300': 'reproaches '}
--------------------------------------------------
4 objecting 1.2213278741915971e-06 note that the optimization objecting function that we have seen on the previous slide is not directly related to retrieval measure 
{'5 - 8 - 4.4 Learning to Rank - Part 3 (00-04-58).srt : 00:00:19,020 --> 00:00:24,630': 'note that the optimization objecting function that we have seen'}
--------------------------------------------------
10 adv 1.2213278741915971e-06 so here i list some for example recommender systems computational adv advertising or summarization and there are many others that you can probably encounter in your applications 
{'5 - 8 - 4.4 Learning to Rank - Part 3 (00-04-58).srt : 00:01:58,999 --> 00:02:03,136': 'computational adv advertising or summarization and'}
--------------------------------------------------
2 werent 2.4426557483831942e-06 such data werent available before 
{'5 - 11 - 4.11 Course Summary (00-18-36).srt : 00:12:34,330 --> 00:12:38,320': 'so that one example of inaudible techniques that we werent able to cover', '5 - 8 - 4.4 Learning to Rank - Part 3 (00-04-58).srt : 00:03:01,330 --> 00:03:04,250': 'such data werent available before '}
--------------------------------------------------
5 combating 1.7445380593498679e-06 so this is designed for combating spams 
{'5 - 8 - 4.4 Learning to Rank - Part 3 (00-04-58).srt : 00:03:35,900 --> 00:03:39,839': 'so this is designed for combating spams '}
--------------------------------------------------
